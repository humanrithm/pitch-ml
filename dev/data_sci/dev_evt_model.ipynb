{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 413,
   "id": "be58af02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from connections import AWS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e20355d",
   "metadata": {},
   "source": [
    "$\\textbf{Functions}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "0e755140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a .sto file (e.g., results from an JRA run)\n",
    "def load_sto_file(path: str, skip_rows: int = 10) -> pd.DataFrame:\n",
    "    return pd.read_csv(path, sep='\\\\s+', skiprows=skip_rows)\n",
    "\n",
    "# mirror columns to match RHP\n",
    "def mirror_columns(\n",
    "        data: pd.DataFrame,\n",
    "        cols: list\n",
    "):\n",
    "    data_mirrored = data.copy()\n",
    "    for col in cols:\n",
    "        data_mirrored.loc[data['throws'] == 'left', col] *= -1\n",
    "    \n",
    "    return data_mirrored"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16b6217",
   "metadata": {},
   "source": [
    "$\\textbf{Data Loading}$\n",
    "\n",
    "- AWS connection\n",
    "- Result summaries, subject info, etc.\n",
    "- All IK and JRA files\n",
    "- Ball flight data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30abb00f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AWS]: Port 5433 is in use by process python3.11 (PID 45748). Killing it.\n",
      "[AWS]: Connected to RDS endpoint.\n"
     ]
    }
   ],
   "source": [
    "\"\"\" INITIALIZE AWS CONNECTION \"\"\"\n",
    "aws_connection = AWS()\n",
    "aws_connection.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9263ec7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" S3 OBJECTS \"\"\"\n",
    "\n",
    "# load all files from S3\n",
    "s3_objects = aws_connection.list_s3_objects(prefix='subjects/')\n",
    "\n",
    "# filter to IK, JRA results files\n",
    "ik_result_files = [obj for obj in s3_objects if 'ik_results' in obj]\n",
    "jra_result_files = [obj for obj in s3_objects if 'jra_results' in obj]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74df99ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" SUMMARIES & SUBJECT INFO \"\"\"\n",
    "\n",
    "# subject info\n",
    "subject_info = aws_connection.load_subject_info()\n",
    "\n",
    "# JRA summary (at peak)\n",
    "    # NOTE: outliers not included here --> 3,401 trials\n",
    "jra_summary_bytes = aws_connection.load_s3_object('subjects/summary/results_jra.csv', return_info=False)\n",
    "jra_summary = pd.read_csv(io.BytesIO(jra_summary_bytes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "id": "3e363b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" LOAD JRA RESULTS \"\"\"\n",
    "\n",
    "# initialize jra results \n",
    "jra_results = []\n",
    "\n",
    "# read all JRA result CSVs\n",
    "for file in jra_result_files:\n",
    "    \n",
    "    # get study ID, check for eligible trial\n",
    "    study_id = file.split('/')[-1].split('_jra')[0]\n",
    "    if study_id not in jra_summary['study_id'].values:\n",
    "        continue  # skip if not eligible\n",
    "    \n",
    "    # load JRA results\n",
    "    jra_bytes = aws_connection.load_s3_object(file, return_info=False)\n",
    "    jra_df = pd.read_csv(io.BytesIO(jra_bytes), sep='\\\\s+', skiprows=11)\n",
    "\n",
    "    # insert study ID\n",
    "    jra_df.insert(0, 'study_id', study_id)\n",
    "\n",
    "    # append to lists\n",
    "    jra_results.append(jra_df)\n",
    "\n",
    "# concatenate all JRA results (NOTE: do not reset index)\n",
    "jra_results_df = pd.concat(jra_results, ignore_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "id": "d189d916",
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate JRA throwing hand columns\n",
    "MOMENT_COLS_LEFT = [\n",
    "    'study_id', 'time', 'elbow_l_on_ulna_l_in_ulna_l_mx', 'elbow_l_on_ulna_l_in_ulna_l_my', 'elbow_l_on_ulna_l_in_ulna_l_mz'\n",
    "]\n",
    "MOMENT_COLS_RIGHT = [\n",
    "    'study_id', 'time', 'elbow_r_on_ulna_r_in_ulna_r_mx', 'elbow_r_on_ulna_r_in_ulna_r_my', 'elbow_r_on_ulna_r_in_ulna_r_mz'\n",
    "]\n",
    "\n",
    "# extract separate dataframes\n",
    "jra_left = jra_results_df[MOMENT_COLS_LEFT].dropna(how='any')\n",
    "jra_right = jra_results_df[MOMENT_COLS_RIGHT].dropna(how='any')\n",
    "\n",
    "# rename columns of each\n",
    "MOMENT_COLS = [\n",
    "    'study_id', 'time', 'elbow_mx', 'elbow_my', 'elbow_mz'\n",
    "]\n",
    "jra_left.columns = MOMENT_COLS\n",
    "jra_right.columns = MOMENT_COLS\n",
    "\n",
    "# combine left and right dfs\n",
    "jra_combined = pd.concat([jra_left, jra_right], ignore_index=False).sort_values(by=['study_id', 'time'])\n",
    "\n",
    "# compute elbow moment magnitude\n",
    "ELBOW_MOMENT_COLS = ['elbow_mx', 'elbow_my', 'elbow_mz']\n",
    "jra_combined['elbow_magnitude'] = np.sqrt(jra_combined['elbow_mx']**2 + jra_combined['elbow_my']**2 + jra_combined['elbow_mz']**2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "id": "34477992",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract peak elbow moment magnitude for each trial\n",
    "peak_moments = {}\n",
    "for trial in jra_combined['study_id'].unique():\n",
    "    trial_data = jra_combined[jra_combined['study_id'] == trial]\n",
    "    peak_idx = trial_data['elbow_magnitude'].idxmax()\n",
    "\n",
    "    # store results\n",
    "    peak_moments[trial] = {\n",
    "        'study_id': trial,\n",
    "        'peak_idx': peak_idx,\n",
    "        'peak_elbow_magnitude': trial_data.loc[peak_idx, 'elbow_magnitude']\n",
    "    }\n",
    "\n",
    "# convert to DataFrame\n",
    "peak_moments_final = pd.DataFrame.from_dict(peak_moments).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "34d1fcc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" LOAD IK RESULTS \"\"\"\n",
    "\n",
    "# set column names\n",
    "ik_cols = [\n",
    "    'study_id', 'time', 'arm_flex', 'arm_add', 'arm_rot', 'humerus_tx', 'humerus_ty', \n",
    "    'humerus_tz', 'elbow_flex', 'pro_sup', 'wrist_flex', 'wrist_dev'\n",
    "]\n",
    "\n",
    "# initialize lists of results\n",
    "ik_results = []\n",
    "ik_peaks = []\n",
    "\n",
    "# read all IK result CSVs\n",
    "for file in ik_result_files:\n",
    "    \n",
    "    # extract study ID, check if in JRA summary\n",
    "    study_id = file.split('/')[-1].split('_ik')[0]\n",
    "    if study_id not in jra_summary['study_id'].values:\n",
    "        continue\n",
    "    \n",
    "    ik_bytes = aws_connection.load_s3_object(file, return_info=False)\n",
    "    ik_df = pd.read_csv(io.BytesIO(ik_bytes))\n",
    "\n",
    "    # update column names & insert study ID\n",
    "    ik_df.insert(0, 'study_id', study_id)\n",
    "    ik_df.columns = ik_cols\n",
    "\n",
    "    # extract values at peak torque\n",
    "        # NOTE: now uses peak moment magnitude\n",
    "    peak_idx = peak_moments_final[peak_moments_final['study_id'] == study_id]['peak_idx'].values[0]\n",
    "    peak_angles = pd.DataFrame(ik_df.iloc[peak_idx, :]).T\n",
    "\n",
    "    # append to lists \n",
    "    ik_results.append(ik_df)\n",
    "    ik_peaks.append(peak_angles)\n",
    "\n",
    "# concatenate all IK results (NOTE: do not reset index)\n",
    "ik_results_df = pd.concat(ik_results, ignore_index=False)\n",
    "ik_peaks_df = pd.concat(ik_peaks, ignore_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "a6c34a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" LOAD IK ERRORS \"\"\"\n",
    "\n",
    "# list all error files (by trial)\n",
    "ik_error_files = [obj for obj in s3_objects if 'ik_errors' in obj]\n",
    "\n",
    "ik_errors = []\n",
    "for file in ik_error_files:\n",
    "    \n",
    "    # extract study ID, check if in JRA summary\n",
    "    study_id = file.split('/')[-1].split('_ik')[0]\n",
    "    if study_id not in jra_summary['study_id'].values:\n",
    "        continue\n",
    "    \n",
    "    # load error file\n",
    "    error_bytes = aws_connection.load_s3_object(file, return_info=False)\n",
    "    error_df = pd.read_csv(io.BytesIO(error_bytes))\n",
    "\n",
    "    # insert study ID\n",
    "    error_df.insert(0, 'study_id', study_id)\n",
    "\n",
    "    # append to list\n",
    "    ik_errors.append(error_df)\n",
    "\n",
    "# concatenate all IK errors (NOTE: do not reset index)\n",
    "ik_errors_df = pd.concat(ik_errors, ignore_index=False)\n",
    "\n",
    "# summarize errors & filter to threshold\n",
    "    # 3,505 trials\n",
    "THRESHOLD = 0.025\n",
    "ik_errors_avg = ik_errors_df.groupby('study_id')['marker_error_RMS'].mean().reset_index()\n",
    "ik_eligible_trials = ik_errors_avg[ik_errors_avg['marker_error_RMS'] < THRESHOLD]['study_id'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "0a66a1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" BALL TRACKING DATA \"\"\"\n",
    "\n",
    "# get ball tracking objects (trackman)\n",
    "trackman_clean_bytes = aws_connection.load_s3_object('subjects/summary/ball_tracking_trackman.csv', return_info=False)\n",
    "trackman_clean = pd.read_csv(io.BytesIO(trackman_clean_bytes))\n",
    "trackman_clean.rename(columns={'taggedPitchType': 'pitchType'}, inplace=True)\n",
    "\n",
    "# get ball tracking objects (rapsodo)\n",
    "rapsodo_clean_bytes = aws_connection.load_s3_object('subjects/summary/ball_tracking_rapsodo.csv', return_info=False)\n",
    "rapsodo_clean = pd.read_csv(io.BytesIO(rapsodo_clean_bytes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "id": "a1c204e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" FINAL MODELING DATA \"\"\"\n",
    "\n",
    "# modeling data: biomech\n",
    "peak_moments_final.insert(\n",
    "    0, \n",
    "    'subject_id', \n",
    "    peak_moments_final['study_id'].str.split('_').str[0].astype(int)\n",
    ")\n",
    "model_data_biomech = peak_moments_final.merge(\n",
    "    ik_peaks_df,\n",
    "    on='study_id',\n",
    "    how='left',\n",
    ")\n",
    "\n",
    "# modeling data: ball tracking\n",
    "ball_flight_cols = [\n",
    "    'study_id', 'pitchType', 'relSpeed', 'vertRelAngle', \n",
    "    'horzRelAngle', 'spinRate', 'spinAxis', 'relHeight', \n",
    "    'relSide', 'inducedVertBreak', 'horzBreak', \n",
    "    'x0', 'y0', 'z0', 'vx0', 'vy0', 'vz0', 'ax0', 'ay0', 'az0' \n",
    "]\n",
    "model_data_trackman = trackman_clean[ball_flight_cols]\n",
    "model_data_rapsodo = rapsodo_clean[ball_flight_cols]\n",
    "model_data_ball = pd.concat([model_data_trackman, model_data_rapsodo])\n",
    "\n",
    "# drop NAs\n",
    "model_data_ball.dropna(subset='pitchType', inplace=True)\n",
    "\n",
    "# merge data together\n",
    "    # 2,986 pitches; 153 pitchers\n",
    "model_data_final = subject_info.merge(model_data_biomech, on='subject_id', how='left').merge(model_data_ball, on='study_id', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "id": "20a1fad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AWS]: Uploaded object to s3://pitch-ml/subjects/summary/model_data.csv\n"
     ]
    }
   ],
   "source": [
    "# save to csv, write to S3\n",
    "model_data_final.to_csv('model_data_raw.csv', index=False)\n",
    "aws_connection.upload_to_s3(model_data_final, 'subjects/summary/model_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1afce33",
   "metadata": {},
   "source": [
    "$\\textbf{Modeling: Peak Elbow Moment Magnitude (Development)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a78ff04",
   "metadata": {},
   "outputs": [],
   "source": [
    "## FINAL DATA PREPROCESSING\n",
    "    # normalize torque values\n",
    "    # mirror LHP columns to match RHP\n",
    "    # scale to metric system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "id": "f65ff3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize torque values\n",
    "model_data_final['normalized_peak_moment'] = model_data_final['peak_elbow_magnitude'] / (model_data_final['height'] * model_data_final['mass'] * 9.81)\n",
    "\n",
    "# mirror LHP columns to match RHP\n",
    "cols_to_mirror = ['x0', 'vx0', 'ax0', 'relSide', 'horzBreak', 'horzRelAngle']\n",
    "model_data_final = mirror_columns(model_data_final, cols_to_mirror).reset_index(drop=True)\n",
    "\n",
    "# convert to metric system\n",
    "conversion_factors = {\n",
    "    'relSpeed': 0.44704,    # mph to m/s\n",
    "    'relSide': 0.3048,      # ft to m\n",
    "    'relHeight': 0.3048,    # ft to m\n",
    "    'ax0': 0.3048,          # ft/s^2 to m/s^2\n",
    "    'ay0': 0.3048,          # ft/s^2 to m/s^2\n",
    "    'az0': 0.3048,          # ft/s^2 to m/s^2\n",
    "}\n",
    "\n",
    "# iterate through conversion factors and apply them in each df\n",
    "for col, factor in conversion_factors.items():\n",
    "    if col in model_data_final.columns:\n",
    "        model_data_final[col] *= factor\n",
    "    if col in model_data_final.columns:\n",
    "        model_data_final[col] *= factor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db0af18",
   "metadata": {},
   "source": [
    "$\\textit{Data Scaling}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "id": "6ebad1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eefaddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AWS]: Uploaded object to s3://pitch-ml/subjects/summary/model_data_processed.csv\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "scale_fts = ['relSide', 'relHeight', 'spinRate', 'spinAxis', 'relSpeed', 'ax0', 'ay0', 'az0'] + ['wrist_flex', 'wrist_dev', 'pro_sup', 'elbow_flex']\n",
    "\n",
    "# fit scaler to data & apply\n",
    "scaler.fit(model_data_final[scale_fts])\n",
    "model_data_final[scale_fts] = scaler.transform(model_data_final[scale_fts])\n",
    "\n",
    "# save the processed datasets\n",
    "model_data_final.to_csv('model_data_processed.csv', index=False)\n",
    "aws_connection.upload_to_s3(model_data_final, 'subjects/summary/model_data_processed.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4395755e",
   "metadata": {},
   "source": [
    "$\\textit{Model Development}$\n",
    "\n",
    "Implemented as follows. \n",
    "\n",
    "_For each subject_:\n",
    "- Train baseline LR, RF models on all other subjects to estimate peak EVT, store errors\n",
    "- Train LR, RF models on all other subjects to estimate kinematics at ball release, store errors; use model with lower error\n",
    "- Train engineered LR, RF models to estimate peak EVT, store errors\n",
    "\n",
    "This loop is more concise than previous iterations and makes it easier to prevent data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "id": "09b51cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import traceback\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import root_mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661ebd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_subject_models(\n",
    "    train_data: pd.DataFrame,\n",
    "    val_data: pd.DataFrame,\n",
    "    target: str = 'normalized_peak_moment'\n",
    ") -> dict:\n",
    "    \n",
    "    \"\"\" Train models for each subject using training and validation data. \n",
    "    \n",
    "    **Parameters**\n",
    "    - train_data: DataFrame containing training data for each subject.\n",
    "    - val_data: DataFrame containing validation data for each subject.\n",
    "    - target: The target variable to predict (default is 'normalized_peak_value').\n",
    "\n",
    "    **Returns**\n",
    "    - results: Dictionary containing trained models and their performance metrics.\n",
    "    \"\"\"\n",
    "\n",
    "    # initialize results dictionary, feature lists\n",
    "    results = {}\n",
    "    baseline_features = [\n",
    "        'relSide', 'relHeight', 'spinRate', 'spinAxis', 'relSpeed', 'ax0', 'ay0', 'az0'\n",
    "    ]\n",
    "    eng_features = baseline_features + ['pred_wrist_flex', 'pred_wrist_dev', 'pred_pro_sup', 'pred_elbow_flex']\n",
    "\n",
    "    \"\"\" BASELINE MODELS \"\"\"\n",
    "    # linear model\n",
    "    baseline_lr = train_linear_model(train_data, val_data, baseline_features, target)\n",
    "    results['baseline_lr'] = baseline_lr\n",
    "\n",
    "    # random forest\n",
    "    baseline_rf = train_rf_model(train_data, val_data, baseline_features, target, retrain=True)\n",
    "    results['baseline_rf'] = baseline_rf\n",
    "\n",
    "    \"\"\" KINEMATIC MODELS \"\"\"\n",
    "    # define kinematic features for each outcome, then train each\n",
    "    kinematic_fts = {\n",
    "        'wrist_flex': baseline_features,\n",
    "        'wrist_dev': baseline_features,\n",
    "        'pro_sup': baseline_features + ['pred_wrist_flex', 'pred_wrist_dev'],\n",
    "        'elbow_flex': baseline_features + ['pred_wrist_flex', 'pred_pro_sup']\n",
    "    }\n",
    "    for outcome, fts in kinematic_fts.items():\n",
    "\n",
    "        # train linear model, extract train preds --> add to train_data as pred_[kinematic]\n",
    "        lr_kinematic_model = train_linear_model(train_data, val_data, fts, outcome)\n",
    "        train_data.loc[:, f'pred_{outcome}'] = lr_kinematic_model['model'].predict(train_data[fts])\n",
    "        val_data.loc[:, f'pred_{outcome}'] = lr_kinematic_model['model'].predict(val_data[fts])\n",
    "        \n",
    "        # train random forest model, extract train preds --> add to train_data as pred_[kinematic]\n",
    "        rf_kinematic_model = train_rf_model(train_data, val_data, fts, outcome)\n",
    "        train_data.loc[:, f'pred_{outcome}'] = rf_kinematic_model['model'].predict(train_data[fts])\n",
    "        val_data.loc[:, f'pred_{outcome}'] = rf_kinematic_model['model'].predict(val_data[fts])\n",
    "\n",
    "    \"\"\" ENGINEERED MODELS \"\"\"\n",
    "    # linear model\n",
    "    eng_lr = train_linear_model(train_data, val_data, eng_features, target)\n",
    "    results['eng_lr'] = eng_lr\n",
    "    \n",
    "    # random forest\n",
    "    eng_rf = train_rf_model(train_data, val_data, eng_features, target, retrain=True)\n",
    "    results['eng_rf'] = eng_rf\n",
    "    \n",
    "    # return dictionary w/ models & results, errors\n",
    "    return results \n",
    "\n",
    "# linear model training\n",
    "    # val_data: subject data for validation\n",
    "def train_linear_model(\n",
    "        train_data: pd.DataFrame,\n",
    "        val_data: pd.DataFrame,\n",
    "        features: list,\n",
    "        target: str\n",
    ") -> dict:\n",
    "    lr_model = LinearRegression()                                               # initialize linear regression model\n",
    "    lr_model.fit(train_data[features], train_data[target])       # fit model to training data\n",
    "\n",
    "    # get validation predictions, error (RMSE) in original units\n",
    "    val_predictions = lr_model.predict(val_data[features]) * val_data['height'] * val_data['mass'] * 9.81\n",
    "    val_errors = abs(val_predictions - (val_data[target] * val_data['height'] * val_data['mass'] * 9.81))\n",
    "    val_rmse = root_mean_squared_error(\n",
    "        val_data[target] * val_data['height'] * val_data['mass'] * 9.81,\n",
    "        val_predictions\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        'model': lr_model,\n",
    "        'predictions': val_predictions,\n",
    "        'errors': val_errors,\n",
    "        'rmse': val_rmse\n",
    "    }\n",
    "\n",
    "# random forest model training\n",
    "    # val_data: subject data for validation\n",
    "def train_rf_model(\n",
    "        train_data: pd.DataFrame,\n",
    "        val_data: pd.DataFrame,\n",
    "        features: list,\n",
    "        target: str,\n",
    "        params: dict = {'n_estimators': 250, 'random_state': 42},\n",
    "        retrain: bool = False\n",
    "):\n",
    "    rf_model = RandomForestRegressor(**params)                            # initialize random forest model\n",
    "    \n",
    "    if retrain:\n",
    "        retrain_data = pd.concat([train_data, val_data])\n",
    "        rf_model.fit(retrain_data[features], retrain_data[target])\n",
    "    else:\n",
    "        rf_model.fit(train_data[features], train_data[target])\n",
    "\n",
    "    # get validation predictions, error (RMSE) in original units\n",
    "    val_predictions = rf_model.predict(val_data[features]) * val_data['height'] * val_data['mass'] * 9.81\n",
    "    val_errors = abs(val_predictions - (val_data[target] * val_data['height'] * val_data['mass'] * 9.81))\n",
    "    val_rmse = root_mean_squared_error(\n",
    "        val_data[target] * val_data['height'] * val_data['mass'] * 9.81,\n",
    "        val_predictions\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        'model': rf_model,\n",
    "        'predictions': val_predictions,\n",
    "        'errors': val_errors,\n",
    "        'rmse': val_rmse\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "id": "e135f1dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Holdout model for subject 2609 trained successfully.\n"
     ]
    }
   ],
   "source": [
    "# get list of all subjects (153 total)\n",
    "subject_list = model_data_final['subject_id'].unique()\n",
    "\n",
    "# initialize all results storage\n",
    "subject_results = {}\n",
    "model_errors = {\n",
    "    'baseline_lr': [],\n",
    "    'baseline_rf': [],\n",
    "    'eng_lr': [],\n",
    "    'eng_rf': []\n",
    "}\n",
    "model_rmse = {\n",
    "    'baseline_lr': [],\n",
    "    'baseline_rf': [],\n",
    "    'eng_lr': [],\n",
    "    'eng_rf': []\n",
    "}\n",
    "error_log = []\n",
    "\n",
    "# iterate through subjects\n",
    "for subject_id in subject_list[0:1]:\n",
    "    try:\n",
    "        print(f\"Training models for holdout subject {subject_id}...\", end='\\r', flush=True)\n",
    "        \n",
    "        # setup LOOCV training/validation data\n",
    "        train_data = model_data_final[model_data_final['subject_id'] != subject_id]\n",
    "        val_data = model_data_final[model_data_final['subject_id'] == subject_id]\n",
    "\n",
    "        # train models holding out subject\n",
    "        subject_summary = train_subject_models(train_data, val_data) \n",
    "        \n",
    "        # store results\n",
    "        subject_results[subject_id] = subject_summary\n",
    "        for model_name, model_info in subject_summary.items():\n",
    "            if model_info is not None:\n",
    "                model_errors[model_name].append(model_info['errors'])\n",
    "                model_rmse[model_name].append(model_info['rmse'])\n",
    "\n",
    "        # log error updates:\n",
    "        print(f\"Holdout model for subject {subject_id} trained successfully.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error training model for subject {subject_id}: {e}\")\n",
    "        error_log.append({\n",
    "            'subject_id': subject_id,\n",
    "            'error': str(e)\n",
    "        })\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "id": "584d45b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'baseline_lr': [12.91052935499846],\n",
       " 'baseline_rf': [4.857586109528131],\n",
       " 'eng_lr': [13.721727079228776],\n",
       " 'eng_rf': [5.331295288970638]}"
      ]
     },
     "execution_count": 411,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "id": "fec23e58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AWS]: Database connection closed.\n",
      "[AWS]: SSH tunnel stopped.\n"
     ]
    }
   ],
   "source": [
    "# close AWS connection\n",
    "aws_connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc625e20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pitch_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
