{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da6e12b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8dd92020",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load tensors from disk\n",
    "with open('storage/pitch_level_tensors.pkl', 'rb') as f:\n",
    "    pitch_level_tensors = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbaa32ab",
   "metadata": {},
   "source": [
    "$\\textbf{Model Architecture}$\n",
    "\n",
    "Both models use a __CNN-(bi)LSTM__ architecture:\n",
    "- The CNN layers progressively extract salient features\n",
    "- The bi-LSTM looks for temporal patterns that may distinguish injured and non-injured pitchers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "702a6789",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" LOSS FUNCTIONS \"\"\"\n",
    "def pitch_level_loss(\n",
    "        logits_step: torch.Tensor, \n",
    "        y_step: torch.Tensor, \n",
    "        mask: torch.Tensor, \n",
    "        pos_weight: bool = False\n",
    ") -> torch.Tensor:\n",
    "    \"\"\" \n",
    "    Compute pitch-level loss given ground truth. Valid for binary or smoothed (e.g., sigmoid) outcome labels.\n",
    "\n",
    "    Args:\n",
    "        logits_step (torch.Tensor): Logits from the model of shape [B, T].\n",
    "        y_step (torch.Tensor): Ground truth labels of shape [B, T]. Should be in [0, 1].\n",
    "        mask (torch.Tensor): Mask indicating valid time steps of shape [B, T].\n",
    "        pos_weight (bool, optional): Whether or not to use weights for positive class in BCE loss. Default is False.\n",
    "    \"\"\"\n",
    "    if pos_weight is None:\n",
    "        # setup weights\n",
    "        pos = y_step[mask].sum()\n",
    "        neg = mask.sum() - pos\n",
    "        pos_weight = neg / pos.clamp(min=1.0)\n",
    "        \n",
    "        return F.binary_cross_entropy_with_logits(logits_step[mask], y_step[mask])\n",
    "    \n",
    "    return F.binary_cross_entropy_with_logits(logits_step[mask], y_step[mask], pos_weight=pos_weight)\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_pos_weight(\n",
    "    train_loader: DataLoader, \n",
    "    device: torch.device = torch.device('cpu')\n",
    ") -> torch.Tensor:\n",
    "    pos = 0.0\n",
    "    tot = 0.0\n",
    "    \n",
    "    # iterate through training data to compute positive and total counts\n",
    "    for x, y, m, L in train_loader:\n",
    "        y, m = y.to(device), m.to(device)\n",
    "        pos += y[m].sum().item()\n",
    "        tot += m.sum().item()\n",
    "    \n",
    "    # compute positive weight\n",
    "    neg = max(tot - pos, 1.0)\n",
    "    pos = max(pos, 1.0)\n",
    "    \n",
    "    return torch.tensor(neg / pos, device=device, dtype=torch.float32)\n",
    "\n",
    "# masked softmax function\n",
    "    # computes softmax over scores while ignoring masked positions\n",
    "def masked_softmax(\n",
    "        scores: torch.Tensor, \n",
    "        mask: torch.Tensor, \n",
    "        dim: int = -1\n",
    ") -> torch.Tensor:\n",
    "    \"\"\" Compute masked softmax over scores.\"\"\"\n",
    "    # fill masked positions with -inf to ignore them in softmax\n",
    "    scores = scores.masked_fill(~mask, float('-inf'))\n",
    "    \n",
    "    # apply softmax to scores along the specified dimension\n",
    "    attn = torch.softmax(scores, dim=dim)\n",
    "    attn = attn * mask.float()  # make sure padded positions are 0 after softmax\n",
    "    \n",
    "    # compute denominator for normalization\n",
    "    denom = attn.sum(dim=dim, keepdim=True).clamp_min(1e-8)\n",
    "    \n",
    "    return attn / denom\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "971113ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" MODEL ARCHITECTURES \"\"\"\n",
    "# CNN block for local patterns\n",
    "class CNNBlock(nn.Module):\n",
    "    \"\"\" \n",
    "    Depthwise-separable 1D convolutional block over time with residual. \n",
    "\n",
    "    Args:\n",
    "        num_channels (int): Number of input channels.\n",
    "        kernel (int): Size of the convolutional kernel. Defaults to 7.\n",
    "        dropout (float): Dropout rate. Defaults to 0.1.\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    \n",
    "    **Note**: CNN expects tensor with shape [B, C, T]. \n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self, \n",
    "            num_channels: int, \n",
    "            kernel: int = 7, \n",
    "            dropout: float = 0.1\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        pad = kernel // 2\n",
    "        \n",
    "        # depthwise convolution\n",
    "        self.dw = nn.Conv1d(num_channels, num_channels, kernel_size=kernel, padding=pad, groups=num_channels)\n",
    "        self.pw = nn.Conv1d(num_channels, num_channels, kernel_size=1)\n",
    "        \n",
    "        # batch normalization, activation, and dropout\n",
    "        self.bn = nn.BatchNorm1d(num_channels)\n",
    "        self.act = nn.GELU()\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(\n",
    "            self, \n",
    "            x: torch.Tensor\n",
    "    ):  # x: [B,C,T]\n",
    "        residual = x\n",
    "        \n",
    "        # apply layers\n",
    "        x = self.dw(x)\n",
    "        x = self.pw(x)\n",
    "        \n",
    "        # batch normalization, activation, and dropout\n",
    "        x = self.bn(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        \n",
    "        return x + residual\n",
    "\n",
    "class CNNbiLSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    CNN + BiLSTM model for time series data with per-pitch head.\n",
    "\n",
    "    Args:\n",
    "        k_in (int): Number of input features (K).\n",
    "        stem (int): Number of channels in the stem layer. Defaults to 64.\n",
    "        c (int): Number of channels after projection. Defaults to 96.\n",
    "        kernel (int): Size of the convolutional kernel. Defaults to 7.\n",
    "        lstm_hidden (int): Hidden size for the LSTM layer. Defaults to 128.\n",
    "        dropout (float): Dropout rate. Defaults to 0.1.\n",
    "        bidir (bool): Whether to use a bidirectional LSTM. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "\n",
    "    **Note**: CNN expects tensor with shape [B, K, T]. \n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self, \n",
    "            k_in: int, \n",
    "            stem: int = 64, \n",
    "            c: int = 96, \n",
    "            kernel: int = 7, \n",
    "            lstm_hidden: int = 128, \n",
    "            dropout: float = 0.1, \n",
    "            bidir: bool = True\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        # 1x1 stem to mix K features into 'stem' channels\n",
    "        self.stem = nn.Sequential(\n",
    "            nn.Conv1d(k_in, stem, kernel_size=1),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "        \n",
    "        # progressive temporal convs\n",
    "        self.conv1 = CNNBlock(stem, kernel=kernel, dropout=dropout)\n",
    "        self.proj1 = nn.Conv1d(stem, c, kernel_size=1)                    # project to C channels\n",
    "        self.conv2 = CNNBlock(c, kernel=kernel, dropout=dropout)\n",
    "\n",
    "        # BiLSTM over time\n",
    "        self.lstm = nn.LSTM(input_size=c, hidden_size=lstm_hidden, batch_first=True, bidirectional=bidir)\n",
    "        hdim = lstm_hidden * (2 if bidir else 1)\n",
    "\n",
    "        # per-pitch head\n",
    "        self.head_step = nn.Sequential(\n",
    "            nn.LayerNorm(hdim),\n",
    "            nn.Linear(hdim, 1)\n",
    "        )\n",
    "\n",
    "        # attention scorer for pooling + sequence head\n",
    "        self.attn_scorer = nn.Linear(hdim, 1)\n",
    "        self.head_seq    = nn.Linear(hdim, 1)\n",
    "\n",
    "    def forward(\n",
    "            self, \n",
    "            x: torch.Tensor, \n",
    "            lengths: torch.Tensor,\n",
    "            mask: torch.Tensor\n",
    "    ) -> None:\n",
    "        # CNN expects [B,K,T]\n",
    "        x = x.transpose(1, 2)                  # [B,K,T]\n",
    "        x = self.stem(x)                       # [B,stem,T]\n",
    "        x = self.conv1(x)                      # [B,stem,T]\n",
    "        x = self.proj1(x)                      # [B,C,T]\n",
    "        x = self.conv2(x)                      # [B,C,T]\n",
    "        x = x.transpose(1, 2)                  # [B,T,C] for LSTM\n",
    "\n",
    "        # pack to ignore padding inside LSTM\n",
    "        packed = pack_padded_sequence(x, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        packed_out, _ = self.lstm(packed)\n",
    "        out, _ = pad_packed_sequence(packed_out, batch_first=True, total_length=x.size(1))    # [B,T,H]\n",
    "\n",
    "        # apply head to get logits for each pitch\n",
    "        logits_step = self.head_step(out).squeeze(-1)  # [B,T]\n",
    "\n",
    "        # attention pooling (maskâ€‘aware)\n",
    "        scores = self.attn_scorer(out).squeeze(-1)     # [B,T]\n",
    "        attn   = masked_softmax(scores, mask, dim=1)   # [B,T]\n",
    "        ctx    = (attn.unsqueeze(-1) * out).sum(dim=1) # [B,H]\n",
    "        logit_seq = self.head_seq(ctx).squeeze(-1)     # [B]\n",
    "        \n",
    "        return logits_step, logit_seq, attn \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8398c2",
   "metadata": {},
   "source": [
    "$\\textbf{Model Development (Beta)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "614316ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "eff07d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_masked_scalers(x, mask):\n",
    "    # x: [B,T,K], mask: [B,T]\n",
    "    m = mask.unsqueeze(-1)                  # [B,T,1]\n",
    "    num = m.sum(dim=(0,1)).clamp(min=1)     # [K]\n",
    "    \n",
    "    # compute mean, var, std\n",
    "    mean = (x * m).sum(dim=(0,1)) / num\n",
    "    var  = ((x - mean) * m).pow(2).sum(dim=(0,1)) / num\n",
    "    std  = var.sqrt().clamp(min=1e-6)\n",
    "    \n",
    "    return mean, std\n",
    "\n",
    "# normalize all splits in-place (or create new tensors)\n",
    "def apply_scalers(x, mean, std): \n",
    "    return (x - mean) / std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "244896a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([1207, 12104, 7]), Device: cpu\n",
      "pos_weight = 3.326164960861206\n"
     ]
    }
   ],
   "source": [
    "# set number of epochs, batch size\n",
    "NUM_EPOCHS = 5      # if none, run intil early stopping\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "## TODO: create compile_model() function --->\n",
    "\n",
    "# standardize example sequence --> example training tensor sequence (x)\n",
    "example_mean, example_std = compute_masked_scalers(pitch_level_tensors['trn']['seq'], pitch_level_tensors['trn']['mask'])\n",
    "x = apply_scalers(pitch_level_tensors['trn']['seq'], example_mean, example_std)\n",
    "\n",
    "# setup (device, shapes)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "B, T, K = x.shape\n",
    "print(f'Input shape: {x.shape}, Device: {device}')\n",
    "\n",
    "# move full tensors to device once (since weâ€™re not using a DataLoader yet)\n",
    "x               = x.float().to(device)\n",
    "y_step_binary   = pitch_level_tensors['trn']['binary'].float().to(device)\n",
    "mask            = pitch_level_tensors['trn']['mask'].bool().to(device)\n",
    "lengths         = pitch_level_tensors['trn']['lengths'].long().to(device)\n",
    "\n",
    "# setup model\n",
    "model = CNNbiLSTM(k_in=K, stem=64, c=96, kernel=7, lstm_hidden=128, dropout=0.1, bidir=True).to(device)\n",
    "\n",
    "# optimizer + class weights\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "\n",
    "# optional: compute pos_weight over valid steps once\n",
    "    # NOTE: for probs this should be 1.0\n",
    "with torch.no_grad():\n",
    "    pos = y_step_binary.sum()\n",
    "    neg = y_step_binary.shape[0] - pos\n",
    "    pos_weight = (neg / pos.clamp(min=1)).float()\n",
    "\n",
    "print(\"pos_weight =\", float(pos_weight))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43e84cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    # set training info\n",
    "    model.train()\n",
    "    idx = torch.randperm(B, device=device)\n",
    "\n",
    "    # loss counters\n",
    "    running_loss = 0.0\n",
    "    running_correct = 0\n",
    "    running_total = 0\n",
    "\n",
    "    # wrap range() with tqdm\n",
    "    pbar = tqdm(range(0, B, BATCH_SIZE), desc=f\"Epoch {epoch}/{NUM_EPOCHS}\", leave=False)\n",
    "    for start in pbar:\n",
    "        end = min(start + BATCH_SIZE, B)\n",
    "        bidx = idx[start:end]\n",
    "\n",
    "        # forward pass setup\n",
    "        xb = x[bidx]                        # [B,T,K]\n",
    "        yb = y_step_binary[bidx]       # [B] sequence-level labels (0/1)\n",
    "        mb = mask[bidx]                     # [B,T]\n",
    "        Lb = lengths[bidx]                  # [B]\n",
    "\n",
    "        # forward pass: get per-pitch + sequence logits\n",
    "        logits_step, logit_seq, attn = model(xb, Lb, mb)\n",
    "\n",
    "        # main sequence-level loss\n",
    "        loss = F.binary_cross_entropy_with_logits(\n",
    "            logit_seq, yb.float(), pos_weight=pos_weight\n",
    "        )\n",
    "\n",
    "        # back propagation\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        # running loss\n",
    "        bs = xb.size(0)\n",
    "        running_loss += loss.item() * bs\n",
    "        \n",
    "       # accuracy (sequence-level)\n",
    "        probs  = torch.sigmoid(logit_seq)\n",
    "        preds  = (probs > 0.5).float()\n",
    "        correct = (preds == yb).sum().item()\n",
    "        total   = yb.size(0)\n",
    "\n",
    "        # update total\n",
    "        running_correct += correct\n",
    "        running_total += total\n",
    "\n",
    "        # update accuracy\n",
    "        run_avg_loss = running_loss / ((start // BATCH_SIZE + 1) * bs)\n",
    "        run_acc      = running_correct / running_total\n",
    "\n",
    "        # show both current and running losses in the bar\n",
    "        pbar.set_postfix(loss=f\"{loss.item():.4f}\", acc=f\"{run_acc:.4f}\")\n",
    "\n",
    "    break\n",
    "\n",
    "    # update epoch loss\n",
    "    epoch_loss = running_loss / B\n",
    "    epoch_acc  = running_correct / running_total\n",
    "    \n",
    "    print(f\"Epoch {epoch:02d} | Training Loss: {epoch_loss:.3f} | Training Accuracy: {epoch_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b78707b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pitch_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
