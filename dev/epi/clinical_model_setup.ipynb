{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b019010f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from connections import AWS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6adbe8e8",
   "metadata": {},
   "source": [
    "$\\textbf{Epidemiology: Clinical Model Setup}$\n",
    "\n",
    "Injury risk estimation at the pitch-level using variable-length sequences. This notebook sets up the data structures and model architecture for development; the full, cleaned versions of each model are trained in `clinical_model_training.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3138f38a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AWS]: Port 5433 is free.\n",
      "[AWS]: Connected to RDS endpoint.\n"
     ]
    }
   ],
   "source": [
    "\"\"\" INITIALIZE AWS CONNECTION \"\"\"\n",
    "aws = AWS()\n",
    "aws.connect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112dc610",
   "metadata": {},
   "source": [
    "$\\textbf{Data Loading}$\n",
    "\n",
    "- Cohort data (matches, preds, statcast)\n",
    "- Ball flight aggregates "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8c5ee68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f33b6318",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function for calculating averages\n",
    "def get_avgs(\n",
    "        data: pd.DataFrame,\n",
    "        group_cols: list,\n",
    "        avg_cols: list = ['rel_speed', 'rel_side', 'rel_ht', 'spin_rate']\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate averages for specified columns grouped by the given columns.\n",
    "    \n",
    "    Args:\n",
    "        data (pd.DataFrame): The input DataFrame containing the data.\n",
    "        group_cols (list): List of columns to group by.\n",
    "        avg_cols (list): List of columns to calculate averages for. Defaults to ball flight features used in injury risk model.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame with the grouped columns and their corresponding averages.\n",
    "    \"\"\"\n",
    "    return data.groupby(group_cols)[avg_cols].mean().reset_index()\n",
    "\n",
    "# filter pitches to date range for a given ID\n",
    "def filter_pitches_by_date(\n",
    "        pitch_data: pd.DataFrame, \n",
    "        pitcher_id: int, \n",
    "        start_date: str, \n",
    "        end_date: str\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Filter pitch data for a specific pitcher within a date range.\n",
    "    \n",
    "    Args:\n",
    "        pitch_data (pd.DataFrame): The DataFrame containing pitch data.\n",
    "        pitcher_id (int): The ID of the pitcher to filter by.\n",
    "        start_date (str): The start date in 'YYYY-MM-DD' format.\n",
    "        end_date (str): The end date in 'YYYY-MM-DD' format.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Filtered DataFrame containing pitches for the specified pitcher and date range.\n",
    "    \"\"\"\n",
    "    return pitch_data[\n",
    "        (pitch_data['pitcher'] == pitcher_id) &\n",
    "        (pitch_data['game_date'] >= start_date) &\n",
    "        (pitch_data['game_date'] <= end_date)\n",
    "    ]\n",
    "\n",
    "# add 'outing_before_injury' column for outing-level model: 1 if injured pitcher and second-to-last outing in season, else 0\n",
    "def get_outing_before_injury(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = data.copy()\n",
    "    \n",
    "    # get unique outing dates per pitcher/season; find second-to-last outing\n",
    "    outing_dates = df.groupby(['pitcher', 'season'])['outing_number'].unique()\n",
    "    second_last_outing = outing_dates.apply(lambda x: sorted(x)[-2] if len(x) >= 2 else pd.NaT)\n",
    "    \n",
    "    second_last_df = pd.DataFrame(second_last_outing).reset_index()\n",
    "    second_last_df['outing_before_injury'] = 1\n",
    "\n",
    "    return second_last_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "765dede9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load/collect all matches\n",
    "n_matches = 5\n",
    "cohort_matches = aws.load_s3_object(f'epidemiology/cohorts/injured/pitcher_info/matches_{n_matches}_per_pitcher.csv')\n",
    "\n",
    "# organize all IDs w/ season\n",
    "cohort_info = []\n",
    "for _, row in cohort_matches.iterrows():\n",
    "    cohort_info.append({\n",
    "        'pitcher': row['mlbamid_injured'],\n",
    "        'season': row['season'],\n",
    "        'injured': 1\n",
    "    })\n",
    "\n",
    "    # append all non-injured pitchers\n",
    "    for mlbamid in ast.literal_eval(row['mlbamid_noninjured']):\n",
    "        cohort_info.append({\n",
    "            'pitcher': mlbamid,\n",
    "            'season': row['season'],\n",
    "            'injured': 0\n",
    "        })\n",
    "\n",
    "# concatenate all pitcher info\n",
    "cohort_info = pd.DataFrame(cohort_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01d272f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all statcast data & model generated predictions\n",
    "    # likely fts: velo, release position, spin rate (no pitch labels)\n",
    "statcast_data = aws.load_s3_object('epidemiology/ml/datasets/full/model_application_data.csv')\n",
    "statcast_preds = aws.load_s3_object('epidemiology/ml/datasets/preds/model_application.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "90dce5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge statcast data with model predictions\n",
    "    # create season column\n",
    "statcast_full = statcast_data.merge(statcast_preds, on=['pitch_id', 'pitcher', 'game_date', 'pitcher_days_since_prev_game', 'injured_cohort_pitcher'])\n",
    "statcast_full['season'] = statcast_full['game_date'].str[0:4].astype(int)\n",
    "\n",
    "# clip to last 90 days for each pitcher\n",
    "pitcher_last_outings = statcast_full.groupby(['pitcher', 'season'])['game_date'].max().reset_index()\n",
    "pitcher_last_outings.rename(columns={'game_date': 'last_outing_date'}, inplace=True)\n",
    "\n",
    "# merge last outing date w/ statcast data\n",
    "statcast_full = statcast_full.merge(pitcher_last_outings, on=['pitcher', 'season'], how='left')\n",
    "\n",
    "# compute time until last outing\n",
    "statcast_full['days_until_last_outing'] = (\n",
    "    pd.to_datetime(statcast_full['last_outing_date']) -\n",
    "    pd.to_datetime(statcast_full['game_date'])\n",
    ").dt.days\n",
    "\n",
    "# filter to last 90 days\n",
    "statcast_full_45 = statcast_full[statcast_full['days_until_last_outing'] <= 45].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be01f0ec",
   "metadata": {},
   "source": [
    "$\\textit{Baseline Ball Flight Averages by Pitcher}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "65134869",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get baseline season-long ball flight averages for reference\n",
    "    # NOTE: these may be referenced for the pitch level model\n",
    "ball_flight_fts = ['rel_speed', 'rel_side', 'rel_ht', 'spin_rate']\n",
    "ball_flight_season_avgs = get_avgs(\n",
    "    statcast_full_45,\n",
    "    group_cols=['pitcher', 'season'],\n",
    "    avg_cols=ball_flight_fts\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0782525b",
   "metadata": {},
   "source": [
    "$\\textbf{Train/Test Splits}$\n",
    "\n",
    "Loads previously set train/test splits (see `clinical_splits.ipynb`) and applies to `cohort_preds_final`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "bfd914d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load splits\n",
    "path_stem = 'epidemiology/ml/datasets/full'\n",
    "cohort_train_ids = aws.load_s3_object(f'{path_stem}/cohort_train_ids.csv')\n",
    "cohort_test_ids = aws.load_s3_object(f'{path_stem}/cohort_test_ids.csv')\n",
    "\n",
    "# filter full dataset by train/test\n",
    "cohort_preds_train = statcast_full_45.merge(\n",
    "    cohort_train_ids,\n",
    "    on=['pitcher', 'season'],\n",
    "    how='inner'\n",
    ")\n",
    "cohort_preds_test = statcast_full_45.merge(\n",
    "    cohort_test_ids,\n",
    "    on=['pitcher', 'season'],\n",
    "    how='inner'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b863452c",
   "metadata": {},
   "source": [
    "$\\textbf{Setup Data Sequences}$\n",
    "\n",
    "Two features are manually created:\n",
    "- Outing number\n",
    "- Within outing cumulative EVT workload\n",
    "\n",
    "All sequences must then be converted to arrays (then tensors) for model development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "c0d9690b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "ca112c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create outing number\n",
    "def create_outing_number(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create an outing number for each pitcher in the dataset.\n",
    "    \n",
    "    Args:\n",
    "        data (pd.DataFrame): The DataFrame containing pitch data.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with an additional 'outing_number' column.\n",
    "    \"\"\"\n",
    "    df = data.copy()\n",
    "    df['outing_number'] = df.groupby(['pitcher', 'season'])['game_date'].rank(method='dense').astype(int)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# create within outing cumulative EVT workload\n",
    "def create_within_outing_cumulative_evt_workload(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create a cumulative EVT workload for each outing.\n",
    "    \n",
    "    Args:\n",
    "        data (pd.DataFrame): The DataFrame containing pitch data.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with an additional 'within_outing_cumulative_evt_workload' column.\n",
    "    \"\"\"\n",
    "    df = data.copy()\n",
    "    df['within_outing_cumulative_evt_workload'] = df.groupby(['pitcher', 'season', 'outing_number'])['pred_peak_evt_normalized'].cumsum()\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "1acfc942",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create addt'l features\n",
    "trn_final = create_within_outing_cumulative_evt_workload(\n",
    "    create_outing_number(cohort_preds_train)\n",
    ")\n",
    "test_final = create_within_outing_cumulative_evt_workload(\n",
    "    create_outing_number(cohort_preds_test)\n",
    ")\n",
    "\n",
    "# sort by pitcher, season, outing number\n",
    "trn_final = trn_final.sort_values(by=['pitcher', 'season', 'outing_number'])\n",
    "test_final = test_final.sort_values(by=['pitcher', 'season', 'outing_number'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "b96163b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set model features -- contextual & time series layers\n",
    "CONTEXTUAL_FTS = [\n",
    "    'p_throws',\n",
    "    'pitcher_days_since_prev_game',\n",
    "    # 'outing_number'\n",
    "]\n",
    "TIME_SERIES_FTS = [\n",
    "    # predicted load\n",
    "    'pred_peak_evt_normalized',\n",
    "    # 'within_outing_cumulative_evt_workload',\n",
    "    \n",
    "    # ball flight\n",
    "    'rel_speed', \n",
    "    'rel_side',\n",
    "    'rel_ht', \n",
    "    'spin_rate'\n",
    "]\n",
    "\n",
    "# combine for full features\n",
    "feature_set = CONTEXTUAL_FTS + TIME_SERIES_FTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "4b910483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training and testing set arrays\n",
    "trn_arrays = np.array([rows[feature_set].values for _, rows in trn_final.groupby(['pitcher', 'season'])])\n",
    "test_arrays = np.array([rows[feature_set].values for _, rows in test_final.groupby(['pitcher', 'season'])])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada5d9df",
   "metadata": {},
   "source": [
    "$\\textbf{Create Outcomes}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "a6f2561c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "d5c22ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create outcome arrays -- 1 per pitcher-season (ie., sequence)\n",
    "trn_outcomes = trn_final[['pitcher', 'season', 'injured']].drop_duplicates()['injured'].values\n",
    "test_outcomes = test_final[['pitcher', 'season', 'injured']].drop_duplicates()['injured'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ac962e",
   "metadata": {},
   "source": [
    "$\\textbf{Aggregate for Streamlined Data Storage}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "f80ba5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "17af64ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset all into a dictionary\n",
    "pitch_level = {\n",
    "    'trn': {\n",
    "        'inputs': trn_arrays,\n",
    "        'outcomes': trn_outcomes\n",
    "    },\n",
    "    'val': {\n",
    "        'inputs': trn_arrays, \n",
    "        'outcomes': trn_outcomes\n",
    "    }\n",
    "}\n",
    "\n",
    "# save to local disk\n",
    "with open('storage/pitch_level_arrays.pkl', 'wb') as f:\n",
    "    pickle.dump(pitch_level, f)\n",
    "\n",
    "# TODO: upload to AWS (--> pytorch folder)\n",
    "# with open('storage/pitch_level_arrays.pkl', 'rb') as f:\n",
    "#     content = f.read()\n",
    "#     aws.upload_to_s3(content, 'epidemiology/ml/datasets/pytorch/pitch_level_arrays.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb76b05",
   "metadata": {},
   "source": [
    "$\\textbf{Tensor Setup}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "05a91689",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "d5624ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to tensors and pad to the same length\n",
    "def create_padded_tensor(sequences: list) -> torch.Tensor:\n",
    "    \"\"\" \n",
    "    Convert a list of sequences to a padded tensor.\n",
    "    \n",
    "    Args:\n",
    "        sequences (list): A list of sequences (arrays) to be converted.\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: A padded tensor of shape (batch, max_seq_len, features).\n",
    "    \"\"\"\n",
    "    seq_tensors = [torch.tensor(np.array(seq, dtype=np.float32)) for seq in sequences]\n",
    "    padded = pad_sequence(seq_tensors, batch_first=True, padding_value=0)        # shape: (batch, max_seq_len, features)\n",
    "\n",
    "    return padded\n",
    "\n",
    "# helper function for creating sorted tensor dictionaries\n",
    "def setup_all_tensors(\n",
    "        input_arrays: dict,\n",
    "        output_arrays: dict,\n",
    "        pad_outcomes: bool = False\n",
    ") -> dict:\n",
    "    \"\"\" Helper function for creating sorted (pitch- or outing-level) tensor dictionaries. Returns sequences, mask, and lengths for each. \"\"\"\n",
    "    # setup tensor dictionaries\n",
    "    sorted_tensors = {\n",
    "        'seq': None,\n",
    "        'mask': None,\n",
    "        'lengths': None,\n",
    "        'probs': None,\n",
    "        'binary': None\n",
    "    }\n",
    "\n",
    "    # create tensors\n",
    "        # shape: (batch, max_seq_len, features)\n",
    "        # also add mask and lengths to denote actual values for training\n",
    "    sorted_tensors['seq'] = create_padded_tensor(input_arrays)\n",
    "    sorted_tensors['mask'] = (sorted_tensors['seq'].abs().sum(dim=2) != 0) \n",
    "    sorted_tensors['lengths'] = sorted_tensors['mask'].sum(dim=1)\n",
    "\n",
    "    # convert outcomes to padded tensors\n",
    "    if pad_outcomes:\n",
    "        sorted_tensors['probs'] = create_padded_tensor(output_arrays['probs'])\n",
    "        sorted_tensors['binary'] = create_padded_tensor(output_arrays['binary'])\n",
    "    else:\n",
    "        sorted_tensors['binary'] = torch.tensor(output_arrays, dtype=torch.float32)\n",
    "\n",
    "    return sorted_tensors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "d69d0e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create tensors from arrays\n",
    "pitch_level_tensors = {\n",
    "    'trn': None,\n",
    "    'val': None\n",
    "}\n",
    "pitch_level_tensors['trn'] = setup_all_tensors(pitch_level['trn']['inputs'], pitch_level['trn']['outcomes'])\n",
    "pitch_level_tensors['val'] = setup_all_tensors(pitch_level['val']['inputs'], pitch_level['val']['outcomes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "00db9a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to local disk\n",
    "with open('storage/pitch_level_tensors.pkl', 'wb') as f:\n",
    "    pickle.dump(pitch_level_tensors, f)\n",
    "\n",
    "# TODO: upload to AWS (--> pytorch folder)\n",
    "# with open('storage/pitch_level_tensors.pkl', 'rb') as f:\n",
    "#     content = f.read()\n",
    "#     aws.upload_to_s3(content, 'epidemiology/ml/datasets/pytorch/pitch_level_tensors.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569efdfd",
   "metadata": {},
   "source": [
    "$\\textbf{Close AWS Connection}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "dcaccd92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AWS]: No active connection to close.\n"
     ]
    }
   ],
   "source": [
    "aws.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1bb2c2",
   "metadata": {},
   "source": [
    "$\\textbf{Sandbox: Development}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "8b3b66ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pred_peak_evt_normalized',\n",
       " 'within_outing_cumulative_evt_workload',\n",
       " 'rel_speed',\n",
       " 'rel_side',\n",
       " 'rel_ht',\n",
       " 'spin_rate',\n",
       " 'p_throws',\n",
       " 'pitcher_days_since_prev_game',\n",
       " 'outing_number']"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list of features for reference\n",
    "TIME_SERIES_FTS + CONTEXTUAL_FTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b18a259",
   "metadata": {},
   "source": [
    "$\\textit{Example Training Sequence}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c3263d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize example sequence --> example training tensor sequence (x)\n",
    "example_mean, example_std = compute_masked_scalers(pitch_level_tensors[7]['seq'], pitch_level_tensors[7]['mask'])\n",
    "x = apply_scalers(pitch_level_tensors[7]['seq'], example_mean, example_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "6b4bddf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([401, 348, 9]), Device: cpu\n"
     ]
    }
   ],
   "source": [
    "# setup (device, shapes)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "B, T, K = x.shape\n",
    "print(f'Input shape: {x.shape}, Device: {device}')\n",
    "\n",
    "# move full tensors to device once (since we’re not using a DataLoader yet)\n",
    "x               = x.float().to(device)\n",
    "y_step          = pitch_level_outcomes[7]['probs'].float().to(device)\n",
    "y_step_binary   = pitch_level_outcomes[7]['binary'].float().to(device)\n",
    "mask            = pitch_level_tensors[7]['mask'].bool().to(device)\n",
    "lengths         = pitch_level_tensors[7]['lengths'].long().to(device)\n",
    "\n",
    "# setup model\n",
    "model = CNNbiLSTM(k_in=K, stem=64, c=96, kernel=7, lstm_hidden=128, dropout=0.1, bidir=True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "833a12a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_weight = 1.0\n"
     ]
    }
   ],
   "source": [
    "# optimizer + class weights\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "\n",
    "# optional: compute pos_weight over valid steps once\n",
    "    # NOTE: for probs this should be 1.0\n",
    "with torch.no_grad():\n",
    "    pos = y_step[mask].sum()\n",
    "    tot = mask.sum()\n",
    "    neg = tot - pos\n",
    "    pos_weight = (neg / pos.clamp(min=1)).float()\n",
    "print(\"pos_weight =\", float(pos_weight))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74687c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 01 | train_loss 0.7532\n"
     ]
    }
   ],
   "source": [
    "epochs     = 1\n",
    "batch_size = 32\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    model.train()\n",
    "\n",
    "    # shuffle indices each epoch\n",
    "    idx = torch.randperm(B, device=device)\n",
    "\n",
    "    running_loss = 0.0\n",
    "    seen = 0\n",
    "\n",
    "    # iterate through mini-batches\n",
    "    for start in range(0, B, batch_size):\n",
    "        end = min(start + batch_size, B)\n",
    "        bidx = idx[start:end]                    # [b]\n",
    "\n",
    "        xb = x[bidx]                             # [b,T,K]\n",
    "        yb = y_step[bidx]                        # [b,T]\n",
    "        mb = mask[bidx]                          # [b,T]\n",
    "        Lb = lengths[bidx].float()                       # [b]\n",
    "\n",
    "        # forward\n",
    "        logits = model(xb, Lb)                   # [b,T]\n",
    "\n",
    "        # masked BCE with optional pos_weight\n",
    "        loss = F.binary_cross_entropy_with_logits(\n",
    "            logits[mb], yb[mb],\n",
    "            pos_weight=pos_weight\n",
    "        )\n",
    "\n",
    "        # back propagation\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        # update running loss\n",
    "        running_loss += loss.item() * xb.size(0)\n",
    "        seen += xb.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / max(seen, 1)\n",
    "    print(f\"Epoch {epoch:02d} | Training Loss: {epoch_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "398fa85e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per-pitch accuracy = 0.561\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    logits = model(x, lengths)             # [B,T]\n",
    "    probs  = torch.sigmoid(logits)         # [B,T]\n",
    "\n",
    "    # mask out padded steps\n",
    "    preds  = (probs >= 0.5).float()[mask]  # [N_valid]\n",
    "    labels = y_step_binary[mask]                  # [N_valid]\n",
    "\n",
    "    acc = (preds == labels).float().mean().item()\n",
    "    \n",
    "    print(f\"Per-pitch accuracy = {acc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "00741fc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per-pitch AUC = 0.653\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "y_true = labels.cpu().numpy()\n",
    "y_score = probs[mask].cpu().numpy()\n",
    "auc = roc_auc_score(y_true, y_score) if y_true.min() < y_true.max() else float(\"nan\")\n",
    "print(f\"Per-pitch AUC = {auc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "d1c04122",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8792)"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "4830d02b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0.,  ..., 1., 1., 1.])"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "f64c1bdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 1.,  ..., 1., 1., 1.])"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c174afe",
   "metadata": {},
   "source": [
    "$\\textit{Outcome Handling}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08009297",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gather outcomes\n",
    "    # non-injured pitcher --> outcomes = 0\n",
    "    # injured pitcher --> outcomes = ...\n",
    "example = outing_level_sequences[7]\n",
    "example_outcomes = example[['pitcher', 'season', 'outing_id', 'injured_cohort_pitcher']].drop_duplicates().reset_index(drop=True).copy()\n",
    "test_probs, test_binary = update_outcome_probabilities(example, model_type='outing_level')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5d3aa612",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_pitches = pitch_level_sequences[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c013fa6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate through all pitcher-pitches in the datespan\n",
    "for group, rows in example_pitches.groupby(['pitcher', 'season']):\n",
    "    rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "a2d9cec9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00247262, 0.00273425, 0.00302348, 0.0033432 , 0.0036966 ,\n",
       "       0.00408721, 0.0045189 , 0.00499596, 0.0055231 , 0.00610552,\n",
       "       0.00674895, 0.00745967, 0.00824462, 0.0091114 , 0.01006839,\n",
       "       0.01112476, 0.01229059, 0.01357692, 0.01499583, 0.01656054,\n",
       "       0.01828548, 0.02018641, 0.02228046, 0.0245863 , 0.02712415,\n",
       "       0.02991593, 0.03298531, 0.03635781, 0.04006084, 0.04412375,\n",
       "       0.04857786, 0.05345645, 0.05879472, 0.06462967, 0.07100002,\n",
       "       0.07794595, 0.08550885, 0.09373097, 0.10265494, 0.11232324,\n",
       "       0.12277754, 0.1340579 , 0.14620194, 0.15924378, 0.17321298,\n",
       "       0.18813338, 0.20402187, 0.22088711, 0.23872837, 0.25753434,\n",
       "       0.27728217, 0.29793663, 0.31944957, 0.3417597 , 0.36479277,\n",
       "       0.38846205, 0.41266938, 0.43730648, 0.46225681, 0.48739763,\n",
       "       0.51260237, 0.53774319, 0.56269352, 0.58733062, 0.61153795,\n",
       "       0.63520723, 0.6582403 , 0.68055043, 0.70206337, 0.72271783,\n",
       "       0.74246566, 0.76127163, 0.77911289, 0.79597813, 0.81186662,\n",
       "       0.82678702, 0.84075622, 0.85379806, 0.8659421 , 0.87722246,\n",
       "       0.88767676, 0.89734506, 0.90626903, 0.91449115, 0.92205405,\n",
       "       0.92899998, 0.93537033, 0.94120528, 0.94654355, 0.95142214,\n",
       "       0.95587625, 0.95993916, 0.96364219, 0.96701469, 0.97008407,\n",
       "       0.97287585, 0.9754137 , 0.97771954, 0.97981359, 0.98171452,\n",
       "       0.98343946, 0.98500417, 0.98642308, 0.98770941, 0.98887524,\n",
       "       0.98993161, 0.9908886 , 0.99175538, 0.99254033, 0.99325105,\n",
       "       0.99389448, 0.9944769 , 0.99500404, 0.9954811 , 0.99591279,\n",
       "       0.9963034 , 0.9966568 , 0.99697652, 0.99726575, 0.99752738])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(1 / (1 + np.exp(-np.linspace(-6, 6, rows.shape[0]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667756c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pitch_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
