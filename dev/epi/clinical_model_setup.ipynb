{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b019010f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from connections import AWS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6adbe8e8",
   "metadata": {},
   "source": [
    "$\\textbf{Epidemiology: Clinical Model Setup}$\n",
    "\n",
    "Injury risk estimation at the pitch-level using variable-length sequences. This notebook sets up the data structures and model architecture for development; the full, cleaned versions of each model are trained in `clinical_model_training.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3138f38a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AWS]: Port 5433 is in use by process python3.11 (PID 27916). Killing it.\n",
      "[AWS]: Connected to RDS endpoint.\n"
     ]
    }
   ],
   "source": [
    "\"\"\" INITIALIZE AWS CONNECTION \"\"\"\n",
    "aws = AWS()\n",
    "aws.connect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112dc610",
   "metadata": {},
   "source": [
    "$\\textbf{Data Loading}$\n",
    "\n",
    "- Cohort data (matches, preds, statcast)\n",
    "- Ball flight aggregates "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c5ee68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f33b6318",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function for calculating averages\n",
    "def get_avgs(\n",
    "        data: pd.DataFrame,\n",
    "        group_cols: list,\n",
    "        avg_cols: list = ['rel_speed', 'rel_side', 'rel_ht', 'spin_rate']\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate averages for specified columns grouped by the given columns.\n",
    "    \n",
    "    Args:\n",
    "        data (pd.DataFrame): The input DataFrame containing the data.\n",
    "        group_cols (list): List of columns to group by.\n",
    "        avg_cols (list): List of columns to calculate averages for. Defaults to ball flight features used in injury risk model.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame with the grouped columns and their corresponding averages.\n",
    "    \"\"\"\n",
    "    return data.groupby(group_cols)[avg_cols].mean().reset_index()\n",
    "\n",
    "# filter pitches to date range for a given ID\n",
    "def filter_pitches_by_date(\n",
    "        pitch_data: pd.DataFrame, \n",
    "        pitcher_id: int, \n",
    "        start_date: str, \n",
    "        end_date: str\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Filter pitch data for a specific pitcher within a date range.\n",
    "    \n",
    "    Args:\n",
    "        pitch_data (pd.DataFrame): The DataFrame containing pitch data.\n",
    "        pitcher_id (int): The ID of the pitcher to filter by.\n",
    "        start_date (str): The start date in 'YYYY-MM-DD' format.\n",
    "        end_date (str): The end date in 'YYYY-MM-DD' format.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Filtered DataFrame containing pitches for the specified pitcher and date range.\n",
    "    \"\"\"\n",
    "    return pitch_data[\n",
    "        (pitch_data['pitcher'] == pitcher_id) &\n",
    "        (pitch_data['game_date'] >= start_date) &\n",
    "        (pitch_data['game_date'] <= end_date)\n",
    "    ]\n",
    "\n",
    "# add 'outing_before_injury' column for outing-level model: 1 if injured pitcher and second-to-last outing in season, else 0\n",
    "def get_outing_before_injury(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = data.copy()\n",
    "    \n",
    "    # get unique outing dates per pitcher/season; find second-to-last outing\n",
    "    outing_dates = df.groupby(['pitcher', 'season'])['outing_number'].unique()\n",
    "    second_last_outing = outing_dates.apply(lambda x: sorted(x)[-2] if len(x) >= 2 else pd.NaT)\n",
    "    \n",
    "    second_last_df = pd.DataFrame(second_last_outing).reset_index()\n",
    "    second_last_df['outing_before_injury'] = 1\n",
    "\n",
    "    return second_last_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "765dede9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load/collect all matches\n",
    "n_matches = 5\n",
    "cohort_matches = aws.load_s3_object(f'epidemiology/cohorts/injured/pitcher_info/matches_{n_matches}_per_pitcher.csv')\n",
    "\n",
    "# organize all IDs w/ season\n",
    "cohort_info = []\n",
    "for _, row in cohort_matches.iterrows():\n",
    "    cohort_info.append({\n",
    "        'pitcher': row['mlbamid_injured'],\n",
    "        'season': row['season'],\n",
    "        'injured': 1\n",
    "    })\n",
    "\n",
    "    # append all non-injured pitchers\n",
    "    for mlbamid in ast.literal_eval(row['mlbamid_noninjured']):\n",
    "        cohort_info.append({\n",
    "            'pitcher': mlbamid,\n",
    "            'season': row['season'],\n",
    "            'injured': 0\n",
    "        })\n",
    "\n",
    "# concatenate all pitcher info\n",
    "cohort_info = pd.DataFrame(cohort_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d01d272f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all statcast data & model generated predictions\n",
    "    # likely fts: velo, release position, spin rate (no pitch labels)\n",
    "statcast_data = aws.load_s3_object('epidemiology/ml/datasets/full/model_application_data.csv')\n",
    "statcast_preds = aws.load_s3_object('epidemiology/ml/datasets/preds/model_application.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90dce5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge statcast data with model predictions\n",
    "    # create season column\n",
    "statcast_full = statcast_data.merge(statcast_preds, on=['pitch_id', 'pitcher', 'game_date', 'pitcher_days_since_prev_game', 'injured_cohort_pitcher'])\n",
    "statcast_full['season'] = statcast_full['game_date'].str[0:4].astype(int)\n",
    "\n",
    "# clip to last 90 days for each pitcher\n",
    "pitcher_last_outings = statcast_full.groupby(['pitcher', 'season'])['game_date'].max().reset_index()\n",
    "pitcher_last_outings.rename(columns={'game_date': 'last_outing_date'}, inplace=True)\n",
    "\n",
    "# merge last outing date w/ statcast data\n",
    "statcast_full = statcast_full.merge(pitcher_last_outings, on=['pitcher', 'season'], how='left')\n",
    "\n",
    "# compute time until last outing\n",
    "statcast_full['days_until_last_outing'] = (\n",
    "    pd.to_datetime(statcast_full['last_outing_date']) -\n",
    "    pd.to_datetime(statcast_full['game_date'])\n",
    ").dt.days\n",
    "\n",
    "# filter to last 90 days\n",
    "statcast_full_45 = statcast_full[statcast_full['days_until_last_outing'] <= 45].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be01f0ec",
   "metadata": {},
   "source": [
    "$\\textit{Baseline Ball Flight Averages by Pitcher}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65134869",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get baseline season-long ball flight averages for reference\n",
    "    # NOTE: these may be referenced for the pitch level model\n",
    "ball_flight_fts = ['rel_speed', 'rel_side', 'rel_ht', 'spin_rate']\n",
    "ball_flight_season_avgs = get_avgs(\n",
    "    statcast_full_45,\n",
    "    group_cols=['pitcher', 'season'],\n",
    "    avg_cols=ball_flight_fts\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0782525b",
   "metadata": {},
   "source": [
    "$\\textbf{Train/Test Splits}$\n",
    "\n",
    "Loads previously set train/test splits (see `clinical_splits.ipynb`) and applies to `cohort_preds_final`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bfd914d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load splits\n",
    "path_stem = 'epidemiology/ml/datasets/full'\n",
    "cohort_train_ids = aws.load_s3_object(f'{path_stem}/cohort_train_ids.csv')\n",
    "cohort_test_ids = aws.load_s3_object(f'{path_stem}/cohort_test_ids.csv')\n",
    "\n",
    "# filter full dataset by train/test\n",
    "cohort_preds_train = statcast_full_45.merge(\n",
    "    cohort_train_ids,\n",
    "    on=['pitcher', 'season'],\n",
    "    how='inner'\n",
    ")\n",
    "cohort_preds_test = statcast_full_45.merge(\n",
    "    cohort_test_ids,\n",
    "    on=['pitcher', 'season'],\n",
    "    how='inner'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ee5b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save test data for post-eval\n",
    "cohort_preds_test.to_csv('storage/model_test_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b863452c",
   "metadata": {},
   "source": [
    "$\\textbf{Setup Data Sequences}$\n",
    "\n",
    "Two features are manually created:\n",
    "- Outing number\n",
    "- Within outing cumulative EVT workload\n",
    "\n",
    "All sequences must then be converted to arrays (then tensors) for model development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c0d9690b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca112c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create outing number\n",
    "def create_outing_number(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create an outing number for each pitcher in the dataset.\n",
    "    \n",
    "    Args:\n",
    "        data (pd.DataFrame): The DataFrame containing pitch data.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with an additional 'outing_number' column.\n",
    "    \"\"\"\n",
    "    df = data.copy()\n",
    "    df['outing_number'] = df.groupby(['pitcher', 'season'])['game_date'].rank(method='dense').astype(int)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# create within outing cumulative EVT workload\n",
    "def create_within_outing_cumulative_evt_workload(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create a cumulative EVT workload for each outing.\n",
    "    \n",
    "    Args:\n",
    "        data (pd.DataFrame): The DataFrame containing pitch data.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with an additional 'within_outing_cumulative_evt_workload' column.\n",
    "    \"\"\"\n",
    "    df = data.copy()\n",
    "    df['within_outing_cumulative_evt_workload'] = df.groupby(['pitcher', 'season', 'outing_number'])['pred_peak_evt_normalized'].cumsum()\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1acfc942",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create addt'l features\n",
    "trn_final = create_within_outing_cumulative_evt_workload(\n",
    "    create_outing_number(cohort_preds_train)\n",
    ")\n",
    "test_final = create_within_outing_cumulative_evt_workload(\n",
    "    create_outing_number(cohort_preds_test)\n",
    ")\n",
    "\n",
    "# sort by pitcher, season, outing number\n",
    "trn_final = trn_final.sort_values(by=['pitcher', 'season', 'outing_number'])\n",
    "test_final = test_final.sort_values(by=['pitcher', 'season', 'outing_number'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b96163b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set model features -- contextual & time series layers\n",
    "CONTEXTUAL_FTS = [\n",
    "    'p_throws',\n",
    "    'pitcher_days_since_prev_game',\n",
    "    # 'outing_number'\n",
    "]\n",
    "TIME_SERIES_FTS = [\n",
    "    # predicted load\n",
    "    'pred_peak_evt_normalized',\n",
    "    # 'within_outing_cumulative_evt_workload',\n",
    "    \n",
    "    # ball flight\n",
    "    'rel_speed', \n",
    "    'rel_side',\n",
    "    'rel_ht', \n",
    "    'spin_rate'\n",
    "]\n",
    "\n",
    "# combine for full features\n",
    "feature_set = CONTEXTUAL_FTS + TIME_SERIES_FTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4b910483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training and testing set arrays\n",
    "trn_arrays = np.array([rows[feature_set].values for _, rows in trn_final.groupby(['pitcher', 'season'])])\n",
    "test_arrays = np.array([rows[feature_set].values for _, rows in test_final.groupby(['pitcher', 'season'])])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada5d9df",
   "metadata": {},
   "source": [
    "$\\textbf{Create Outcomes}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a6f2561c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "90c10a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wehther or not to label outcomes by pitch\n",
    "PER_PITCH = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d5c22ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create outcome arrays -- one per pitcher\n",
    "if not PER_PITCH:\n",
    "    trn_outcomes = trn_final[['pitcher', 'season', 'injured']].drop_duplicates()['injured'].values\n",
    "    test_outcomes = test_final[['pitcher', 'season', 'injured']].drop_duplicates()['injured'].values\n",
    "    # convert to 1D arrays\n",
    "    \n",
    "\n",
    "else:\n",
    "    # create outcome arrays -- per-pitch\n",
    "        # NOTE: all pitches in the last outing are labeled as \"injured\"\n",
    "        # group by pitcher & season and store outcome arrays separately\n",
    "    trn_outcomes = [\n",
    "        np.where(\n",
    "            (group['injured'] == 1) & (group['days_until_last_outing'] == 0),\n",
    "            1,\n",
    "            0\n",
    "        )\n",
    "        for _, group in trn_final.groupby(['pitcher', 'season'])\n",
    "    ]\n",
    "    test_outcomes = [\n",
    "        np.where(\n",
    "            (group['injured'] == 1) & (group['days_until_last_outing'] == 0),\n",
    "            1,\n",
    "            0\n",
    "        )\n",
    "        for _, group in test_final.groupby(['pitcher', 'season'])\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ac962e",
   "metadata": {},
   "source": [
    "$\\textbf{Aggregate for Streamlined Data Storage}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f80ba5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "17af64ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset all into a dictionary\n",
    "pitch_level = {\n",
    "    'trn': {\n",
    "        'inputs': trn_arrays,\n",
    "        'outcomes': np.array(trn_outcomes)\n",
    "    },\n",
    "    'val': {\n",
    "        'inputs': test_arrays, \n",
    "        'outcomes': np.array(test_outcomes)\n",
    "    }\n",
    "}\n",
    "\n",
    "# save to local disk\n",
    "with open('storage/pitch_level_arrays.pkl', 'wb') as f:\n",
    "    pickle.dump(pitch_level, f)\n",
    "\n",
    "# TODO: upload to AWS (--> pytorch folder)\n",
    "# with open('storage/pitch_level_arrays.pkl', 'rb') as f:\n",
    "#     content = f.read()\n",
    "#     aws.upload_to_s3(content, 'epidemiology/ml/datasets/pytorch/pitch_level_arrays.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb76b05",
   "metadata": {},
   "source": [
    "$\\textbf{Tensor Setup}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "05a91689",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "d5624ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to tensors and pad to the same length\n",
    "def create_padded_tensor(sequences: list) -> torch.Tensor:\n",
    "    \"\"\" \n",
    "    Convert a list of sequences to a padded tensor.\n",
    "    \n",
    "    Args:\n",
    "        sequences (list): A list of sequences (arrays) to be converted.\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: A padded tensor of shape (batch, max_seq_len, features).\n",
    "    \"\"\"\n",
    "    seq_tensors = [torch.tensor(np.array(seq, dtype=np.float32)) for seq in sequences]\n",
    "    padded = pad_sequence(seq_tensors, batch_first=True, padding_value=0)        # shape: (batch, max_seq_len, features)\n",
    "\n",
    "    return padded\n",
    "\n",
    "# helper function for creating sorted tensor dictionaries\n",
    "def setup_all_tensors(\n",
    "        input_arrays: dict,\n",
    "        output_arrays: dict,\n",
    "        pad_outcomes: bool = False\n",
    ") -> dict:\n",
    "    \"\"\" Helper function for creating sorted (pitch- or outing-level) tensor dictionaries. Returns sequences, mask, and lengths for each. \"\"\"\n",
    "    # setup tensor dictionaries\n",
    "    sorted_tensors = {\n",
    "        'seq': None,\n",
    "        'mask': None,\n",
    "        'lengths': None,\n",
    "        'binary': None\n",
    "    }\n",
    "\n",
    "    # create tensors\n",
    "        # shape: (batch, max_seq_len, features)\n",
    "        # also add mask and lengths to denote actual values for training\n",
    "    sorted_tensors['seq'] = create_padded_tensor(input_arrays)\n",
    "    sorted_tensors['mask'] = (sorted_tensors['seq'].abs().sum(dim=2) != 0) \n",
    "    sorted_tensors['lengths'] = sorted_tensors['mask'].sum(dim=1)\n",
    "\n",
    "    # convert outcomes to padded tensors\n",
    "    if pad_outcomes:\n",
    "        sorted_tensors['binary'] = create_padded_tensor(output_arrays)\n",
    "    else:\n",
    "        sorted_tensors['binary'] = torch.tensor(output_arrays, dtype=torch.float32)\n",
    "\n",
    "    return sorted_tensors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "d69d0e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create tensors from arrays\n",
    "pitch_level_tensors = {\n",
    "    'trn': None,\n",
    "    'val': None\n",
    "}\n",
    "\n",
    "# set up tensors\n",
    "if PER_PITCH:\n",
    "    pitch_level_tensors['trn'] = setup_all_tensors(\n",
    "        pitch_level['trn']['inputs'], \n",
    "        pitch_level['trn']['outcomes'], \n",
    "        pad_outcomes=True\n",
    "    )\n",
    "    pitch_level_tensors['val'] = setup_all_tensors(\n",
    "        pitch_level['val']['inputs'], \n",
    "        pitch_level['val']['outcomes'],\n",
    "        pad_outcomes=True\n",
    "    )\n",
    "\n",
    "else:\n",
    "    pitch_level_tensors['trn'] = setup_all_tensors(\n",
    "        pitch_level['trn']['inputs'], \n",
    "        pitch_level['trn']['outcomes'], \n",
    "        pad_outcomes=False\n",
    "    )\n",
    "    pitch_level_tensors['val'] = setup_all_tensors(\n",
    "        pitch_level['val']['inputs'], \n",
    "        pitch_level['val']['outcomes'],\n",
    "        pad_outcomes=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "00db9a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to local disk\n",
    "if PER_PITCH:\n",
    "    with open('storage/pitch_level_tensors.pkl', 'wb') as f:\n",
    "        pickle.dump(pitch_level_tensors, f)\n",
    "\n",
    "    # TODO: upload to AWS (--> pytorch folder)\n",
    "    # with open('storage/pitch_level_tensors.pkl', 'rb') as f:\n",
    "    #     content = f.read()\n",
    "    #     aws.upload_to_s3(content, 'epidemiology/ml/datasets/pytorch/pitch_level_tensors.pkl')\n",
    "\n",
    "else:\n",
    "    with open('storage/outing_level_tensors.pkl', 'wb') as f:\n",
    "        pickle.dump(pitch_level_tensors, f)\n",
    "\n",
    "    # upload to AWS (--> pytorch folder)\n",
    "    # with open('storage/outing_level_tensors.pkl', 'rb') as f:\n",
    "    #     content = f.read()\n",
    "    #     aws.upload_to_s3(content, 'epidemiology/ml/datasets/pytorch/outing_level_tensors.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569efdfd",
   "metadata": {},
   "source": [
    "$\\textbf{Close AWS Connection}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dcaccd92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AWS]: Database connection closed.\n",
      "[AWS]: SSH tunnel stopped.\n"
     ]
    }
   ],
   "source": [
    "aws.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca8c4fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pitch_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
