{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b019010f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from connections import AWS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6adbe8e8",
   "metadata": {},
   "source": [
    "$\\textbf{Epidemiology: Clinical Model Setup}$\n",
    "\n",
    "Injury risk estimation from two perspectives: \n",
    "- __Season__ (i.e., post-outing injury probability; postgame)\n",
    "- __Pitch-Level__ (i.e., next-pitch injury probability; within game)\n",
    "\n",
    "This notebook sets up the data structures and model architecture for development; the full, cleaned versions of each model are trained in `clinical_model_training.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3138f38a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AWS]: Port 5433 is free.\n",
      "[AWS]: Connected to RDS endpoint.\n"
     ]
    }
   ],
   "source": [
    "\"\"\" INITIALIZE AWS CONNECTION \"\"\"\n",
    "aws = AWS()\n",
    "aws.connect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112dc610",
   "metadata": {},
   "source": [
    "$\\textbf{Data Loading}$\n",
    "\n",
    "- Cohort data (matches, preds, statcast)\n",
    "- Ball flight aggregates "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f33b6318",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function for calculating averages\n",
    "def get_avgs(\n",
    "        data: pd.DataFrame,\n",
    "        group_cols: list,\n",
    "        avg_cols: list = ['rel_speed', 'rel_side', 'rel_ht', 'spin_rate']\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate averages for specified columns grouped by the given columns.\n",
    "    \n",
    "    Args:\n",
    "        data (pd.DataFrame): The input DataFrame containing the data.\n",
    "        group_cols (list): List of columns to group by.\n",
    "        avg_cols (list): List of columns to calculate averages for. Defaults to ball flight features used in injury risk model.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame with the grouped columns and their corresponding averages.\n",
    "    \"\"\"\n",
    "    return data.groupby(group_cols)[avg_cols].mean().reset_index()\n",
    "\n",
    "# filter pitches to date range for a given ID\n",
    "def filter_pitches_by_date(\n",
    "        pitch_data: pd.DataFrame, \n",
    "        pitcher_id: int, \n",
    "        start_date: str, \n",
    "        end_date: str\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Filter pitch data for a specific pitcher within a date range.\n",
    "    \n",
    "    Args:\n",
    "        pitch_data (pd.DataFrame): The DataFrame containing pitch data.\n",
    "        pitcher_id (int): The ID of the pitcher to filter by.\n",
    "        start_date (str): The start date in 'YYYY-MM-DD' format.\n",
    "        end_date (str): The end date in 'YYYY-MM-DD' format.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Filtered DataFrame containing pitches for the specified pitcher and date range.\n",
    "    \"\"\"\n",
    "    return pitch_data[\n",
    "        (pitch_data['pitcher'] == pitcher_id) &\n",
    "        (pitch_data['game_date'] >= start_date) &\n",
    "        (pitch_data['game_date'] <= end_date)\n",
    "    ]\n",
    "\n",
    "# add 'outing_before_injury' column for outing-level model: 1 if injured pitcher and second-to-last outing in season, else 0\n",
    "def get_outing_before_injury(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = data.copy()\n",
    "    \n",
    "    # get unique outing dates per pitcher/season; find second-to-last outing\n",
    "    outing_dates = df.groupby(['pitcher', 'season'])['outing_number'].unique()\n",
    "    second_last_outing = outing_dates.apply(lambda x: sorted(x)[-2] if len(x) >= 2 else pd.NaT)\n",
    "    \n",
    "    second_last_df = pd.DataFrame(second_last_outing).reset_index()\n",
    "    second_last_df['outing_before_injury'] = 1\n",
    "\n",
    "    return second_last_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d01d272f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load cohort of matches, model predictions\n",
    "cohort_matches = aws.load_s3_object('epidemiology/ml/datasets/full/cohort_matches_final.csv')\n",
    "cohort_preds = aws.load_s3_object('epidemiology/ml/datasets/preds/model_cohort.csv')\n",
    "\n",
    "# load all statcast data\n",
    "    # likely fts: velo, release position, spin rate (no pitch labels)\n",
    "statcast_data = aws.load_s3_object('epidemiology/ml/datasets/full/model_application_data.csv')\n",
    "cohort_preds_statcast = cohort_preds.merge(statcast_data, on=['pitch_id', 'pitcher', 'game_date', 'pitcher_days_since_prev_game', 'injured_cohort_pitcher'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c923efc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# update dataset to match windows btw each injured & noninjured pitcher\n",
    "cohort_preds_final = []\n",
    "for id in cohort_matches['mlbamid_injured'].unique(): \n",
    "    # get date of first and last pitch for each pitcher\n",
    "    first_pitch = cohort_preds_statcast[cohort_preds_statcast['pitcher'] == id]['game_date'].min()\n",
    "    last_pitch = cohort_preds_statcast[cohort_preds_statcast['pitcher'] == id]['game_date'].max()\n",
    "\n",
    "    # get all pitches for this pitcher, append to final dataset\n",
    "    pitcher_data = filter_pitches_by_date(\n",
    "        cohort_preds_statcast, \n",
    "        id, \n",
    "        first_pitch, \n",
    "        last_pitch\n",
    "    ).reset_index(drop=True)\n",
    "    cohort_preds_final.append(pitcher_data)\n",
    "\n",
    "    # get pitches for match\n",
    "    matched_id = cohort_matches[cohort_matches['mlbamid_injured'] == id]['mlbamid_noninjured'].values[0]\n",
    "    matched_data = filter_pitches_by_date(\n",
    "        cohort_preds_statcast, \n",
    "        matched_id, \n",
    "        first_pitch, \n",
    "        last_pitch\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "    # append to final dataset\n",
    "    cohort_preds_final.append(matched_data)\n",
    "\n",
    "# concatenate all data\n",
    "cohort_preds_final = pd.concat(cohort_preds_final, ignore_index=True)\n",
    "injured_proportion = cohort_preds_final['injured_cohort_pitcher'].mean()        # check proportion of injured pitches --> should be close-ish to 50%\n",
    "\n",
    "# create outing_before_injury column to use as outcome for outing-level model\n",
    "outings_before_injury = get_outing_before_injury(cohort_preds_final)\n",
    "cohort_preds_final = cohort_preds_final.merge(\n",
    "    outings_before_injury, \n",
    "    on=['pitcher', 'season', 'outing_number'], \n",
    "    how='left'\n",
    ")\n",
    "cohort_preds_final['outing_before_injury'] = cohort_preds_final['outing_before_injury'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "c9161114",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(230779, 22)"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cohort_preds_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65134869",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get baseline season-long ball flight averages for reference\n",
    "    # NOTE: these may be referenced for the pitch level model\n",
    "ball_flight_fts = ['rel_speed', 'rel_side', 'rel_ht', 'spin_rate']\n",
    "ball_flight_season_avgs = get_avgs(\n",
    "    cohort_preds_final,\n",
    "    group_cols=['pitcher', 'season'],\n",
    "    avg_cols=ball_flight_fts\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0782525b",
   "metadata": {},
   "source": [
    "$\\textbf{Train/Test Splits}$\n",
    "\n",
    "Loads previously set train/test splits (see `clinical_splits.ipynb`) and applies to `cohort_preds_final`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bfd914d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load splits\n",
    "path_stem = 'epidemiology/ml/datasets/full'\n",
    "cohort_train_ids = aws.load_s3_object(f'{path_stem}/cohort_train_ids.csv')\n",
    "cohort_test_ids = aws.load_s3_object(f'{path_stem}/cohort_test_ids.csv')\n",
    "\n",
    "# filter full dataset by train/test\n",
    "cohort_preds_train = cohort_preds_final[cohort_preds_final['pitcher'].isin(list(cohort_train_ids['mlbamid']))]\n",
    "cohort_preds_test = cohort_preds_final[cohort_preds_final['pitcher'].isin(list(cohort_test_ids['mlbamid']))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b863452c",
   "metadata": {},
   "source": [
    "$\\textbf{Setup Data Sequences}$\n",
    "\n",
    "- __Outing-Level Model__: Up to last outing prior to injury (last outing not included)\n",
    "- __Pitch-Level Model__: Up to final pitch prior to injury\n",
    "\n",
    "Both will be stored as days prior to last outing/pitch (7-, 15-, 30-, 45-, and 90-days prior). All sequences must be converted to arrays (then likely tensors) for model development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fe381f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# days until second-to-last outing (outing-level model)\n",
    "def days_until_2nd_last_outing(group):\n",
    "    \"\"\"\n",
    "    Calculate the number of days until the second-to-last outing for each pitcher in a group.\n",
    "\n",
    "    Args:\n",
    "        group (pd.DataFrame): A DataFrame containing outings for a specific pitcher and season.\n",
    "\n",
    "    Returns:\n",
    "        pd.Series: A Series with the number of days until the second-to-last outing for each row in the group.\n",
    "    \"\"\"\n",
    "    # get unique outing dates for the pitcher\n",
    "    unique_dates = sorted(pd.to_datetime(group['game_date']).unique())\n",
    "    \n",
    "    # check if there are at least two unique outing dates\n",
    "    if len(unique_dates) < 2:\n",
    "        # if not enough outings (eg., only 1), fill with NaN\n",
    "        return pd.Series([pd.NA] * len(group), index=group.index)\n",
    "    \n",
    "    # get the second-to-last unique date\n",
    "    second_last = unique_dates[-2]\n",
    "    \n",
    "    return (second_last - pd.to_datetime(group['game_date'])).dt.days\n",
    "\n",
    "# days until last outing (pitch-level model)\n",
    "def days_until_last_outing(group):\n",
    "    \"\"\"\n",
    "    Calculate the number of days until the last outing for each pitcher in a group.\n",
    "\n",
    "    Args:\n",
    "        group (pd.DataFrame): A DataFrame containing outings for a specific pitcher and season.\n",
    "\n",
    "    Returns:\n",
    "        pd.Series: A Series with the number of days until the last outing for each row in the group.\n",
    "    \"\"\"\n",
    "    last_date = pd.to_datetime(group['game_date']).max()\n",
    "    return (last_date - pd.to_datetime(group['game_date'])).dt.days\n",
    "\n",
    "# create pitch-level sequence dictionary\n",
    "def create_pitch_level_sequences(\n",
    "        data: pd.DataFrame,\n",
    "        features: list\n",
    ") -> dict:\n",
    "    \"\"\" \n",
    "    Creates a dictionary of pitch level sequences organized by days until last outing (currently 7, 15, 39, 45, and 90 days).\n",
    "    \n",
    "    Args:\n",
    "        data (pd.DataFrame): A sorted DataFrame containing all pitch-level sequences.\n",
    "        features (list): The columns to be used as features in the dataset.\n",
    "    \n",
    "    Returns:\n",
    "        dict: A dictionary structured as days until last outing as keys and sequences as values.\n",
    "    \"\"\"\n",
    "    \n",
    "    # setup pitch-level dataset\n",
    "        # pitch_uuid: unique identifier for model dataset\n",
    "    pitch_dataset = data.copy()\n",
    "    pitch_dataset.insert(0, 'pitch_uuid', pitch_dataset.index)\n",
    "\n",
    "    # compute days until last outing for each pitcher\n",
    "    pitch_dataset['days_until_last_outing'] = pitch_dataset.groupby(['pitcher', 'season']).apply(days_until_last_outing).reset_index(level=[0,1], drop=True)\n",
    "\n",
    "    # store sequences by days until last outing prior to injury\n",
    "    pitch_level_sequences = {\n",
    "        7: [],\n",
    "        15: [],\n",
    "        30: [],\n",
    "        45: [],\n",
    "        90: []\n",
    "    }\n",
    "    for day in pitch_level_sequences.keys():\n",
    "        pitch_level_sequences[day] = [\n",
    "            group[features].values\n",
    "                for _, group in pitch_dataset[pitch_dataset['days_until_last_outing'] <= day].groupby(['pitcher', 'season'])\n",
    "        ]\n",
    "\n",
    "    return pitch_level_sequences\n",
    "\n",
    "# create outing-level sequence dictionary\n",
    "def create_outing_level_sequences(\n",
    "        data: pd.DataFrame,\n",
    "        features: list\n",
    "):\n",
    "    \"\"\" \n",
    "    Creates a dictionary of outing level sequences organized by days until **second-to-last** outing (ie., outing prior to injury occurrence). \n",
    "    Currently 7, 15, 39, 45, and 90 days of data are included.\n",
    "    \n",
    "    Args:\n",
    "        data (pd.DataFrame): A sorted DataFrame containing all pitch-level sequences.\n",
    "        features (list): The columns to be used as features in the dataset.\n",
    "    \n",
    "    Returns:\n",
    "        dict: A dictionary structured as days until second-to-last outing as keys and sequences as values.\n",
    "    \"\"\"\n",
    "    # setup outing-level dataset\n",
    "    outing_dataset = data.copy()\n",
    "    outing_dataset.insert(0, 'outing_id', outing_dataset.groupby(['pitcher', 'game_date']).ngroup())\n",
    "\n",
    "    # compute days until second-to-last outing for each pitcher\n",
    "        # NOTE: second-to-last because we're trying to predict \"next\" outing injury\n",
    "        # last outings will have negative days --> drop\n",
    "    outing_dataset['days_until_2nd_last_outing'] = outing_dataset.groupby(\n",
    "        ['pitcher', 'season']\n",
    "    ).apply(days_until_2nd_last_outing).reset_index(level=[0,1], drop=True)\n",
    "    outing_dataset_clean = outing_dataset[outing_dataset['days_until_2nd_last_outing'] >= 0].reset_index(drop=True)\n",
    "\n",
    "    # store sequence lengths by days until second-to-last outing (ie., prior to injured outing)\n",
    "    outing_level_sequences = {\n",
    "        7: [],\n",
    "        15: [],\n",
    "        30: [],\n",
    "        45: [],\n",
    "        90: []\n",
    "    }\n",
    "    for day in outing_level_sequences.keys():\n",
    "        outing_level_sequences[day] = [\n",
    "        group[features].values\n",
    "            for _, group in outing_dataset_clean[outing_dataset_clean['days_until_2nd_last_outing'] <= day].groupby(['pitcher', 'season'])\n",
    "        ]  \n",
    "\n",
    "    return outing_level_sequences\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b96163b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set model features -- contextual & time series layers\n",
    "CONTEXTUAL_FTS = [\n",
    "    'p_throws',\n",
    "    'pitcher_days_since_prev_game',\n",
    "    'outing_number'\n",
    "]\n",
    "TIME_SERIES_FTS = [\n",
    "    # predicted load\n",
    "    'pred_peak_evt_normalized',\n",
    "    'within_outing_cumulative_evt_workload',\n",
    "    \n",
    "    # ball flight\n",
    "    'rel_speed', \n",
    "    'rel_side',\n",
    "    'rel_ht', \n",
    "    'spin_rate'\n",
    "]\n",
    "\n",
    "# combine for full features\n",
    "feature_set = CONTEXTUAL_FTS + TIME_SERIES_FTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c325759e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup pitch- and outing-level training sets: inputs\n",
    "pitch_level_trn = create_pitch_level_sequences(cohort_preds_train, feature_set)\n",
    "outing_level_trn = create_outing_level_sequences(cohort_preds_train, feature_set)\n",
    "\n",
    "# setup pitch- and outing-level validation sets: inputs\n",
    "pitch_level_val = create_pitch_level_sequences(cohort_preds_test, feature_set)\n",
    "outing_level_val = create_outing_level_sequences(cohort_preds_test, feature_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada5d9df",
   "metadata": {},
   "source": [
    "$\\textbf{Create Outcome Probability Grid}$\n",
    "\n",
    "Binary outcome (__1 := injured, 0 := non-injured__) is also converted to a probability for injured pitchers to encourage model learning:\n",
    "- For _outing sequences_, applies a linear increase with each outing until 1 is reached\n",
    "- For _pitch-level sequences_, a sigmoid is created over all pitches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a6f2561c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "7ccf8156",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to approximate a sigmoid given a sequence length\n",
    "def sigmoid(length: int) -> np.ndarray:\n",
    "    \"\"\" Generate a sigmoid curve for a given length.\"\"\"\n",
    "    return 1 / (1 + np.exp(-np.linspace(-6, 6, length)))\n",
    "\n",
    "# update outcome probabilities based on sequence length\n",
    "    # --> creates soft probabilities to aid with loss calculations\n",
    "def update_outcome_probabilities(\n",
    "        data: pd.DataFrame, \n",
    "        model_type: str,\n",
    "        outcome_col: str = 'injured_cohort_pitcher',\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Update the outcome probabilities for each pitcher based on their outing history.\n",
    "    \n",
    "    Args:\n",
    "        data (pd.DataFrame): The DataFrame containing the data.\n",
    "        model_type (str): The type of model being used ('pitch_level' or 'outing_level').\n",
    "        outcome_col (str): The column containing the binary outcome. Defaults to 'injured_cohort_pitcher'.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: The DataFrame with updated probabilities.\n",
    "    \"\"\"\n",
    "    # update group columns based on model type\n",
    "    match model_type:\n",
    "        case 'pitch_level':\n",
    "            pass\n",
    "        case 'outing_level':\n",
    "            group_cols = ['pitcher', 'outing_id', 'season']\n",
    "\n",
    "    # trim to outcomes\n",
    "        # applies a linear increasing probability with each outing for injured pitchers\n",
    "    outcomes_orig = data[group_cols + [outcome_col]].drop_duplicates().reset_index(drop=True).copy()\n",
    "    outcome_probs = outcomes_orig[outcome_col] / (outcomes_orig.sort_values(by=group_cols, ascending=False).groupby(['pitcher', 'season']).cumcount() + 1)\n",
    "\n",
    "    # clean outcomes to binary\n",
    "    outcomes_binary = np.where(outcome_probs == 1, 1, outcome_probs)\n",
    "\n",
    "    return outcome_probs, outcomes_binary\n",
    "\n",
    "# create pitch level outcomes \n",
    "    # uses outcome & last outing cols to create a model outcome --> approx 20% of pitches will be labeled as injured\n",
    "    # returns binary and soft labels\n",
    "def create_pitch_level_outcomes(\n",
    "        input_data: pd.DataFrame,\n",
    "        outcome_col: str = 'injured_cohort_pitcher',\n",
    "        last_outing_col: str = 'days_until_last_outing',\n",
    "        model_outcome_name: str = 'injured_pitch'\n",
    ") -> dict:\n",
    "    \"\"\" \n",
    "    Creates a dictionary of pitch level outcomes organized by days until last outing (currently 7, 15, 39, 45, and 90 days).\n",
    "    \n",
    "    Args:\n",
    "        input_data (pd.DataFrame): Original data used to create the sequences.\n",
    "        outcome_col (str): The column containing the binary outcome. Defaults to 'injured_cohort_pitcher'.\n",
    "        last_outing_col (str): The column containing the days until last outing. Defaults to 'days_until_last_outing'.\n",
    "        model_outcome_name (str): The name of the model outcome column to be created. Defaults to 'injured_pitch'.\n",
    "    \n",
    "    Returns:\n",
    "        dict: A dictionary structured as days until last outing as keys and sequences as values.\n",
    "    \"\"\"\n",
    "    \n",
    "    # setup outcome dictionaries\n",
    "    pitch_level_outcomes = {\n",
    "        7: {\n",
    "            'probs': [],\n",
    "            'binary': []\n",
    "        },\n",
    "        15: {\n",
    "            'probs': [],\n",
    "            'binary': []\n",
    "        },\n",
    "        30: {\n",
    "            'probs': [],\n",
    "            'binary': []\n",
    "        },\n",
    "        45: {\n",
    "            'probs': [],\n",
    "            'binary': []\n",
    "        },\n",
    "        90: {\n",
    "            'probs': [],\n",
    "            'binary': []\n",
    "        }\n",
    "    }\n",
    "\n",
    "     # iterate through pitch-level and update (sigmoid probs & binary)\n",
    "    \n",
    "    # iterate through days \n",
    "    for day in pitch_level_outcomes.keys():\n",
    "        day_pitches = input_data.copy()\n",
    "\n",
    "        # filter to proper date range given by the key\n",
    "        day_pitches['days_until_last_outing'] = day_pitches.groupby(['pitcher', 'season']).apply(days_until_last_outing).reset_index(level=[0,1], drop=True)\n",
    "        day_pitches = day_pitches[day_pitches['days_until_last_outing'] <= day].reset_index(drop=True)\n",
    "\n",
    "        # create model outcome col\n",
    "        day_pitches[model_outcome_name] = np.where(\n",
    "            (day_pitches[outcome_col] == 1) & (day_pitches[last_outing_col] == 0),\n",
    "            1, \n",
    "            0\n",
    "        )\n",
    "        \n",
    "        # iterate through pitcher-season combos in the df\n",
    "        for _, rows in day_pitches.groupby(['pitcher', 'season']):\n",
    "            # get sigmoid probs\n",
    "            sigmoid_probs = 1 / (1 + np.exp(-(np.array(rows[model_outcome_name]))))\n",
    "            pitch_level_outcomes[day]['probs'].append(list(sigmoid_probs))\n",
    "\n",
    "            # convert to binary outcomes\n",
    "            pitch_level_outcomes[day]['binary'].append(list(np.where(sigmoid_probs > 0.5, 1, 0)))\n",
    "\n",
    "    return pitch_level_outcomes\n",
    "\n",
    "# create pitch level outcomes \n",
    "    # returns binary and soft labels\n",
    "def create_outing_level_outcomes(\n",
    "        input_data: pd.DataFrame,\n",
    "        outcome_col: str = 'outing_before_injury'\n",
    ") -> dict:\n",
    "    \"\"\" \n",
    "    Creates a dictionary of outing level outcomes organized by days until **second-to-last** last outing (currently 7, 15, 39, 45, and 90 days).\n",
    "    \n",
    "    Args:\n",
    "        input_data (pd.DataFrame): Original data used to create the sequences.\n",
    "    \n",
    "    Returns:\n",
    "        dict: A dictionary structured as days until last outing as keys and sequences as values.\n",
    "    \"\"\"\n",
    "\n",
    "    # setup outcomes dictionary\n",
    "    outing_level_outcomes = {\n",
    "        7: {\n",
    "            'probs': [],\n",
    "            'binary': []\n",
    "        },\n",
    "        15: {\n",
    "            'probs': [],\n",
    "            'binary': []\n",
    "        },\n",
    "        30: {\n",
    "            'probs': [],\n",
    "            'binary': []\n",
    "        },\n",
    "        45: {\n",
    "            'probs': [],\n",
    "            'binary': []\n",
    "        },\n",
    "        90: {\n",
    "            'probs': [],\n",
    "            'binary': []\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # iterate through outing-level and update (linear probs & binary)\n",
    "    for day in outing_level_outcomes.keys():\n",
    "        # setup data for day\n",
    "            # --> compute days until second-to-last outing for each pitcher\n",
    "        outing_dataset = input_data.copy()\n",
    "        outing_dataset.insert(0, 'outing_id', outing_dataset.groupby(['pitcher', 'game_date']).ngroup())\n",
    "        outing_dataset['days_until_2nd_last_outing'] = outing_dataset.groupby(\n",
    "            ['pitcher', 'season']\n",
    "        ).apply(days_until_2nd_last_outing).reset_index(level=[0,1], drop=True)\n",
    "        \n",
    "        # drop negative days, then filter to window\n",
    "        outing_dataset_clean = outing_dataset[outing_dataset['days_until_2nd_last_outing'] >= 0].reset_index(drop=True)\n",
    "        day_outings = outing_dataset_clean[outing_dataset_clean['days_until_2nd_last_outing'] <= day].reset_index(drop=True)\n",
    "        \n",
    "        # iterate through pitcher-season combos in the df\n",
    "        for _, rows in day_outings.groupby(['pitcher', 'season']):\n",
    "            # get sigmoid probs\n",
    "            sigmoid_probs = 1 / (1 + np.exp(-(np.array(rows[outcome_col]))))\n",
    "            outing_level_outcomes[day]['probs'].append(list(sigmoid_probs))\n",
    "\n",
    "            # convert to binary outcomes\n",
    "            outing_level_outcomes[day]['binary'].append(list(rows[outcome_col]))\n",
    "\n",
    "    return outing_level_outcomes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "7685d125",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup pitch- and outing-level training sets: outcomes\n",
    "pitch_level_trn_y = create_pitch_level_outcomes(cohort_preds_train)\n",
    "outing_level_trn_y = create_outing_level_outcomes(cohort_preds_train)\n",
    "\n",
    "# setup pitch- and outing-level validation sets: outcomes\n",
    "pitch_level_val_y = create_pitch_level_outcomes(cohort_preds_test)\n",
    "outing_level_val_y = create_outing_level_outcomes(cohort_preds_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ac962e",
   "metadata": {},
   "source": [
    "$\\textbf{Aggregate for Streamlined Data Storage}$\n",
    "\n",
    "Combine into pitch- and outing-level dictionaries for better storage, downstream processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "f80ba5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "17af64ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset all into a dictionary\n",
    "pitch_level = {\n",
    "    'trn': {\n",
    "        'inputs': pitch_level_trn,\n",
    "        'outputs': pitch_level_trn_y\n",
    "    },\n",
    "    'val': {\n",
    "        'inputs': pitch_level_val, \n",
    "        'outputs': pitch_level_val_y\n",
    "    }\n",
    "}\n",
    "outing_level = {\n",
    "    'trn': {\n",
    "        'inputs': outing_level_trn,\n",
    "        'outputs': outing_level_trn_y\n",
    "    },\n",
    "    'val': {\n",
    "        'inputs': outing_level_val, \n",
    "        'outputs': outing_level_val_y\n",
    "    }\n",
    "}\n",
    "\n",
    "# save to local disk\n",
    "with open('storage/pitch_level.pkl', 'wb') as f:\n",
    "    pickle.dump(pitch_level, f)\n",
    "with open('storage/outing_level.pkl', 'wb') as f:\n",
    "    pickle.dump(outing_level, f)\n",
    "\n",
    "# TODO: upload to AWS (--> pytorch folder)\n",
    "# with open('storage/pitch_level.pkl', 'rb') as f:\n",
    "#     content = f.read()\n",
    "#     aws.upload_to_s3(content, 'epidemiology/ml/datasets/pytorch/pitch_level_arrays.pkl')\n",
    "# with open('storage/outing_level.pkl', 'rb') as f:\n",
    "#     content = f.read()\n",
    "#     aws.upload_to_s3(content, 'epidemiology/ml/datasets/pytorch/outing_level_arrays.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb76b05",
   "metadata": {},
   "source": [
    "$\\textbf{Tensor Setup}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "05a91689",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "d5624ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to tensors and pad to the same length\n",
    "def create_padded_tensor(sequences: list) -> torch.Tensor:\n",
    "    \"\"\" \n",
    "    Convert a list of sequences to a padded tensor.\n",
    "    \n",
    "    Args:\n",
    "        sequences (list): A list of sequences (arrays) to be converted.\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: A padded tensor of shape (batch, max_seq_len, features).\n",
    "    \"\"\"\n",
    "    seq_tensors = [torch.tensor(np.array(seq, dtype=np.float32)) for seq in sequences]\n",
    "    padded = pad_sequence(seq_tensors, batch_first=True, padding_value=0)        # shape: (batch, max_seq_len, features)\n",
    "\n",
    "    return padded\n",
    "\n",
    "# helper function for creating sorted tensor dictionaries\n",
    "def setup_all_tensors(\n",
    "        input_arrays: dict,\n",
    "        output_arrays: dict\n",
    ") -> dict:\n",
    "    \"\"\" Helper function for creating sorted (pitch- or outing-level) tensor dictionaries. Returns sequences, mask, and lengths for each. \"\"\"\n",
    "    # setup tensor dictionaries\n",
    "    sorted_tensors = {\n",
    "        7: {\n",
    "            'seq': None,\n",
    "            'mask': None,\n",
    "            'lengths': None\n",
    "        },\n",
    "        15: {\n",
    "            'seq': None,\n",
    "            'mask': None,\n",
    "            'lengths': None\n",
    "        },\n",
    "        30: {\n",
    "            'seq': None,\n",
    "            'mask': None,\n",
    "            'lengths': None\n",
    "        },\n",
    "        45: {\n",
    "            'seq': None,\n",
    "            'mask': None,\n",
    "            'lengths': None\n",
    "        },\n",
    "        90: {\n",
    "            'seq': None,\n",
    "            'mask': None,\n",
    "            'lengths': None\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # iterate through pitch-level and create tensors\n",
    "    # shape: (batch, max_seq_len, features)\n",
    "    # also add mask and lengths to denote actual values for training\n",
    "    for day in sorted_tensors.keys():\n",
    "        sorted_tensors[day]['seq'] = create_padded_tensor(input_arrays[day])\n",
    "        sorted_tensors[day]['mask'] = (sorted_tensors[day]['seq'].abs().sum(dim=2) != 0) \n",
    "        sorted_tensors[day]['lengths'] = sorted_tensors[day]['mask'].sum(dim=1)\n",
    "\n",
    "        # convert outcomes to padded tensors\n",
    "        sorted_tensors[day]['probs'] = create_padded_tensor(output_arrays[day]['probs'])\n",
    "        sorted_tensors[day]['binary'] = create_padded_tensor(output_arrays[day]['binary'])\n",
    "\n",
    "    return sorted_tensors\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "d69d0e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create tensors -- pitch level\n",
    "pitch_level_tensors = {\n",
    "    'trn': None,\n",
    "    'val': None\n",
    "}\n",
    "pitch_level_tensors['trn'] = setup_all_tensors(pitch_level['trn']['inputs'], pitch_level['trn']['outputs'])\n",
    "pitch_level_tensors['val'] = setup_all_tensors(pitch_level['val']['inputs'], pitch_level['val']['outputs'])\n",
    "\n",
    "# create tensors -- outing level\n",
    "outing_level_tensors = {\n",
    "    'trn': None,\n",
    "    'val': None\n",
    "}\n",
    "outing_level_tensors['trn'] = setup_all_tensors(outing_level['trn']['inputs'], outing_level['trn']['outputs'])\n",
    "outing_level_tensors['val'] = setup_all_tensors(outing_level['val']['inputs'], outing_level['val']['outputs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "00db9a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to local disk\n",
    "with open('storage/pitch_level_tensors.pkl', 'wb') as f:\n",
    "    pickle.dump(pitch_level_tensors, f)\n",
    "with open('storage/outing_level_tensors.pkl', 'wb') as f:\n",
    "    pickle.dump(outing_level_tensors, f)\n",
    "\n",
    "# TODO: upload to AWS (--> pytorch folder)\n",
    "# with open('storage/pitch_level_tensors.pkl', 'rb') as f:\n",
    "#     content = f.read()\n",
    "#     aws.upload_to_s3(content, 'epidemiology/ml/datasets/pytorch/pitch_level_tensors.pkl')\n",
    "# with open('storage/outing_level_tensors.pkl', 'rb') as f:\n",
    "#     content = f.read()\n",
    "#     aws.upload_to_s3(content, 'epidemiology/ml/datasets/pytorch/outing_level_tensors.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7bc61cf",
   "metadata": {},
   "source": [
    "$\\textit{Create Tensor Datasets}$\n",
    "\n",
    "__Note__: In the future, these can be created using dictionaries loaded from above. They won't be necessary if manual batch iteration is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "56d872a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup tensor datasets\n",
    "    # also store relevant model information: loss, num. of sequences, etc\n",
    "pitch_level_datasets = {\n",
    "    'trn': {\n",
    "        7: {\n",
    "        'probs': None,\n",
    "        'binary': None,\n",
    "        'loss_function': None,\n",
    "        'num_sequences': None,\n",
    "        'max_seq_len': None\n",
    "    },\n",
    "        15: {\n",
    "            'probs': None,\n",
    "            'binary': None,\n",
    "            'loss_function': None,\n",
    "            'num_sequences': None\n",
    "        },\n",
    "        30: {\n",
    "            'probs': None,\n",
    "            'binary': None,\n",
    "            'loss_function': None,\n",
    "            'num_sequences': None\n",
    "        },\n",
    "        45: {\n",
    "            'probs': None,\n",
    "            'binary': None,\n",
    "            'loss_function': None,\n",
    "            'num_sequences': None\n",
    "        },\n",
    "        90: {\n",
    "            'probs': None,\n",
    "            'binary': None,\n",
    "            'loss_function': None,\n",
    "            'num_sequences': None\n",
    "        }\n",
    "    },\n",
    "    'val': {\n",
    "        7: {\n",
    "        'probs': None,\n",
    "        'binary': None,\n",
    "        'loss_function': None,\n",
    "        'num_sequences': None,\n",
    "        'max_seq_len': None\n",
    "    },\n",
    "        15: {\n",
    "            'probs': None,\n",
    "            'binary': None,\n",
    "            'loss_function': None,\n",
    "            'num_sequences': None\n",
    "        },\n",
    "        30: {\n",
    "            'probs': None,\n",
    "            'binary': None,\n",
    "            'loss_function': None,\n",
    "            'num_sequences': None\n",
    "        },\n",
    "        45: {\n",
    "            'probs': None,\n",
    "            'binary': None,\n",
    "            'loss_function': None,\n",
    "            'num_sequences': None\n",
    "        },\n",
    "        90: {\n",
    "            'probs': None,\n",
    "            'binary': None,\n",
    "            'loss_function': None,\n",
    "            'num_sequences': None\n",
    "        }\n",
    "    }\n",
    "}\n",
    "outing_level_datasets = {\n",
    "    'trn': {\n",
    "        7: {\n",
    "            'probs': None,\n",
    "            'binary': None,\n",
    "            'loss_function': None,\n",
    "            'num_sequences': None\n",
    "        },\n",
    "        15: {\n",
    "            'probs': None,\n",
    "            'binary': None,\n",
    "            'loss_function': None,\n",
    "            'num_sequences': None\n",
    "        },\n",
    "        30: {\n",
    "            'probs': None,\n",
    "            'binary': None,\n",
    "            'loss_function': None,\n",
    "            'num_sequences': None\n",
    "        },\n",
    "        45: {\n",
    "            'probs': None,\n",
    "            'binary': None,\n",
    "            'loss_function': None,\n",
    "            'num_sequences': None\n",
    "        },\n",
    "        90: {\n",
    "            'probs': None,\n",
    "            'binary': None,\n",
    "            'loss_function': None,\n",
    "            'num_sequences': None\n",
    "        }\n",
    "    },\n",
    "    'val': {\n",
    "        7: {\n",
    "            'probs': None,\n",
    "            'binary': None,\n",
    "            'loss_function': None,\n",
    "            'num_sequences': None\n",
    "        },\n",
    "        15: {\n",
    "            'probs': None,\n",
    "            'binary': None,\n",
    "            'loss_function': None,\n",
    "            'num_sequences': None\n",
    "        },\n",
    "        30: {\n",
    "            'probs': None,\n",
    "            'binary': None,\n",
    "            'loss_function': None,\n",
    "            'num_sequences': None\n",
    "        },\n",
    "        45: {\n",
    "            'probs': None,\n",
    "            'binary': None,\n",
    "            'loss_function': None,\n",
    "            'num_sequences': None\n",
    "        },\n",
    "        90: {\n",
    "            'probs': None,\n",
    "            'binary': None,\n",
    "            'loss_function': None,\n",
    "            'num_sequences': None\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# update datasets (binary & probs)\n",
    "for set in ['trn', 'val']:\n",
    "    for day in [7, 15, 30, 45, 90]:\n",
    "        for outcome_type in ['binary', 'probs']:\n",
    "            pitch_level_datasets[set][day][outcome_type] = TensorDataset(\n",
    "                pitch_level_tensors[set][day]['seq'], \n",
    "                pitch_level_tensors[set][day][outcome_type],\n",
    "                pitch_level_tensors[set][day]['mask'],\n",
    "                pitch_level_tensors[set][day]['lengths']\n",
    "            )\n",
    "            outing_level_datasets[set][day][outcome_type] = TensorDataset(\n",
    "                outing_level_tensors[set][day]['seq'], \n",
    "                outing_level_tensors[set][day][outcome_type],\n",
    "                outing_level_tensors[set][day]['mask'],\n",
    "                outing_level_tensors[set][day]['lengths']\n",
    "            )\n",
    "\n",
    "            # update metadata\n",
    "            pitch_level_datasets[set][day]['num_sequences'] = pitch_level_tensors[set][day]['seq'].shape[0]\n",
    "            outing_level_datasets[set][day]['num_sequences'] = outing_level_tensors[set][day]['seq'].shape[0]\n",
    "            pitch_level_datasets[set][day]['max_seq_len'] = pitch_level_tensors[set][day]['seq'].shape[1]\n",
    "            outing_level_datasets[set][day]['max_seq_len'] = outing_level_tensors[set][day]['seq'].shape[1]\n",
    "\n",
    "            # TODO: set loss function?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd468bf",
   "metadata": {},
   "source": [
    "$\\textbf{Model Architecture}$\n",
    "\n",
    "Both models use a __CNN-(bi)LSTM__ architecture:\n",
    "- The CNN layers progressively extract salient features\n",
    "- The bi-LSTM looks for temporal patterns that may distinguish injured and non-injured pitchers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "4767d08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "ba80ff31",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" MODEL ARCHITECTURES \"\"\"\n",
    "# CNN block for local patterns\n",
    "class CNNBlock(nn.Module):\n",
    "    \"\"\" \n",
    "    Depthwise-separable 1D convolutional block over time with residual. \n",
    "\n",
    "    Args:\n",
    "        num_channels (int): Number of input channels.\n",
    "        kernel (int): Size of the convolutional kernel. Defaults to 7.\n",
    "        dropout (float): Dropout rate. Defaults to 0.1.\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    \n",
    "    **Note**: CNN expects tensor with shape [B, C, T]. \n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self, \n",
    "            num_channels: int, \n",
    "            kernel: int = 7, \n",
    "            dropout: float = 0.1\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        pad = kernel // 2\n",
    "        \n",
    "        # depthwise convolution\n",
    "        self.dw = nn.Conv1d(num_channels, num_channels, kernel_size=kernel, padding=pad, groups=num_channels)\n",
    "        self.pw = nn.Conv1d(num_channels, num_channels, kernel_size=1)\n",
    "        \n",
    "        # batch normalization, activation, and dropout\n",
    "        self.bn = nn.BatchNorm1d(num_channels)\n",
    "        self.act = nn.GELU()\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(\n",
    "            self, \n",
    "            x: torch.Tensor\n",
    "    ):  # x: [B,C,T]\n",
    "        residual = x\n",
    "        \n",
    "        # apply layers\n",
    "        x = self.dw(x)\n",
    "        x = self.pw(x)\n",
    "        \n",
    "        # batch normalization, activation, and dropout\n",
    "        x = self.bn(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        \n",
    "        return x + residual\n",
    "\n",
    "class CNNbiLSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    CNN + BiLSTM model for time series data with per-pitch head.\n",
    "\n",
    "    Args:\n",
    "        k_in (int): Number of input features (K).\n",
    "        stem (int): Number of channels in the stem layer. Defaults to 64.\n",
    "        c (int): Number of channels after projection. Defaults to 96.\n",
    "        kernel (int): Size of the convolutional kernel. Defaults to 7.\n",
    "        lstm_hidden (int): Hidden size for the LSTM layer. Defaults to 128.\n",
    "        dropout (float): Dropout rate. Defaults to 0.1.\n",
    "        bidir (bool): Whether to use a bidirectional LSTM. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "\n",
    "    **Note**: CNN expects tensor with shape [B, K, T]. \n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self, \n",
    "            k_in: int, \n",
    "            stem: int = 64, \n",
    "            c: int = 96, \n",
    "            kernel: int = 7, \n",
    "            lstm_hidden: int = 128, \n",
    "            dropout: float = 0.1, \n",
    "            bidir: bool = True\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        # 1x1 stem to mix K features into 'stem' channels\n",
    "        self.stem = nn.Sequential(\n",
    "            nn.Conv1d(k_in, stem, kernel_size=1),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "        \n",
    "        # progressive temporal convs\n",
    "        self.conv1 = CNNBlock(stem, kernel=kernel, dropout=dropout)\n",
    "        self.proj1 = nn.Conv1d(stem, c, kernel_size=1)                    # project to C channels\n",
    "        self.conv2 = CNNBlock(c, kernel=kernel, dropout=dropout)\n",
    "\n",
    "        # BiLSTM over time\n",
    "        self.lstm = nn.LSTM(input_size=c, hidden_size=lstm_hidden, batch_first=True, bidirectional=bidir)\n",
    "        hdim = lstm_hidden * (2 if bidir else 1)\n",
    "\n",
    "        # per-pitch head\n",
    "        self.head_step = nn.Sequential(\n",
    "            nn.LayerNorm(hdim),\n",
    "            nn.Linear(hdim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "            self, \n",
    "            x: torch.Tensor, \n",
    "            lengths: torch.Tensor\n",
    "    ) -> None:\n",
    "        # CNN expects [B,K,T]\n",
    "        x = x.transpose(1, 2)                  # [B,K,T]\n",
    "        x = self.stem(x)                       # [B,stem,T]\n",
    "        x = self.conv1(x)                      # [B,stem,T]\n",
    "        x = self.proj1(x)                      # [B,C,T]\n",
    "        x = self.conv2(x)                      # [B,C,T]\n",
    "        x = x.transpose(1, 2)                  # [B,T,C] for LSTM\n",
    "\n",
    "        # pack to ignore padding inside LSTM\n",
    "        packed = pack_padded_sequence(x, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        packed_out, _ = self.lstm(packed)\n",
    "        out, _ = pad_packed_sequence(packed_out, batch_first=True, total_length=x.size(1))    # [B,T,H]\n",
    "\n",
    "        # apply head to get logits for each pitch\n",
    "        logits_step = self.head_step(out).squeeze(-1)  # [B,T]\n",
    "        \n",
    "        return logits_step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "3f7cd859",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" LOSS FUNCTIONS \"\"\"\n",
    "def pitch_level_loss(\n",
    "        logits_step: torch.Tensor, \n",
    "        y_step: torch.Tensor, \n",
    "        mask: torch.Tensor, \n",
    "        pos_weight: bool = False\n",
    ") -> torch.Tensor:\n",
    "    \"\"\" \n",
    "    Compute pitch-level loss given ground truth. Valid for binary or smoothed (e.g., sigmoid) outcome labels.\n",
    "\n",
    "    Args:\n",
    "        logits_step (torch.Tensor): Logits from the model of shape [B, T].\n",
    "        y_step (torch.Tensor): Ground truth labels of shape [B, T]. Should be in [0, 1].\n",
    "        mask (torch.Tensor): Mask indicating valid time steps of shape [B, T].\n",
    "        pos_weight (bool, optional): Whether or not to use weights for positive class in BCE loss. Default is False.\n",
    "    \"\"\"\n",
    "    if pos_weight is None:\n",
    "        # setup weights\n",
    "        pos = y_step[mask].sum()\n",
    "        neg = mask.sum() - pos\n",
    "        pos_weight = neg / pos.clamp(min=1.0)\n",
    "        \n",
    "        return F.binary_cross_entropy_with_logits(logits_step[mask], y_step[mask])\n",
    "    \n",
    "    return F.binary_cross_entropy_with_logits(logits_step[mask], y_step[mask], pos_weight=pos_weight)\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_pos_weight(\n",
    "    train_loader: DataLoader, \n",
    "    device: torch.device = torch.device('cpu')\n",
    ") -> torch.Tensor:\n",
    "    pos = 0.0\n",
    "    tot = 0.0\n",
    "    \n",
    "    # iterate through training data to compute positive and total counts\n",
    "    for x, y, m, L in train_loader:\n",
    "        y, m = y.to(device), m.to(device)\n",
    "        pos += y[m].sum().item()\n",
    "        tot += m.sum().item()\n",
    "    \n",
    "    # compute positive weight\n",
    "    neg = max(tot - pos, 1.0)\n",
    "    pos = max(pos, 1.0)\n",
    "    \n",
    "    return torch.tensor(neg / pos, device=device, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60dd9177",
   "metadata": {},
   "source": [
    "$\\textbf{Model Development}$\n",
    "\n",
    "Some additional notes: \n",
    "- __Train/Test Split__: Applied above. 75% of matches are used for the training set; 25% are held out for validation. \n",
    "- __Data Scaling__: Applied to training splits.\n",
    "- __Number of Epochs__: Run until early stopping criterion is met."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "e5402fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "e12b2557",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_masked_scalers(x, mask):\n",
    "    # x: [B,T,K], mask: [B,T]\n",
    "    m = mask.unsqueeze(-1)                  # [B,T,1]\n",
    "    num = m.sum(dim=(0,1)).clamp(min=1)     # [K]\n",
    "    \n",
    "    # compute mean, var, std\n",
    "    mean = (x * m).sum(dim=(0,1)) / num\n",
    "    var  = ((x - mean) * m).pow(2).sum(dim=(0,1)) / num\n",
    "    std  = var.sqrt().clamp(min=1e-6)\n",
    "    \n",
    "    return mean, std\n",
    "\n",
    "# normalize all splits in-place (or create new tensors)\n",
    "def apply_scalers(x, mean, std): \n",
    "    return (x - mean) / std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "61e8f3c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([321, 510, 9]), Device: cpu\n",
      "pos_weight = 6.168867588043213\n"
     ]
    }
   ],
   "source": [
    "# set number of epochs, batch size\n",
    "NUM_EPOCHS = 5      # if none, run intil early stopping\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "## TODO: create compile_model() function --->\n",
    "\n",
    "# standardize example sequence --> example training tensor sequence (x)\n",
    "example_mean, example_std = compute_masked_scalers(pitch_level_tensors['trn'][15]['seq'], pitch_level_tensors['trn'][15]['mask'])\n",
    "x = apply_scalers(pitch_level_tensors['trn'][15]['seq'], example_mean, example_std)\n",
    "\n",
    "# setup (device, shapes)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "B, T, K = x.shape\n",
    "print(f'Input shape: {x.shape}, Device: {device}')\n",
    "\n",
    "# move full tensors to device once (since were not using a DataLoader yet)\n",
    "x               = x.float().to(device)\n",
    "y_step          = pitch_level_tensors['trn'][15]['probs'].float().to(device)\n",
    "y_step_binary   = pitch_level_tensors['trn'][15]['binary'].float().to(device)\n",
    "mask            = pitch_level_tensors['trn'][15]['mask'].bool().to(device)\n",
    "lengths         = pitch_level_tensors['trn'][15]['lengths'].long().to(device)\n",
    "\n",
    "# setup model\n",
    "model = CNNbiLSTM(k_in=K, stem=64, c=96, kernel=7, lstm_hidden=128, dropout=0.1, bidir=True).to(device)\n",
    "\n",
    "# optimizer + class weights\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "\n",
    "# optional: compute pos_weight over valid steps once\n",
    "    # NOTE: for probs this should be 1.0\n",
    "with torch.no_grad():\n",
    "    pos = y_step_binary[mask].sum()\n",
    "    tot = mask.sum()\n",
    "    neg = tot - pos\n",
    "    pos_weight = (neg / pos.clamp(min=1)).float()\n",
    "\n",
    "print(\"pos_weight =\", float(pos_weight))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99767dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    # set training info\n",
    "    model.train()\n",
    "    idx = torch.randperm(B, device=device)\n",
    "\n",
    "    # loss counters\n",
    "    running_loss = 0.0\n",
    "    running_correct = 0\n",
    "    running_total = 0\n",
    "\n",
    "    # wrap range() with tqdm\n",
    "    pbar = tqdm(range(0, B, BATCH_SIZE), desc=f\"Epoch {epoch}/{NUM_EPOCHS}\", leave=False)\n",
    "    for start in pbar:\n",
    "        end = min(start + BATCH_SIZE, B)\n",
    "        bidx = idx[start:end]\n",
    "\n",
    "        # forward pass\n",
    "        xb = x[bidx]\n",
    "        yb = y_step[bidx]\n",
    "        mb = mask[bidx]\n",
    "        Lb = lengths[bidx]\n",
    "\n",
    "        # update loss\n",
    "        logits = model(xb, Lb)\n",
    "        loss = F.binary_cross_entropy_with_logits(\n",
    "            logits[mb], yb[mb], pos_weight=pos_weight\n",
    "        )\n",
    "\n",
    "        # back propagation\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        # running loss\n",
    "        bs = xb.size(0)\n",
    "        running_loss += loss.item() * bs\n",
    "        \n",
    "        # running accuracy\n",
    "        probs  = torch.sigmoid(logits[mb])\n",
    "        preds  = (probs > 0.5).float()\n",
    "        correct = (preds == y_step_binary[bidx][mb]).sum().item()\n",
    "        total   = mb.sum().item()\n",
    "\n",
    "        # update total\n",
    "        running_correct += correct\n",
    "        running_total += total\n",
    "\n",
    "        # update accuracy\n",
    "        run_avg_loss = running_loss / ((start // BATCH_SIZE + 1) * bs)\n",
    "        run_acc      = running_correct / running_total\n",
    "\n",
    "        # show both current and running losses in the bar\n",
    "        pbar.set_postfix(loss=f\"{loss.item():.4f}\", acc=f\"{run_acc:.4f}\")\n",
    "\n",
    "    # update epoch loss\n",
    "    epoch_loss = running_loss / B\n",
    "    epoch_acc  = running_correct / running_total\n",
    "    \n",
    "    print(f\"Epoch {epoch:02d} | Training Loss: {epoch_loss:.3f} | Training Accuracy: {epoch_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569efdfd",
   "metadata": {},
   "source": [
    "$\\textbf{Close AWS Connection}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "dcaccd92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AWS]: No active connection to close.\n"
     ]
    }
   ],
   "source": [
    "aws.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1bb2c2",
   "metadata": {},
   "source": [
    "$\\textbf{Sandbox: Development}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "8b3b66ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pred_peak_evt_normalized',\n",
       " 'within_outing_cumulative_evt_workload',\n",
       " 'rel_speed',\n",
       " 'rel_side',\n",
       " 'rel_ht',\n",
       " 'spin_rate',\n",
       " 'p_throws',\n",
       " 'pitcher_days_since_prev_game',\n",
       " 'outing_number']"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list of features for reference\n",
    "TIME_SERIES_FTS + CONTEXTUAL_FTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b18a259",
   "metadata": {},
   "source": [
    "$\\textit{Example Training Sequence}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c3263d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize example sequence --> example training tensor sequence (x)\n",
    "example_mean, example_std = compute_masked_scalers(pitch_level_tensors[7]['seq'], pitch_level_tensors[7]['mask'])\n",
    "x = apply_scalers(pitch_level_tensors[7]['seq'], example_mean, example_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "6b4bddf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([401, 348, 9]), Device: cpu\n"
     ]
    }
   ],
   "source": [
    "# setup (device, shapes)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "B, T, K = x.shape\n",
    "print(f'Input shape: {x.shape}, Device: {device}')\n",
    "\n",
    "# move full tensors to device once (since were not using a DataLoader yet)\n",
    "x               = x.float().to(device)\n",
    "y_step          = pitch_level_outcomes[7]['probs'].float().to(device)\n",
    "y_step_binary   = pitch_level_outcomes[7]['binary'].float().to(device)\n",
    "mask            = pitch_level_tensors[7]['mask'].bool().to(device)\n",
    "lengths         = pitch_level_tensors[7]['lengths'].long().to(device)\n",
    "\n",
    "# setup model\n",
    "model = CNNbiLSTM(k_in=K, stem=64, c=96, kernel=7, lstm_hidden=128, dropout=0.1, bidir=True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "833a12a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_weight = 1.0\n"
     ]
    }
   ],
   "source": [
    "# optimizer + class weights\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "\n",
    "# optional: compute pos_weight over valid steps once\n",
    "    # NOTE: for probs this should be 1.0\n",
    "with torch.no_grad():\n",
    "    pos = y_step[mask].sum()\n",
    "    tot = mask.sum()\n",
    "    neg = tot - pos\n",
    "    pos_weight = (neg / pos.clamp(min=1)).float()\n",
    "print(\"pos_weight =\", float(pos_weight))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74687c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 01 | train_loss 0.7532\n"
     ]
    }
   ],
   "source": [
    "epochs     = 1\n",
    "batch_size = 32\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    model.train()\n",
    "\n",
    "    # shuffle indices each epoch\n",
    "    idx = torch.randperm(B, device=device)\n",
    "\n",
    "    running_loss = 0.0\n",
    "    seen = 0\n",
    "\n",
    "    # iterate through mini-batches\n",
    "    for start in range(0, B, batch_size):\n",
    "        end = min(start + batch_size, B)\n",
    "        bidx = idx[start:end]                    # [b]\n",
    "\n",
    "        xb = x[bidx]                             # [b,T,K]\n",
    "        yb = y_step[bidx]                        # [b,T]\n",
    "        mb = mask[bidx]                          # [b,T]\n",
    "        Lb = lengths[bidx].float()                       # [b]\n",
    "\n",
    "        # forward\n",
    "        logits = model(xb, Lb)                   # [b,T]\n",
    "\n",
    "        # masked BCE with optional pos_weight\n",
    "        loss = F.binary_cross_entropy_with_logits(\n",
    "            logits[mb], yb[mb],\n",
    "            pos_weight=pos_weight\n",
    "        )\n",
    "\n",
    "        # back propagation\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        # update running loss\n",
    "        running_loss += loss.item() * xb.size(0)\n",
    "        seen += xb.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / max(seen, 1)\n",
    "    print(f\"Epoch {epoch:02d} | Training Loss: {epoch_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "398fa85e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per-pitch accuracy = 0.561\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    logits = model(x, lengths)             # [B,T]\n",
    "    probs  = torch.sigmoid(logits)         # [B,T]\n",
    "\n",
    "    # mask out padded steps\n",
    "    preds  = (probs >= 0.5).float()[mask]  # [N_valid]\n",
    "    labels = y_step_binary[mask]                  # [N_valid]\n",
    "\n",
    "    acc = (preds == labels).float().mean().item()\n",
    "    \n",
    "    print(f\"Per-pitch accuracy = {acc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "00741fc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per-pitch AUC = 0.653\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "y_true = labels.cpu().numpy()\n",
    "y_score = probs[mask].cpu().numpy()\n",
    "auc = roc_auc_score(y_true, y_score) if y_true.min() < y_true.max() else float(\"nan\")\n",
    "print(f\"Per-pitch AUC = {auc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "d1c04122",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8792)"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "4830d02b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0.,  ..., 1., 1., 1.])"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "f64c1bdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 1.,  ..., 1., 1., 1.])"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c174afe",
   "metadata": {},
   "source": [
    "$\\textit{Outcome Handling}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08009297",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gather outcomes\n",
    "    # non-injured pitcher --> outcomes = 0\n",
    "    # injured pitcher --> outcomes = ...\n",
    "example = outing_level_sequences[7]\n",
    "example_outcomes = example[['pitcher', 'season', 'outing_id', 'injured_cohort_pitcher']].drop_duplicates().reset_index(drop=True).copy()\n",
    "test_probs, test_binary = update_outcome_probabilities(example, model_type='outing_level')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5d3aa612",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_pitches = pitch_level_sequences[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c013fa6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate through all pitcher-pitches in the datespan\n",
    "for group, rows in example_pitches.groupby(['pitcher', 'season']):\n",
    "    rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "a2d9cec9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00247262, 0.00273425, 0.00302348, 0.0033432 , 0.0036966 ,\n",
       "       0.00408721, 0.0045189 , 0.00499596, 0.0055231 , 0.00610552,\n",
       "       0.00674895, 0.00745967, 0.00824462, 0.0091114 , 0.01006839,\n",
       "       0.01112476, 0.01229059, 0.01357692, 0.01499583, 0.01656054,\n",
       "       0.01828548, 0.02018641, 0.02228046, 0.0245863 , 0.02712415,\n",
       "       0.02991593, 0.03298531, 0.03635781, 0.04006084, 0.04412375,\n",
       "       0.04857786, 0.05345645, 0.05879472, 0.06462967, 0.07100002,\n",
       "       0.07794595, 0.08550885, 0.09373097, 0.10265494, 0.11232324,\n",
       "       0.12277754, 0.1340579 , 0.14620194, 0.15924378, 0.17321298,\n",
       "       0.18813338, 0.20402187, 0.22088711, 0.23872837, 0.25753434,\n",
       "       0.27728217, 0.29793663, 0.31944957, 0.3417597 , 0.36479277,\n",
       "       0.38846205, 0.41266938, 0.43730648, 0.46225681, 0.48739763,\n",
       "       0.51260237, 0.53774319, 0.56269352, 0.58733062, 0.61153795,\n",
       "       0.63520723, 0.6582403 , 0.68055043, 0.70206337, 0.72271783,\n",
       "       0.74246566, 0.76127163, 0.77911289, 0.79597813, 0.81186662,\n",
       "       0.82678702, 0.84075622, 0.85379806, 0.8659421 , 0.87722246,\n",
       "       0.88767676, 0.89734506, 0.90626903, 0.91449115, 0.92205405,\n",
       "       0.92899998, 0.93537033, 0.94120528, 0.94654355, 0.95142214,\n",
       "       0.95587625, 0.95993916, 0.96364219, 0.96701469, 0.97008407,\n",
       "       0.97287585, 0.9754137 , 0.97771954, 0.97981359, 0.98171452,\n",
       "       0.98343946, 0.98500417, 0.98642308, 0.98770941, 0.98887524,\n",
       "       0.98993161, 0.9908886 , 0.99175538, 0.99254033, 0.99325105,\n",
       "       0.99389448, 0.9944769 , 0.99500404, 0.9954811 , 0.99591279,\n",
       "       0.9963034 , 0.9966568 , 0.99697652, 0.99726575, 0.99752738])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(1 / (1 + np.exp(-np.linspace(-6, 6, rows.shape[0]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667756c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pitch_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
