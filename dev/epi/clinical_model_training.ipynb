{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4f63445a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "from connections import AWS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f5062c",
   "metadata": {},
   "source": [
    "$\\textbf{Epidemiology: Clinical Model Training}$\n",
    "\n",
    "Injury risk estimation is possible from two perspectives: \n",
    "- __Season__ (i.e., post-outing injury probability; postgame)\n",
    "- __Pitch-Level__ (i.e., next-pitch injury probability; within game)\n",
    "\n",
    "This notebook uses the data structures and model architecture established in `clinical_model_setup.ipynb`. The architecture was shown to work with a manually crafted batched-training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "221de76a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AWS]: Port 5433 is free.\n",
      "[AWS]: Connected to RDS endpoint.\n"
     ]
    }
   ],
   "source": [
    "# setup AWS connection\n",
    "aws = AWS()\n",
    "aws.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "18b3e8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download most up-to-date tensor dictionaries\n",
    "    # TODO: wait until the tensors are uploaded to S3\n",
    "# pitch_level_tensors = aws.s3.download_file(\n",
    "#     aws.bucket_name, \n",
    "#     'epidemiology/ml/datasets/pytorch/pitch_level_tensors.pkl', \n",
    "#     'storage/pitch_level_tensors.pkl'\n",
    "# )\n",
    "\n",
    "# load tensors into memory\n",
    "with open('storage/pitch_level_tensors.pkl', 'rb') as f:\n",
    "    pitch_level_tensors = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693616b9",
   "metadata": {},
   "source": [
    "$\\textbf{Model Development}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "25794636",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "from nnet import CNNbiLSTM\n",
    "import torch.nn.functional as F\n",
    "from nnet.loss_functions import pitch_level_loss\n",
    "from sklearn.metrics import roc_auc_score, f1_score\n",
    "from services.scalers import compute_masked_scalers, apply_scalers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "2e9496e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile model for training --> model, optimizer, loss function, and data setup\n",
    "def compile_model(\n",
    "        train_data: dict,\n",
    "        val_data: dict,\n",
    "        model_config: dict = {\n",
    "            'stem': 64,\n",
    "            'c': 96,\n",
    "            'kernel': 7,\n",
    "            'lstm_hidden': 128,\n",
    "            'dropout': 0.1,\n",
    "            'bidir': True\n",
    "        },\n",
    "        device: str = 'cpu',\n",
    "        use_pos_weight: bool = False\n",
    ") -> dict:\n",
    "    \"\"\" \n",
    "    Compile a nnet architecture for training. Includes scaling, model instantiation, and loss function setup.\n",
    "    \n",
    "    Args:\n",
    "        train_data (dict): Training data containing tensors and masks.\n",
    "        val_data (dict): Validation data containing tensors and masks.\n",
    "        model_config (dict): Configuration for the CNNbiLSTM model. Defaults to basic setup.\n",
    "        device (str): Device to run the model on ('cpu' or 'cuda').\n",
    "        use_pos_weight (bool): Whether to use positive class weights in loss function.\n",
    "    \n",
    "    Returns:\n",
    "        model_setup (dict): Dictionary with all relevant information..\n",
    "    \"\"\"\n",
    "\n",
    "    # standardize example sequence --> example training tensor sequence (x)\n",
    "    mean, std = compute_masked_scalers(train_data['seq'], train_data['mask'])\n",
    "    x = apply_scalers(train_data['seq'], mean, std)\n",
    "    B, T, K = x.shape       # NOTE: B = batch size, T = time steps, K = features; K is used for model setup\n",
    "\n",
    "    # setup (device, shapes)\n",
    "        # --> move full tensors to device\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    x_trn, y_step_trn, y_binary_trn, mask_trn, lengths_trn = move_tensors_to_device(x, train_data, device)\n",
    "\n",
    "    # setup model, optimizer\n",
    "    model = CNNbiLSTM(\n",
    "        k_in=K, \n",
    "        stem=model_config.get('stem', 64),\n",
    "        c=model_config.get('c', 96),\n",
    "        kernel=model_config.get('kernel', 7),\n",
    "        lstm_hidden=model_config.get('lstm_hidden', 128),\n",
    "        dropout=model_config.get('dropout', 0.1),\n",
    "        bidir=model_config.get('bidir', True),\n",
    "    ).to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "\n",
    "    # optional: compute pos_weight over valid steps once\n",
    "        # NOTE: for probs this should be 1.0\n",
    "    if use_pos_weight:\n",
    "        with torch.no_grad():\n",
    "            pos = y_binary_trn[mask_trn].sum()\n",
    "            tot = mask_trn.sum()\n",
    "            neg = tot - pos\n",
    "            pos_weight = (neg / pos.clamp(min=1)).float()\n",
    "    else:\n",
    "        pos_weight = torch.tensor(1.0, device=device)\n",
    "\n",
    "    # package all into dictionary\n",
    "    model_setup = {\n",
    "        'model': model.train(),\n",
    "        'optimizer': optimizer,\n",
    "        'loss_fn': pitch_level_loss,\n",
    "        'pos_weight': pos_weight,\n",
    "        'mean': mean,\n",
    "        'std': std,\n",
    "        'device': device,\n",
    "        'train_setup': {\n",
    "            'x': x_trn,\n",
    "            'y_step': y_step_trn,\n",
    "            'y_binary': y_binary_trn,\n",
    "            'mask': mask_trn,\n",
    "            'lengths': lengths_trn\n",
    "        },\n",
    "        'val_setup': {\n",
    "            'x': apply_scalers(val_data['seq'], mean, std).to(device),\n",
    "            'y_step': val_data['probs'].float().to(device),\n",
    "            'y_binary': val_data['binary'].float().to(device),\n",
    "            'mask': val_data['mask'].bool().to(device),\n",
    "            'lengths': val_data['lengths'].float().to(device)\n",
    "        }\n",
    "    }\n",
    "\n",
    "    return model_setup\n",
    "\n",
    "# move tensors to device\n",
    "    # mostly a helper function for training data \n",
    "def move_tensors_to_device(\n",
    "        x: torch.Tensor,\n",
    "        train_data: dict,\n",
    "        device: str\n",
    ") -> tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Move tensors to the specified device.\n",
    "    \n",
    "    Args:\n",
    "        x (torch.Tensor): Input tensor.\n",
    "        train_data (dict): Dictionary containing training data tensors.\n",
    "        device (str): Device to move tensors to ('cpu' or 'cuda').\n",
    "    \n",
    "    Returns:\n",
    "        tuple: Tensors moved to the specified device.\n",
    "    \"\"\"\n",
    "    return x.float().to(device), train_data['probs'].float().to(device), train_data['binary'].float().to(device), train_data['mask'].bool().to(device), train_data['lengths'].long().to(device)\n",
    "    \n",
    "# early stopping check\n",
    "def check_early_stopping(\n",
    "        val_loss: float,\n",
    "        best_val_loss: float,\n",
    "        epochs_no_improve: int,\n",
    "        patience: int\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Check if early stopping criteria are met.\n",
    "    \n",
    "    Args:\n",
    "        val_loss (float): Current validation loss.\n",
    "        best_val_loss (float): Best validation loss observed so far.\n",
    "        epochs_no_improve (int): Number of epochs since last improvement.\n",
    "        patience (int): Patience for early stopping.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary indicating whether to stop, if there's a new best, updated best loss, and epochs without improvement.\n",
    "    \"\"\"\n",
    "    if val_loss < best_val_loss:\n",
    "        return {\n",
    "            'should_stop': False,\n",
    "            'new_best': True,\n",
    "            'best_val_loss': val_loss,\n",
    "            'epochs_no_improve': 0\n",
    "        }\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        if epochs_no_improve >= patience:\n",
    "            return {\n",
    "                'should_stop': True,\n",
    "                'new_best': False,\n",
    "                'best_val_loss': best_val_loss,\n",
    "                'epochs_no_improve': epochs_no_improve\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                'should_stop': False,\n",
    "                'new_best': False,\n",
    "                'best_val_loss': best_val_loss,\n",
    "                'epochs_no_improve': epochs_no_improve\n",
    "            }\n",
    "\n",
    "# ROC/AUC calcs\n",
    "def compute_roc_auc( \n",
    "        probs: torch.Tensor,\n",
    "        y_true: torch.Tensor,\n",
    "        mask: torch.Tensor\n",
    ") -> tuple[float, float, float]:\n",
    "    \"\"\"\n",
    "    Compute ROC AUC score.\n",
    "    \n",
    "    Args:\n",
    "        probs (torch.Tensor): Predicted probabilities.\n",
    "        y_true (torch.Tensor): True labels.\n",
    "        mask (torch.Tensor): Mask indicating valid entries.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (batch_scores (torch.Tensor), batch_labels (torch.Tensor), roc_auc (float))\n",
    "    \"\"\"\n",
    "    # per-batch valid scores/labels for AUC (use binary labels)\n",
    "    batch_scores = probs.detach().float()\n",
    "    batch_labels = y_true[mask].detach().float()\n",
    "\n",
    "    # compute AUC\n",
    "    if batch_labels.sum() == 0 or batch_labels.sum() == len(batch_labels):\n",
    "        roc_auc = 0.5 \n",
    "    else:\n",
    "        roc_auc = roc_auc_score(batch_labels.cpu().numpy(), batch_scores.cpu().numpy())\n",
    "\n",
    "    return batch_scores, batch_labels, roc_auc\n",
    "\n",
    "# training loop for a model\n",
    "    # NOTE: sets up a dictionary for storing model loss history\n",
    "def train_model(\n",
    "        model_name: str,\n",
    "        model_setup: dict,\n",
    "        n_epochs: int = 100,\n",
    "        batch_size: int = 32,\n",
    "        patience: int = 10,\n",
    "        verbose: bool = True\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Train the model using the provided training and validation data. Manually handles batching and early stopping.\n",
    "    \n",
    "    Args:\n",
    "        model_setup (dict): Dictionary containing model, optimizer, loss function, and data setup.\n",
    "        n_epochs (int): Number of epochs to train. Defaults to 100.\n",
    "        batch_size (int): Batch size for training. Defaults to 32.\n",
    "        patience (int): Patience for early stopping. Defaults to 10.\n",
    "        verbose (bool): Whether to print training progress. Defaults to True.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary containing training and validation loss history.\n",
    "    \"\"\"\n",
    "    # unpack model setup\n",
    "    model = model_setup['model']\n",
    "    optimizer = model_setup['optimizer']\n",
    "    loss_fn = model_setup['loss_fn']\n",
    "    pos_weight = model_setup['pos_weight']\n",
    "\n",
    "    # extract training data\n",
    "    x = model_setup['train_setup']['x']\n",
    "    y_step_binary = model_setup['train_setup']['y_binary']\n",
    "    mask = model_setup['train_setup']['mask']\n",
    "    lengths = model_setup['train_setup']['lengths']\n",
    "\n",
    "    # setup loss storage\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'val_loss': [],\n",
    "        'best_val_loss': None,\n",
    "        'stopped_epoch': None\n",
    "    }\n",
    "\n",
    "    # early stopping setup\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "\n",
    "    # MAIN LOOP: iterate through epochs\n",
    "    B = x.shape[0]\n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        \n",
    "        \"\"\" TRAINING \"\"\"\n",
    "        # set model to train mode, shuffle indices, create batches\n",
    "        model.train()\n",
    "        idx = torch.randperm(B, device=model_setup['device'])  # shuffle indices each epoch\n",
    "\n",
    "        # loss counters\n",
    "        running_loss = 0.0\n",
    "\n",
    "        # wrap range() with tqdm\n",
    "        pbar = tqdm(range(0, B, batch_size), desc=f\"Epoch {epoch}/{n_epochs}\", leave=False)\n",
    "        for start in pbar:\n",
    "            end = min(start + batch_size, B)\n",
    "            bidx = idx[start:end]\n",
    "\n",
    "            # forward pass\n",
    "            xb = x[bidx]\n",
    "            yb = y_step_binary[bidx]\n",
    "            mb = mask[bidx]\n",
    "            Lb = lengths[bidx]\n",
    "\n",
    "            # update loss\n",
    "            logits = model(xb, Lb)\n",
    "            loss = F.binary_cross_entropy_with_logits(\n",
    "                logits[mb], yb[mb], pos_weight=pos_weight\n",
    "            )\n",
    "\n",
    "            # back propagation\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            # running loss\n",
    "            bs = xb.size(0)\n",
    "            running_loss += loss.item() * bs\n",
    "            \n",
    "            # running accuracy\n",
    "            probs  = torch.sigmoid(logits[mb])\n",
    "            preds  = (probs > 0.5).float()\n",
    "            correct = (preds == y_step_binary[bidx][mb]).sum().item()\n",
    "            total   = mb.sum().item()\n",
    "\n",
    "            # shape checks\n",
    "            assert preds.shape == y_step_binary[bidx][mb].shape\n",
    "            assert probs.shape == y_step_binary[bidx][mb].shape\n",
    "\n",
    "            # update loss\n",
    "            run_avg_loss = running_loss / ((start // batch_size + 1) * bs)\n",
    "\n",
    "            # show both current and running losses in the bar\n",
    "            pbar.set_postfix(loss=f\"{run_avg_loss:.4f}\")\n",
    "\n",
    "        # update epoch loss\n",
    "        epoch_loss = running_loss / B\n",
    "        \n",
    "        # print epoch update\n",
    "        if verbose:\n",
    "            print(f\"Epoch {epoch:02d} | Training Loss: {epoch_loss:.3f}\")\n",
    "\n",
    "        \"\"\" VALIDATION \"\"\"\n",
    "        # set model to eval mode\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            logits = model(\n",
    "                model_setup['val_setup']['x'].float(), \n",
    "                model_setup['val_setup']['lengths'].float()\n",
    "            )\n",
    "            val_loss = loss_fn(\n",
    "                logits,\n",
    "                model_setup['val_setup']['y_step'],\n",
    "                model_setup['val_setup']['mask'],\n",
    "                pos_weight\n",
    "            ).item()\n",
    "            history['val_loss'].append(val_loss)\n",
    "            \n",
    "            # print update\n",
    "            if verbose:\n",
    "                print(f\"Epoch {epoch:02d} | Validation Loss: {val_loss:.3f}\")\n",
    "\n",
    "            \"\"\" EARLY STOPPING CHECK \"\"\"\n",
    "            early_stop_info = check_early_stopping(\n",
    "                val_loss, best_val_loss, epochs_no_improve, patience\n",
    "            )\n",
    "            \n",
    "            # update values with results\n",
    "            best_val_loss = early_stop_info['best_val_loss']\n",
    "            epochs_no_improve = early_stop_info['epochs_no_improve']\n",
    "\n",
    "            # option 1: new best --> save intermediate model\n",
    "            if early_stop_info['new_best']:\n",
    "\n",
    "                # update history\n",
    "                history['best_val_loss'] = best_val_loss\n",
    "                \n",
    "                # save model (+ state dict)\n",
    "                    # ensure model is in eval mode before saving\n",
    "                model.eval()\n",
    "                torch.save(model, f\"models/inj/{model_name}.pt\")\n",
    "                torch.save(model.state_dict(), f\"models/inj/{model_name}_state_dict.pt\")\n",
    "\n",
    "                # print update\n",
    "                if verbose:\n",
    "                    print(f\"New best model saved with validation loss: {best_val_loss:.3f}\")\n",
    "\n",
    "            # option 2: not best, but not yet time to stop --> continue\n",
    "            if not early_stop_info['should_stop']:\n",
    "                continue\n",
    "\n",
    "            # option 3: stop training early\n",
    "            if early_stop_info['should_stop']:\n",
    "                if verbose:\n",
    "                    print(f\"Early stopping triggered after {epoch} epochs. Best validation loss: {best_val_loss:.3f}\")\n",
    "                \n",
    "                # add to history\n",
    "                history['stopped_epoch'] = epoch\n",
    "\n",
    "                # NOTE: don't need to save model here, since we already saved the best one above\n",
    "                \n",
    "                return history\n",
    "\n",
    "    # NOTE: if we get here, training completed without early stopping\n",
    "    if verbose:\n",
    "        print(f\"Training completed after {epoch} epochs with best validation loss: {best_val_loss:.3f}\")\n",
    "\n",
    "    # save final model\n",
    "        # ensure model is in eval mode before saving\n",
    "    model.eval()\n",
    "    torch.save(model, f\"models/inj/{model_name}.pt\")\n",
    "    torch.save(model.state_dict(), f\"models/inj/{model_name}_state_dict.pt\")\n",
    "\n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "c2dd2b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model setups for outing- and pitch-level data by day windows\n",
    "model_setups = {}\n",
    "model_results = {}\n",
    "\n",
    "# compile models for each day window\n",
    "for day in [7, 15, 30, 45, 90]:\n",
    "    model_setups[day] = compile_model(\n",
    "        train_data=pitch_level_tensors['trn'][day],\n",
    "        val_data=pitch_level_tensors['val'][day],\n",
    "        use_pos_weight=True\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training pitch-level model for preceding day window: 7 days\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | Training Loss: 1.266\n",
      "Epoch 01 | Validation Loss: 1.043\n",
      "New best model saved with validation loss: 1.043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 02 | Training Loss: 1.037\n",
      "Epoch 02 | Validation Loss: 1.013\n",
      "New best model saved with validation loss: 1.013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 03 | Training Loss: 1.010\n",
      "Epoch 03 | Validation Loss: 0.939\n",
      "New best model saved with validation loss: 0.939\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 04 | Training Loss: 0.941\n",
      "Epoch 04 | Validation Loss: 1.050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 05 | Training Loss: 0.981\n",
      "Epoch 05 | Validation Loss: 0.874\n",
      "New best model saved with validation loss: 0.874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 06 | Training Loss: 0.817\n",
      "Epoch 06 | Validation Loss: 0.890\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 07 | Training Loss: 0.835\n",
      "Epoch 07 | Validation Loss: 0.796\n",
      "New best model saved with validation loss: 0.796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 08 | Training Loss: 0.825\n",
      "Epoch 08 | Validation Loss: 0.790\n",
      "New best model saved with validation loss: 0.790\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 09 | Training Loss: 0.764\n",
      "Epoch 09 | Validation Loss: 0.822\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 | Training Loss: 0.799\n",
      "Epoch 10 | Validation Loss: 0.848\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 | Training Loss: 0.874\n",
      "Epoch 11 | Validation Loss: 0.801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 | Training Loss: 0.792\n",
      "Epoch 12 | Validation Loss: 0.786\n",
      "New best model saved with validation loss: 0.786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 | Training Loss: 0.740\n",
      "Epoch 13 | Validation Loss: 0.810\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 | Training Loss: 0.737\n",
      "Epoch 14 | Validation Loss: 0.826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 | Training Loss: 0.749\n",
      "Epoch 15 | Validation Loss: 0.840\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 | Training Loss: 0.706\n",
      "Epoch 16 | Validation Loss: 0.804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 | Training Loss: 0.709\n",
      "Epoch 17 | Validation Loss: 0.763\n",
      "New best model saved with validation loss: 0.763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 | Training Loss: 0.699\n",
      "Epoch 18 | Validation Loss: 0.818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19 | Training Loss: 0.812\n",
      "Epoch 19 | Validation Loss: 0.783\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 | Training Loss: 0.714\n",
      "Epoch 20 | Validation Loss: 0.790\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21 | Training Loss: 0.691\n",
      "Epoch 21 | Validation Loss: 0.769\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22 | Training Loss: 0.679\n",
      "Epoch 22 | Validation Loss: 0.750\n",
      "New best model saved with validation loss: 0.750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23 | Training Loss: 0.677\n",
      "Epoch 23 | Validation Loss: 0.804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24 | Training Loss: 0.677\n",
      "Epoch 24 | Validation Loss: 0.739\n",
      "New best model saved with validation loss: 0.739\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25 | Training Loss: 0.649\n",
      "Epoch 25 | Validation Loss: 0.752\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26 | Training Loss: 0.632\n",
      "Epoch 26 | Validation Loss: 0.746\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27 | Training Loss: 0.653\n",
      "Epoch 27 | Validation Loss: 0.757\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28 | Training Loss: 0.642\n",
      "Epoch 28 | Validation Loss: 0.769\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29 | Training Loss: 0.629\n",
      "Epoch 29 | Validation Loss: 0.782\n",
      "Early stopping triggered after 29 epochs. Best validation loss: 0.739\n",
      "[AWS]: Uploaded object to s3://pitch-ml/epidemiology/ml/models/inj/pitch_model_7d_results.pkl\n"
     ]
    }
   ],
   "source": [
    "\"\"\" PITCH-LEVEL MODELS \"\"\"\n",
    "# for day in [7, 15, 30, 45, 90]:\n",
    "for day in [7]:\n",
    "    \n",
    "    print(f'Training pitch-level model for preceding day window: {day} days')\n",
    "    \n",
    "    # pitch-level model loop\n",
    "    pitch_model_results = train_model(\n",
    "        model_name=f'pitch_model_{day}d',\n",
    "        model_setup=model_setups[day],\n",
    "        n_epochs=100,\n",
    "        batch_size=32,\n",
    "        patience=5,\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "    # save to disk, upload to S3\n",
    "    with open(f'storage/pitch_model_{day}d_results.pkl', 'wb') as f:\n",
    "        pickle.dump(pitch_model_results, f)\n",
    "    with open(f'storage/pitch_model_{day}d_results.pkl', 'rb') as f:\n",
    "        content = f.read()\n",
    "        aws.upload_to_s3(content, f'epidemiology/ml/models/inj/pitch_model_{day}d_results.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c2a183",
   "metadata": {},
   "source": [
    "$\\textbf{Save Preprocessed Training/Validation Data}$\n",
    "\n",
    "Mostly to save the scaled data, since saving scalers isn't finalized yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "4919c7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for day in [7, 15, 30, 45, 90]:\n",
    "    # get model setup\n",
    "    day_setup_train = model_setups[day]['train_setup']\n",
    "    day_setup_val = model_setups[day]['val_setup']\n",
    "\n",
    "    # save to disk\n",
    "    with open(f'models/data/pitch_model_{day}d_trn_data.pkl', 'wb') as f:\n",
    "        pickle.dump(day_setup_train, f)\n",
    "    with open(f'models/data/pitch_model_{day}d_val_data.pkl', 'wb') as f:\n",
    "        pickle.dump(day_setup_val, f)\n",
    "\n",
    "    # TODO: upload to S3\n",
    "    # with open(f'models/data/pitch_model_{day}d_trn_data.pkl', 'rb') as f:\n",
    "    #     content = f.read()\n",
    "    #     aws.upload_to_s3(content, f'epidemiology/ml/models/inj/pitch_model_{day}d_trn_data.pkl')\n",
    "    # with open(f'models/data/pitch_model_{day}d_val_data.pkl', 'rb') as f:\n",
    "    #     content = f.read()\n",
    "    #     aws.upload_to_s3(content, f'epidemiology/ml/models/inj/pitch_model_{day}d_val_data.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99a2671",
   "metadata": {},
   "source": [
    "$\\textbf{Close AWS Connection}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "a21578d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AWS]: No active connection to close.\n"
     ]
    }
   ],
   "source": [
    "# close connection\n",
    "aws.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc1e350",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pitch_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
