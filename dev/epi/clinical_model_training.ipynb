{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f63445a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f5062c",
   "metadata": {},
   "source": [
    "$\\textbf{Epidemiology: Clinical Model Training}$\n",
    "\n",
    "Injury risk estimation is possible from two perspectives: \n",
    "- __Season__ (i.e., post-outing injury probability; postgame)\n",
    "- __Pitch-Level__ (i.e., next-pitch injury probability; within game)\n",
    "\n",
    "This notebook uses the data structures and model architecture established in `clinical_model_setup.ipynb`. The architecture was shown to work with a manually crafted batched-training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "18b3e8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download most up-to-date tensor dictionaries\n",
    "    # TODO: wait until the tensors are uploaded to S3\n",
    "# pitch_level_tensors = aws.s3.download_file(\n",
    "#     aws.bucket_name, \n",
    "#     'epidemiology/ml/datasets/pytorch/pitch_level_tensors.pkl', \n",
    "#     'storage/pitch_level_tensors.pkl'\n",
    "# )\n",
    "\n",
    "# load tensors into memory\n",
    "with open('storage/pitch_level_tensors.pkl', 'rb') as f:\n",
    "    pitch_level_tensors = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693616b9",
   "metadata": {},
   "source": [
    "$\\textbf{Model Development}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25794636",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "from nnet import CNNbiLSTM\n",
    "import torch.nn.functional as F\n",
    "from nnet.loss_functions import pitch_level_loss\n",
    "from sklearn.metrics import roc_auc_score, f1_score\n",
    "from services.scaling import compute_masked_scalers, apply_scalers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2e9496e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile model for training --> model, optimizer, loss function, and data setup\n",
    "def compile_model(\n",
    "        train_data: dict,\n",
    "        val_data: dict,\n",
    "        model_config: dict = {\n",
    "            'stem': 64,\n",
    "            'c': 96,\n",
    "            'kernel': 7,\n",
    "            'lstm_hidden': 128,\n",
    "            'dropout': 0.1,\n",
    "            'bidir': True\n",
    "        },\n",
    "        device: str = 'cpu',\n",
    "        use_pos_weight: bool = False\n",
    ") -> dict:\n",
    "    \"\"\" \n",
    "    Compile a nnet architecture for training. Includes scaling, model instantiation, and loss function setup.\n",
    "    \n",
    "    Args:\n",
    "        train_data (dict): Training data containing tensors and masks.\n",
    "        val_data (dict): Validation data containing tensors and masks.\n",
    "        model_config (dict): Configuration for the CNNbiLSTM model. Defaults to basic setup.\n",
    "        device (str): Device to run the model on ('cpu' or 'cuda').\n",
    "        use_pos_weight (bool): Whether to use positive class weights in loss function.\n",
    "    \n",
    "    Returns:\n",
    "        model_setup (dict): Dictionary with all relevant information..\n",
    "    \"\"\"\n",
    "\n",
    "    # standardize example sequence --> example training tensor sequence (x)\n",
    "    mean, std = compute_masked_scalers(train_data['seq'], train_data['mask'])\n",
    "    x = apply_scalers(train_data['seq'], mean, std)\n",
    "    B, T, K = x.shape       # NOTE: B = batch size, T = time steps, K = features; K is used for model setup\n",
    "\n",
    "    # setup (device, shapes)\n",
    "        # --> move full tensors to device\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    x_trn, y_binary_trn, mask_trn, lengths_trn = move_tensors_to_device(x, train_data, device)\n",
    "\n",
    "    # setup model, optimizer\n",
    "    model = CNNbiLSTM(\n",
    "        k_in=K, \n",
    "        stem=model_config.get('stem', 64),\n",
    "        c=model_config.get('c', 96),\n",
    "        kernel=model_config.get('kernel', 7),\n",
    "        lstm_hidden=model_config.get('lstm_hidden', 128),\n",
    "        dropout=model_config.get('dropout', 0.1),\n",
    "        bidir=model_config.get('bidir', True),\n",
    "    ).to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "\n",
    "    # optional: compute pos_weight over valid steps once\n",
    "        # NOTE: for probs this should be 1.0\n",
    "    if use_pos_weight:\n",
    "        with torch.no_grad():\n",
    "            pos = y_binary_trn.sum()\n",
    "            neg = y_binary_trn.shape[0] - pos\n",
    "            pos_weight = (neg / pos.clamp(min=1)).float()\n",
    "    else:\n",
    "        pos_weight = torch.tensor(1.0, device=device)\n",
    "\n",
    "    # package all into dictionary\n",
    "    model_setup = {\n",
    "        'model': model.train(),\n",
    "        'optimizer': optimizer,\n",
    "        'loss_fn': pitch_level_loss,\n",
    "        'pos_weight': pos_weight,\n",
    "        'mean': mean,\n",
    "        'std': std,\n",
    "        'device': device,\n",
    "        'train_setup': {\n",
    "            'x': x_trn,\n",
    "            'y_binary': y_binary_trn,\n",
    "            'mask': mask_trn,\n",
    "            'lengths': lengths_trn\n",
    "        },\n",
    "        'val_setup': {\n",
    "            'x': apply_scalers(val_data['seq'], mean, std).to(device),\n",
    "            'y_binary': val_data['binary'].float().to(device),\n",
    "            'mask': val_data['mask'].bool().to(device),\n",
    "            'lengths': val_data['lengths'].float().to(device)\n",
    "        }\n",
    "    }\n",
    "\n",
    "    return model_setup\n",
    "\n",
    "# move tensors to device\n",
    "    # mostly a helper function for training data \n",
    "def move_tensors_to_device(\n",
    "        x: torch.Tensor,\n",
    "        train_data: dict,\n",
    "        device: str\n",
    ") -> tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Move tensors to the specified device.\n",
    "    \n",
    "    Args:\n",
    "        x (torch.Tensor): Input tensor.\n",
    "        train_data (dict): Dictionary containing training data tensors.\n",
    "        device (str): Device to move tensors to ('cpu' or 'cuda').\n",
    "    \n",
    "    Returns:\n",
    "        tuple: Tensors moved to the specified device.\n",
    "    \"\"\"\n",
    "    return x.float().to(device), train_data['binary'].float().to(device), train_data['mask'].bool().to(device), train_data['lengths'].long().to(device)\n",
    "    \n",
    "# early stopping check\n",
    "def check_early_stopping(\n",
    "        val_loss: float,\n",
    "        best_val_loss: float,\n",
    "        epochs_no_improve: int,\n",
    "        patience: int\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Check if early stopping criteria are met.\n",
    "    \n",
    "    Args:\n",
    "        val_loss (float): Current validation loss.\n",
    "        best_val_loss (float): Best validation loss observed so far.\n",
    "        epochs_no_improve (int): Number of epochs since last improvement.\n",
    "        patience (int): Patience for early stopping.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary indicating whether to stop, if there's a new best, updated best loss, and epochs without improvement.\n",
    "    \"\"\"\n",
    "    if val_loss < best_val_loss:\n",
    "        return {\n",
    "            'should_stop': False,\n",
    "            'new_best': True,\n",
    "            'best_val_loss': val_loss,\n",
    "            'epochs_no_improve': 0\n",
    "        }\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        if epochs_no_improve >= patience:\n",
    "            return {\n",
    "                'should_stop': True,\n",
    "                'new_best': False,\n",
    "                'best_val_loss': best_val_loss,\n",
    "                'epochs_no_improve': epochs_no_improve\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                'should_stop': False,\n",
    "                'new_best': False,\n",
    "                'best_val_loss': best_val_loss,\n",
    "                'epochs_no_improve': epochs_no_improve\n",
    "            }\n",
    "\n",
    "# ROC/AUC calcs\n",
    "def compute_roc_auc( \n",
    "        probs: torch.Tensor,\n",
    "        y_true: torch.Tensor,\n",
    "        mask: torch.Tensor\n",
    ") -> tuple[float, float, float]:\n",
    "    \"\"\"\n",
    "    Compute ROC AUC score.\n",
    "    \n",
    "    Args:\n",
    "        probs (torch.Tensor): Predicted probabilities.\n",
    "        y_true (torch.Tensor): True labels.\n",
    "        mask (torch.Tensor): Mask indicating valid entries.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (batch_scores (torch.Tensor), batch_labels (torch.Tensor), roc_auc (float))\n",
    "    \"\"\"\n",
    "    # per-batch valid scores/labels for AUC (use binary labels)\n",
    "    batch_scores = probs.detach().float()\n",
    "    batch_labels = y_true[mask].detach().float()\n",
    "\n",
    "    # compute AUC\n",
    "    if batch_labels.sum() == 0 or batch_labels.sum() == len(batch_labels):\n",
    "        roc_auc = 0.5 \n",
    "    else:\n",
    "        roc_auc = roc_auc_score(batch_labels.cpu().numpy(), batch_scores.cpu().numpy())\n",
    "\n",
    "    return batch_scores, batch_labels, roc_auc\n",
    "\n",
    "# training loop for a model\n",
    "    # NOTE: sets up a dictionary for storing model loss history\n",
    "def train_model(\n",
    "        model_name: str,\n",
    "        model_setup: dict,\n",
    "        n_epochs: int = 100,\n",
    "        batch_size: int = 32,\n",
    "        patience: int = 10,\n",
    "        verbose: bool = True\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Train the model using the provided training and validation data. Manually handles batching and early stopping.\n",
    "    \n",
    "    Args:\n",
    "        model_setup (dict): Dictionary containing model, optimizer, loss function, and data setup.\n",
    "        n_epochs (int): Number of epochs to train. Defaults to 100.\n",
    "        batch_size (int): Batch size for training. Defaults to 32.\n",
    "        patience (int): Patience for early stopping. Defaults to 10.\n",
    "        verbose (bool): Whether to print training progress. Defaults to True.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary containing training and validation loss history.\n",
    "    \"\"\"\n",
    "    # unpack model setup\n",
    "    model = model_setup['model']\n",
    "    optimizer = model_setup['optimizer']\n",
    "    loss_fn = model_setup['loss_fn']\n",
    "    pos_weight = model_setup['pos_weight']\n",
    "\n",
    "    # extract training data\n",
    "    x = model_setup['train_setup']['x']\n",
    "    y_step_binary = model_setup['train_setup']['y_binary']\n",
    "    mask = model_setup['train_setup']['mask']\n",
    "    lengths = model_setup['train_setup']['lengths']\n",
    "\n",
    "    # setup loss storage\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'val_loss': [],\n",
    "        'best_val_loss': None,\n",
    "        'stopped_epoch': None\n",
    "    }\n",
    "\n",
    "    # loss counters\n",
    "    running_loss = 0.0\n",
    "    running_correct = 0\n",
    "    running_total = 0\n",
    "\n",
    "    # early stopping setup\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "\n",
    "    # MAIN LOOP: iterate through epochs\n",
    "    B = x.shape[0]\n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        \n",
    "        \"\"\" TRAINING \"\"\"\n",
    "        # set model to train mode, shuffle indices, create batches\n",
    "        model.train()\n",
    "        idx = torch.randperm(B, device=model_setup['device'])  # shuffle indices each epoch\n",
    "\n",
    "        # loss counters\n",
    "        running_loss = 0.0\n",
    "\n",
    "        # wrap range() with tqdm\n",
    "        pbar = tqdm(range(0, B, batch_size), desc=f\"Epoch {epoch}/{n_epochs}\", leave=False)\n",
    "        for start in pbar:\n",
    "            end = min(start + batch_size, B)\n",
    "            bidx = idx[start:end]\n",
    "\n",
    "            # forward pass setup\n",
    "            xb = x[bidx]                        # [B,T,K]\n",
    "            yb = y_step_binary[bidx]            # [B] sequence-level labels (0/1)\n",
    "            mb = mask[bidx]                     # [B,T]\n",
    "            Lb = lengths[bidx]                  # [B]\n",
    "\n",
    "            # forward pass: get per-pitch + sequence logits\n",
    "            logits_step, logit_seq, attn = model(xb, Lb, mb)\n",
    "\n",
    "            # main sequence-level loss\n",
    "            loss = F.binary_cross_entropy_with_logits(\n",
    "                logit_seq, yb.float(), pos_weight=pos_weight\n",
    "            )\n",
    "\n",
    "            # back propagation\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            # running loss\n",
    "            bs = xb.size(0)\n",
    "            running_loss += loss.item() * bs\n",
    "            \n",
    "        # accuracy (sequence-level)\n",
    "            probs  = torch.sigmoid(logit_seq)\n",
    "            preds  = (probs > 0.5).float()\n",
    "            correct = (preds == yb).sum().item()\n",
    "            total   = yb.size(0)\n",
    "\n",
    "            # update total\n",
    "            running_correct += correct\n",
    "            running_total += total\n",
    "\n",
    "            # update accuracy\n",
    "            run_avg_loss = running_loss / ((start // batch_size + 1) * bs)\n",
    "            run_acc      = running_correct / running_total\n",
    "\n",
    "            # show both current and running losses in the bar\n",
    "            pbar.set_postfix(loss=f\"{loss.item():.4f}\", acc=f\"{run_acc:.4f}\")\n",
    "\n",
    "        # update epoch loss\n",
    "        epoch_loss = running_loss / B\n",
    "        \n",
    "        # print epoch update\n",
    "        if verbose:\n",
    "            print(f\"Epoch {epoch:02d} | Training Loss: {epoch_loss:.3f}\")\n",
    "\n",
    "        \"\"\" VALIDATION \"\"\"\n",
    "        # set model to eval mode\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            _, logits, _ = model(\n",
    "                model_setup['val_setup']['x'].float(), \n",
    "                model_setup['val_setup']['lengths'].float(),\n",
    "                model_setup['val_setup']['mask'].bool()\n",
    "            )\n",
    "            val_loss = F.binary_cross_entropy_with_logits(\n",
    "                logits, \n",
    "                model_setup['val_setup']['y_binary'], \n",
    "                pos_weight=model_setup['pos_weight']\n",
    "            ).item()\n",
    "            history['val_loss'].append(val_loss)\n",
    "            \n",
    "            # print update\n",
    "            if verbose:\n",
    "                print(f\"Epoch {epoch:02d} | Validation Loss: {val_loss:.3f}\")\n",
    "\n",
    "            \"\"\" EARLY STOPPING CHECK \"\"\"\n",
    "            early_stop_info = check_early_stopping(\n",
    "                val_loss, best_val_loss, epochs_no_improve, patience\n",
    "            )\n",
    "            \n",
    "            # update values with results\n",
    "            best_val_loss = early_stop_info['best_val_loss']\n",
    "            epochs_no_improve = early_stop_info['epochs_no_improve']\n",
    "\n",
    "            # option 1: new best --> save intermediate model\n",
    "            if early_stop_info['new_best']:\n",
    "\n",
    "                # update history\n",
    "                history['best_val_loss'] = best_val_loss\n",
    "                \n",
    "                # save model (+ state dict)\n",
    "                    # ensure model is in eval mode before saving\n",
    "                model.eval()\n",
    "                torch.save(model, f\"models/inj/{model_name}.pt\")\n",
    "                torch.save(model.state_dict(), f\"models/inj/{model_name}_state_dict.pt\")\n",
    "\n",
    "                # print update\n",
    "                if verbose:\n",
    "                    print(f\"New best model saved with validation loss: {best_val_loss:.3f}\")\n",
    "\n",
    "            # option 2: not best, but not yet time to stop --> continue\n",
    "            if not early_stop_info['should_stop']:\n",
    "                continue\n",
    "\n",
    "            # option 3: stop training early\n",
    "            if early_stop_info['should_stop']:\n",
    "                if verbose:\n",
    "                    print(f\"Early stopping triggered after {epoch} epochs. Best validation loss: {best_val_loss:.3f}\")\n",
    "                \n",
    "                # add to history\n",
    "                history['stopped_epoch'] = epoch\n",
    "\n",
    "                # NOTE: don't need to save model here, since we already saved the best one above\n",
    "                \n",
    "                return history\n",
    "\n",
    "    # NOTE: if we get here, training completed without early stopping\n",
    "    if verbose:\n",
    "        print(f\"Training completed after {epoch} epochs with best validation loss: {best_val_loss:.3f}\")\n",
    "\n",
    "    # save final model\n",
    "        # ensure model is in eval mode before saving\n",
    "    model.eval()\n",
    "    torch.save(model, f\"models/inj/{model_name}.pt\")\n",
    "    torch.save(model.state_dict(), f\"models/inj/{model_name}_state_dict.pt\")\n",
    "\n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c2dd2b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model setups for outing- and pitch-level data by day windows\n",
    "model_setups = {}\n",
    "model_results = {}\n",
    "\n",
    "# compile models for each day window\n",
    "model_setup = compile_model(\n",
    "    train_data=pitch_level_tensors['trn'],\n",
    "    val_data=pitch_level_tensors['val'],\n",
    "    use_pos_weight=True\n",
    ")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training pitch-level model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining pitch-level model...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# pitch-level model loop\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m pitch_model_results \u001b[38;5;241m=\u001b[39m train_model(\n\u001b[1;32m      6\u001b[0m     model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpitch_model\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      7\u001b[0m     model_setup\u001b[38;5;241m=\u001b[39mmodel_setup,\n\u001b[1;32m      8\u001b[0m     n_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m      9\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m,\n\u001b[1;32m     10\u001b[0m     patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m,\n\u001b[1;32m     11\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     12\u001b[0m )\n",
      "Cell \u001b[0;32mIn[23], line 266\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model_name, model_setup, n_epochs, batch_size, patience, verbose)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# back propagation\u001b[39;00m\n\u001b[1;32m    265\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad(set_to_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 266\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m    267\u001b[0m nn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;241m1.0\u001b[39m)\n\u001b[1;32m    268\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m/opt/miniconda3/envs/pitch_ml/lib/python3.11/site-packages/torch/_tensor.py:648\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    638\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    639\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    640\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    641\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    646\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    647\u001b[0m     )\n\u001b[0;32m--> 648\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[1;32m    649\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[1;32m    650\u001b[0m )\n",
      "File \u001b[0;32m/opt/miniconda3/envs/pitch_ml/lib/python3.11/site-packages/torch/autograd/__init__.py:353\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    348\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    350\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 353\u001b[0m _engine_run_backward(\n\u001b[1;32m    354\u001b[0m     tensors,\n\u001b[1;32m    355\u001b[0m     grad_tensors_,\n\u001b[1;32m    356\u001b[0m     retain_graph,\n\u001b[1;32m    357\u001b[0m     create_graph,\n\u001b[1;32m    358\u001b[0m     inputs,\n\u001b[1;32m    359\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    360\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    361\u001b[0m )\n",
      "File \u001b[0;32m/opt/miniconda3/envs/pitch_ml/lib/python3.11/site-packages/torch/autograd/graph.py:824\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    822\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    823\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 824\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    825\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    826\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    827\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    828\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\" TRAIN MODEL \"\"\"\n",
    "print(f'Training pitch-level model...')\n",
    "\n",
    "# pitch-level model loop\n",
    "pitch_model_results = train_model(\n",
    "    model_name=f'pitch_model',\n",
    "    model_setup=model_setup,\n",
    "    n_epochs=2,\n",
    "    batch_size=32,\n",
    "    patience=5,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# # save to disk, upload to S3\n",
    "# with open(f'storage/pitch_model_{day}d_results.pkl', 'wb') as f:\n",
    "#     pickle.dump(pitch_model_results, f)\n",
    "# with open(f'storage/pitch_model_{day}d_results.pkl', 'rb') as f:\n",
    "#     content = f.read()\n",
    "#     aws.upload_to_s3(content, f'epidemiology/ml/models/inj/pitch_model_{day}d_results.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c2a183",
   "metadata": {},
   "source": [
    "$\\textbf{Save Preprocessed Training/Validation Data}$\n",
    "\n",
    "Mostly to save the scaled data, since saving scalers isn't finalized yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "4919c7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for day in [7, 15, 30, 45, 90]:\n",
    "    # get model setup\n",
    "    day_setup_train = model_setups[day]['train_setup']\n",
    "    day_setup_val = model_setups[day]['val_setup']\n",
    "\n",
    "    # save to disk\n",
    "    with open(f'models/data/pitch_model_{day}d_trn_data.pkl', 'wb') as f:\n",
    "        pickle.dump(day_setup_train, f)\n",
    "    with open(f'models/data/pitch_model_{day}d_val_data.pkl', 'wb') as f:\n",
    "        pickle.dump(day_setup_val, f)\n",
    "\n",
    "    # TODO: upload to S3\n",
    "    # with open(f'models/data/pitch_model_{day}d_trn_data.pkl', 'rb') as f:\n",
    "    #     content = f.read()\n",
    "    #     aws.upload_to_s3(content, f'epidemiology/ml/models/inj/pitch_model_{day}d_trn_data.pkl')\n",
    "    # with open(f'models/data/pitch_model_{day}d_val_data.pkl', 'rb') as f:\n",
    "    #     content = f.read()\n",
    "    #     aws.upload_to_s3(content, f'epidemiology/ml/models/inj/pitch_model_{day}d_val_data.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99a2671",
   "metadata": {},
   "source": [
    "$\\textbf{Close AWS Connection}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21578d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AWS]: No active connection to close.\n"
     ]
    }
   ],
   "source": [
    "# close connection\n",
    "aws.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc1e350",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pitch_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
