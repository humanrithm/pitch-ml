{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f63445a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f5062c",
   "metadata": {},
   "source": [
    "$\\textbf{Epidemiology: Clinical Model Training}$\n",
    "\n",
    "Injury risk estimation is possible from two perspectives: \n",
    "- __Season__ (i.e., post-outing injury probability; postgame)\n",
    "- __Pitch-Level__ (i.e., next-pitch injury probability; within game)\n",
    "\n",
    "This notebook uses the data structures and model architecture established in `clinical_model_setup.ipynb`. The architecture was shown to work with a manually crafted batched-training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "18b3e8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download most up-to-date tensor dictionaries\n",
    "    # TODO: wait until the tensors are uploaded to S3\n",
    "# pitch_level_tensors = aws.s3.download_file(\n",
    "#     aws.bucket_name, \n",
    "#     'epidemiology/ml/datasets/pytorch/pitch_level_tensors.pkl', \n",
    "#     'storage/pitch_level_tensors.pkl'\n",
    "# )\n",
    "\n",
    "# load tensors into memory\n",
    "    # outing_ --> one outcome per pitcher\n",
    "with open('storage/outing_level_tensors.pkl', 'rb') as f:\n",
    "    pitch_level_tensors = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693616b9",
   "metadata": {},
   "source": [
    "$\\textbf{Model Development}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25794636",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "from nnet import CNNbiLSTM\n",
    "import torch.nn.functional as F\n",
    "from nnet.loss_functions import pitch_level_loss\n",
    "from sklearn.metrics import roc_auc_score, f1_score\n",
    "from services.scaling import compute_masked_scalers, apply_scalers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "2e9496e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile model for training --> model, optimizer, loss function, and data setup\n",
    "def compile_model(\n",
    "        train_data: dict,\n",
    "        val_data: dict,\n",
    "        model_config: dict = {\n",
    "            'stem': 64,\n",
    "            'c': 96,\n",
    "            'kernel': 7,\n",
    "            'lstm_hidden': 128,\n",
    "            'dropout': 0.1,\n",
    "            'bidir': True\n",
    "        },\n",
    "        device: str = 'cpu',\n",
    "        use_pos_weight: bool = False\n",
    ") -> dict:\n",
    "    \"\"\" \n",
    "    Compile a nnet architecture for training. Includes scaling, model instantiation, and loss function setup.\n",
    "    \n",
    "    Args:\n",
    "        train_data (dict): Training data containing tensors and masks.\n",
    "        val_data (dict): Validation data containing tensors and masks.\n",
    "        model_config (dict): Configuration for the CNNbiLSTM model. Defaults to basic setup.\n",
    "        device (str): Device to run the model on ('cpu' or 'cuda').\n",
    "        use_pos_weight (bool): Whether to use positive class weights in loss function.\n",
    "    \n",
    "    Returns:\n",
    "        model_setup (dict): Dictionary with all relevant information..\n",
    "    \"\"\"\n",
    "\n",
    "    # standardize example sequence --> example training tensor sequence (x)\n",
    "    mean, std = compute_masked_scalers(train_data['seq'], train_data['mask'])\n",
    "    x = apply_scalers(train_data['seq'], mean, std)\n",
    "    B, T, K = x.shape       # NOTE: B = batch size, T = time steps, K = features; K is used for model setup\n",
    "\n",
    "    # setup (device, shapes)\n",
    "        # --> move full tensors to device\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    x_trn, y_binary_trn, mask_trn, lengths_trn = move_tensors_to_device(x, train_data, device)\n",
    "\n",
    "    # setup model, optimizer\n",
    "    model = CNNbiLSTM(\n",
    "        k_in=K, \n",
    "        stem=model_config.get('stem', 64),\n",
    "        c=model_config.get('c', 96),\n",
    "        kernel=model_config.get('kernel', 7),\n",
    "        lstm_hidden=model_config.get('lstm_hidden', 128),\n",
    "        dropout=model_config.get('dropout', 0.1),\n",
    "        bidir=model_config.get('bidir', True),\n",
    "    ).to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "\n",
    "    # optional: compute pos_weight over valid steps once\n",
    "        # NOTE: for probs this should be 1.0\n",
    "    if use_pos_weight:\n",
    "        with torch.no_grad():\n",
    "            pos = y_binary_trn.sum()\n",
    "            neg = y_binary_trn.shape[0] - pos\n",
    "            pos_weight = (neg / pos.clamp(min=1)).float()\n",
    "    else:\n",
    "        pos_weight = torch.tensor(1.0, device=device)\n",
    "\n",
    "    # package all into dictionary\n",
    "    model_setup = {\n",
    "        'model': model.train(),\n",
    "        'optimizer': optimizer,\n",
    "        'loss_fn': pitch_level_loss,\n",
    "        'pos_weight': pos_weight,\n",
    "        'mean': mean,\n",
    "        'std': std,\n",
    "        'device': device,\n",
    "        'train_setup': {\n",
    "            'x': x_trn,\n",
    "            'y_binary': y_binary_trn,\n",
    "            'mask': mask_trn,\n",
    "            'lengths': lengths_trn\n",
    "        },\n",
    "        'val_setup': {\n",
    "            'x': apply_scalers(val_data['seq'], mean, std).to(device),\n",
    "            'y_binary': val_data['binary'].float().to(device),\n",
    "            'mask': val_data['mask'].bool().to(device),\n",
    "            'lengths': val_data['lengths'].float().to(device)\n",
    "        }\n",
    "    }\n",
    "\n",
    "    return model_setup\n",
    "\n",
    "# move tensors to device\n",
    "    # mostly a helper function for training data \n",
    "def move_tensors_to_device(\n",
    "        x: torch.Tensor,\n",
    "        train_data: dict,\n",
    "        device: str\n",
    ") -> tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Move tensors to the specified device.\n",
    "    \n",
    "    Args:\n",
    "        x (torch.Tensor): Input tensor.\n",
    "        train_data (dict): Dictionary containing training data tensors.\n",
    "        device (str): Device to move tensors to ('cpu' or 'cuda').\n",
    "    \n",
    "    Returns:\n",
    "        tuple: Tensors moved to the specified device.\n",
    "    \"\"\"\n",
    "    return x.float().to(device), train_data['binary'].float().to(device), train_data['mask'].bool().to(device), train_data['lengths'].long().to(device)\n",
    "    \n",
    "# early stopping check\n",
    "def check_early_stopping(\n",
    "        val_loss: float,\n",
    "        best_val_loss: float,\n",
    "        epochs_no_improve: int,\n",
    "        patience: int\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Check if early stopping criteria are met.\n",
    "    \n",
    "    Args:\n",
    "        val_loss (float): Current validation loss.\n",
    "        best_val_loss (float): Best validation loss observed so far.\n",
    "        epochs_no_improve (int): Number of epochs since last improvement.\n",
    "        patience (int): Patience for early stopping.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary indicating whether to stop, if there's a new best, updated best loss, and epochs without improvement.\n",
    "    \"\"\"\n",
    "    if val_loss < best_val_loss:\n",
    "        return {\n",
    "            'should_stop': False,\n",
    "            'new_best': True,\n",
    "            'best_val_loss': val_loss,\n",
    "            'epochs_no_improve': 0\n",
    "        }\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        if epochs_no_improve >= patience:\n",
    "            return {\n",
    "                'should_stop': True,\n",
    "                'new_best': False,\n",
    "                'best_val_loss': best_val_loss,\n",
    "                'epochs_no_improve': epochs_no_improve\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                'should_stop': False,\n",
    "                'new_best': False,\n",
    "                'best_val_loss': best_val_loss,\n",
    "                'epochs_no_improve': epochs_no_improve\n",
    "            }\n",
    "\n",
    "# ROC/AUC calcs\n",
    "def compute_roc_auc( \n",
    "        probs: torch.Tensor,\n",
    "        y_true: torch.Tensor,\n",
    "        mask: torch.Tensor\n",
    ") -> tuple[float, float, float]:\n",
    "    \"\"\"\n",
    "    Compute ROC AUC score.\n",
    "    \n",
    "    Args:\n",
    "        probs (torch.Tensor): Predicted probabilities.\n",
    "        y_true (torch.Tensor): True labels.\n",
    "        mask (torch.Tensor): Mask indicating valid entries.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (batch_scores (torch.Tensor), batch_labels (torch.Tensor), roc_auc (float))\n",
    "    \"\"\"\n",
    "    # per-batch valid scores/labels for AUC (use binary labels)\n",
    "    batch_scores = probs.detach().float()\n",
    "    batch_labels = y_true[mask].detach().float()\n",
    "\n",
    "    # compute AUC\n",
    "    if batch_labels.sum() == 0 or batch_labels.sum() == len(batch_labels):\n",
    "        roc_auc = 0.5 \n",
    "    else:\n",
    "        roc_auc = roc_auc_score(batch_labels.cpu().numpy(), batch_scores.cpu().numpy())\n",
    "\n",
    "    return batch_scores, batch_labels, roc_auc\n",
    "\n",
    "# training loop for a model\n",
    "    # NOTE: sets up a dictionary for storing model loss history\n",
    "def train_model(\n",
    "        model_name: str,\n",
    "        model_setup: dict,\n",
    "        n_epochs: int = 100,\n",
    "        batch_size: int = 32,\n",
    "        patience: int = 10,\n",
    "        verbose: bool = True,\n",
    "        epoch_start: int = 1\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Train the model using the provided training and validation data. Manually handles batching and early stopping.\n",
    "    \n",
    "    Args:\n",
    "        model_setup (dict): Dictionary containing model, optimizer, loss function, and data setup.\n",
    "        n_epochs (int): Number of epochs to train. Defaults to 100.\n",
    "        batch_size (int): Batch size for training. Defaults to 32.\n",
    "        patience (int): Patience for early stopping. Defaults to 10.\n",
    "        verbose (bool): Whether to print training progress. Defaults to True.\n",
    "        epoch_start (int): Starting epoch number (useful for resuming training). Defaults to 1.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary containing training and validation loss history.\n",
    "    \"\"\"\n",
    "    # unpack model setup\n",
    "    model = model_setup['model']\n",
    "    optimizer = model_setup['optimizer']\n",
    "    pos_weight = model_setup['pos_weight']\n",
    "\n",
    "    # extract training data\n",
    "    x = model_setup['train_setup']['x']\n",
    "    y_step_binary = model_setup['train_setup']['y_binary']\n",
    "    mask = model_setup['train_setup']['mask']\n",
    "    lengths = model_setup['train_setup']['lengths']\n",
    "\n",
    "    # setup loss storage\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'val_loss': [],\n",
    "        'best_val_preds': None,\n",
    "        'best_val_loss': None,\n",
    "        'stopped_epoch': None\n",
    "    }\n",
    "\n",
    "    # loss counters\n",
    "    running_loss = 0.0\n",
    "    running_correct = 0\n",
    "    running_total = 0\n",
    "\n",
    "    # early stopping setup\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "\n",
    "    # MAIN LOOP: iterate through epochs\n",
    "    B = x.shape[0]\n",
    "    for epoch in range(0 + epoch_start, epoch_start + n_epochs + 1):\n",
    "        \n",
    "        \"\"\" TRAINING \"\"\"\n",
    "        # set model to train mode, shuffle indices, create batches\n",
    "        model.train()\n",
    "        idx = torch.randperm(B, device=model_setup['device'])  # shuffle indices each epoch\n",
    "\n",
    "        # loss counters\n",
    "        running_loss = 0.0\n",
    "\n",
    "        # wrap range() with tqdm\n",
    "        pbar = tqdm(range(0, B, batch_size), desc=f\"Epoch {epoch}/{epoch_start + n_epochs}\", leave=False)\n",
    "        for start in pbar:\n",
    "            end = min(start + batch_size, B)\n",
    "            bidx = idx[start:end]\n",
    "\n",
    "            # forward pass setup\n",
    "            xb = x[bidx]                        # [B,T,K]\n",
    "            yb = y_step_binary[bidx]            # [B] sequence-level labels (0/1)\n",
    "            mb = mask[bidx]                     # [B,T]\n",
    "            Lb = lengths[bidx]                  # [B]\n",
    "\n",
    "            # forward pass: get per-pitch + sequence logits\n",
    "            _, logit_seq, _ = model(xb, Lb, mb)\n",
    "\n",
    "            # main sequence-level loss\n",
    "            loss = F.binary_cross_entropy_with_logits(\n",
    "                logit_seq, yb.float(), pos_weight=pos_weight\n",
    "            )\n",
    "\n",
    "            # back propagation\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            # running loss\n",
    "            bs = xb.size(0)\n",
    "            running_loss += loss.item() * bs\n",
    "            \n",
    "        # accuracy (sequence-level)\n",
    "            probs  = torch.sigmoid(logit_seq)\n",
    "            preds  = (probs > 0.5).float()\n",
    "            correct = (preds == yb).sum().item()\n",
    "            total   = yb.size(0)\n",
    "\n",
    "            # update total\n",
    "            running_correct += correct\n",
    "            running_total += total\n",
    "\n",
    "            # update accuracy\n",
    "            run_avg_loss = running_loss / ((start // batch_size + 1) * bs)\n",
    "            run_acc      = running_correct / running_total\n",
    "\n",
    "            # show both current and running losses in the bar\n",
    "            pbar.set_postfix(loss=f\"{run_avg_loss:.4f}\", acc=f\"{run_acc:.4f}\")\n",
    "\n",
    "        # update epoch loss\n",
    "        epoch_loss = running_loss / B\n",
    "        history['train_loss'].append(epoch_loss)\n",
    "        \n",
    "        # print epoch update\n",
    "        if verbose:\n",
    "            print(f\"Epoch {epoch:02d} | Training Loss: {epoch_loss:.3f}\")\n",
    "\n",
    "        \"\"\" VALIDATION \"\"\"\n",
    "        # set model to eval mode\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            _, logits, _ = model(\n",
    "                model_setup['val_setup']['x'].float(), \n",
    "                model_setup['val_setup']['lengths'].float(),\n",
    "                model_setup['val_setup']['mask'].bool()\n",
    "            )\n",
    "            val_loss = F.binary_cross_entropy_with_logits(\n",
    "                logits, \n",
    "                model_setup['val_setup']['y_binary'], \n",
    "                pos_weight=model_setup['pos_weight']\n",
    "            ).item()\n",
    "            history['val_loss'].append(val_loss)\n",
    "            \n",
    "            # print update\n",
    "            if verbose:\n",
    "                print(f\"Epoch {epoch:02d} | Validation Loss: {val_loss:.3f}\")\n",
    "\n",
    "            \"\"\" EARLY STOPPING CHECK \"\"\"\n",
    "            early_stop_info = check_early_stopping(\n",
    "                val_loss, best_val_loss, epochs_no_improve, patience\n",
    "            )\n",
    "            \n",
    "            # update values with results\n",
    "            best_val_loss = early_stop_info['best_val_loss']\n",
    "            epochs_no_improve = early_stop_info['epochs_no_improve']\n",
    "\n",
    "            # option 1: new best --> save intermediate model\n",
    "            if early_stop_info['new_best']:\n",
    "\n",
    "                # update history\n",
    "                history['best_val_loss'] = best_val_loss\n",
    "                history['best_val_preds'] = torch.sigmoid(logits).detach().cpu().numpy()\n",
    "                \n",
    "                # save model (+ state dict)\n",
    "                    # ensure model is in eval mode before saving\n",
    "                model.eval()\n",
    "                torch.save(model, f\"models/inj/{model_name}.pt\")\n",
    "                torch.save(model.state_dict(), f\"models/inj/{model_name}_state_dict.pt\")\n",
    "\n",
    "                # print update\n",
    "                if verbose:\n",
    "                    print(f\"New best model saved with validation loss: {best_val_loss:.3f}\")\n",
    "\n",
    "            # option 2: not best, but not yet time to stop --> continue\n",
    "            if not early_stop_info['should_stop']:\n",
    "                continue\n",
    "\n",
    "            # option 3: stop training early\n",
    "            if early_stop_info['should_stop']:\n",
    "                if verbose:\n",
    "                    print(f\"Early stopping triggered after {epoch} epochs. Best validation loss: {best_val_loss:.3f}\")\n",
    "                \n",
    "                # add to history\n",
    "                history['stopped_epoch'] = epoch\n",
    "\n",
    "                # NOTE: don't need to save model here, since we already saved the best one above\n",
    "                \n",
    "                return history\n",
    "\n",
    "    # NOTE: if we get here, training completed without early stopping\n",
    "    if verbose:\n",
    "        print(f\"Training completed after {epoch} epochs with best validation loss: {best_val_loss:.3f}\")\n",
    "\n",
    "    # save final model\n",
    "        # ensure model is in eval mode before saving\n",
    "    model.eval()\n",
    "    torch.save(model, f\"models/inj/{model_name}.pt\")\n",
    "    torch.save(model.state_dict(), f\"models/inj/{model_name}_state_dict.pt\")\n",
    "\n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2dd2b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model setups for outing- and pitch-level data by day windows\n",
    "model_setups = {}\n",
    "model_results = {}\n",
    "\n",
    "# compile models for each day window\n",
    "model_setup = compile_model(\n",
    "    train_data=pitch_level_tensors['trn'],\n",
    "    val_data=pitch_level_tensors['val'],\n",
    "    use_pos_weight=True,\n",
    "    model_config={\n",
    "        'stem': 64,\n",
    "        'c': 96,\n",
    "        'kernel': 7,\n",
    "        'lstm_hidden': 64,\n",
    "        'dropout': 0.1,\n",
    "        'bidir': True\n",
    "    },\n",
    ")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training pitch-level model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00 | Training Loss: 1.070\n",
      "Epoch 00 | Validation Loss: 1.019\n",
      "New best model saved with validation loss: 1.019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | Training Loss: 1.070\n",
      "Epoch 01 | Validation Loss: 1.017\n",
      "New best model saved with validation loss: 1.017\n",
      "Training completed after 1 epochs with best validation loss: 1.017\n"
     ]
    }
   ],
   "source": [
    "\"\"\" TRAIN MODEL \"\"\"\n",
    "print(f'Training pitch-level model...')\n",
    "\n",
    "# pitch-level model loop\n",
    "    # 60 epochs run so far (as of 8/25 am)\n",
    "    # save in pitch_model_results\n",
    "ext_results = train_model(\n",
    "    model_name=f'pitch_model_test',     # NOTE: test\n",
    "    model_setup=model_setup,\n",
    "    n_epochs=1,\n",
    "    batch_size=32,\n",
    "    patience=10,\n",
    "    verbose=True, \n",
    "    epoch_start=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c0bd9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to disk\n",
    "# with open(f'storage/pitch_model_results.pkl', 'wb') as f:\n",
    "#     pickle.dump(ext_results, f)\n",
    "\n",
    "# TODO: upload to S3\n",
    "# with open(f'storage/pitch_model_results.pkl', 'rb') as f:\n",
    "#     content = f.read()\n",
    "#     aws.upload_to_s3(content, f'epidemiology/ml/models/inj/pitch_model_results.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c2a183",
   "metadata": {},
   "source": [
    "$\\textbf{Save Preprocessed Train/Val Data}$\n",
    "\n",
    "Mostly to save the scaled data, since saving scalers isn't finalized yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "4919c7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get model setup\n",
    "day_setup_train = model_setup['train_setup']\n",
    "day_setup_val = model_setup['val_setup']\n",
    "\n",
    "# save to disk\n",
    "with open(f'models/data/pitch_model_trn_data.pkl', 'wb') as f:\n",
    "    pickle.dump(day_setup_train, f)\n",
    "with open(f'models/data/pitch_model_val_data.pkl', 'wb') as f:\n",
    "    pickle.dump(day_setup_val, f)\n",
    "\n",
    "# TODO: upload to S3\n",
    "# with open(f'models/data/pitch_model_{day}d_trn_data.pkl', 'rb') as f:\n",
    "#     content = f.read()\n",
    "#     aws.upload_to_s3(content, f'epidemiology/ml/models/inj/pitch_model_{day}d_trn_data.pkl')\n",
    "# with open(f'models/data/pitch_model_{day}d_val_data.pkl', 'rb') as f:\n",
    "#     content = f.read()\n",
    "#     aws.upload_to_s3(content, f'epidemiology/ml/models/inj/pitch_model_{day}d_val_data.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pitch_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
