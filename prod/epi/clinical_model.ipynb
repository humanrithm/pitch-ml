{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b019010f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from connections import AWS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6adbe8e8",
   "metadata": {},
   "source": [
    "$\\textbf{Epidemiology: Clinical Model}$\n",
    "\n",
    "Injury risk estimation from two perspectives: \n",
    "- __Season__ (i.e., post-outing injury probability; postgame)\n",
    "- __Pitch-Level__ (i.e., next-pitch injury probability; within game)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3138f38a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AWS]: Port 5433 is free.\n",
      "[AWS]: Connected to RDS endpoint.\n"
     ]
    }
   ],
   "source": [
    "\"\"\" INITIALIZE AWS CONNECTION \"\"\"\n",
    "aws = AWS()\n",
    "aws.connect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112dc610",
   "metadata": {},
   "source": [
    "$\\textbf{Data Loading}$\n",
    "\n",
    "- Cohort data (matches, preds, statcast)\n",
    "- Ball flight aggregates "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f33b6318",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function for calculating averages\n",
    "def get_avgs(\n",
    "        data: pd.DataFrame,\n",
    "        group_cols: list,\n",
    "        avg_cols: list = ['rel_speed', 'rel_side', 'rel_ht', 'spin_rate']\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate averages for specified columns grouped by the given columns.\n",
    "    \n",
    "    Args:\n",
    "        data (pd.DataFrame): The input DataFrame containing the data.\n",
    "        group_cols (list): List of columns to group by.\n",
    "        avg_cols (list): List of columns to calculate averages for. Defaults to ball flight features used in injury risk model.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame with the grouped columns and their corresponding averages.\n",
    "    \"\"\"\n",
    "    return data.groupby(group_cols)[avg_cols].mean().reset_index()\n",
    "\n",
    "# filter pitches to date range for a given ID\n",
    "def filter_pitches_by_date(\n",
    "        pitch_data: pd.DataFrame, \n",
    "        pitcher_id: int, \n",
    "        start_date: str, \n",
    "        end_date: str\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Filter pitch data for a specific pitcher within a date range.\n",
    "    \n",
    "    Args:\n",
    "        pitch_data (pd.DataFrame): The DataFrame containing pitch data.\n",
    "        pitcher_id (int): The ID of the pitcher to filter by.\n",
    "        start_date (str): The start date in 'YYYY-MM-DD' format.\n",
    "        end_date (str): The end date in 'YYYY-MM-DD' format.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Filtered DataFrame containing pitches for the specified pitcher and date range.\n",
    "    \"\"\"\n",
    "    return pitch_data[\n",
    "        (pitch_data['pitcher'] == pitcher_id) &\n",
    "        (pitch_data['game_date'] >= start_date) &\n",
    "        (pitch_data['game_date'] <= end_date)\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d01d272f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load cohort of matches, model predictions\n",
    "cohort_matches = aws.load_s3_object('epidemiology/ml/datasets/full/cohort_matches_final.csv')\n",
    "cohort_preds = aws.load_s3_object('epidemiology/ml/datasets/preds/model_cohort.csv')\n",
    "\n",
    "# load all statcast data\n",
    "    # likely fts: velo, release position, spin rate (no pitch labels)\n",
    "statcast_data = aws.load_s3_object('epidemiology/ml/datasets/full/model_application_data.csv')\n",
    "cohort_preds_statcast = cohort_preds.merge(statcast_data, on=['pitch_id', 'pitcher', 'game_date', 'pitcher_days_since_prev_game', 'injured_cohort_pitcher'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c923efc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: update dataset to match windows btw each injured & noninjured pitcher\n",
    "cohort_preds_final = []\n",
    "for id in cohort_matches['mlbamid_injured'].unique(): \n",
    "    # get date of first and last pitch for each pitcher\n",
    "    first_pitch = cohort_preds_statcast[cohort_preds_statcast['pitcher'] == id]['game_date'].min()\n",
    "    last_pitch = cohort_preds_statcast[cohort_preds_statcast['pitcher'] == id]['game_date'].max()\n",
    "\n",
    "    # get all pitches for this pitcher, append to final dataset\n",
    "    pitcher_data = filter_pitches_by_date(\n",
    "        cohort_preds_statcast, \n",
    "        id, \n",
    "        first_pitch, \n",
    "        last_pitch\n",
    "    ).reset_index(drop=True)\n",
    "    cohort_preds_final.append(pitcher_data)\n",
    "\n",
    "    # get pitches for match\n",
    "    matched_id = cohort_matches[cohort_matches['mlbamid_injured'] == id]['mlbamid_noninjured'].values[0]\n",
    "    matched_data = filter_pitches_by_date(\n",
    "        cohort_preds_statcast, \n",
    "        matched_id, \n",
    "        first_pitch, \n",
    "        last_pitch\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "    # append to final dataset\n",
    "    cohort_preds_final.append(matched_data)\n",
    "\n",
    "# concatenate all data\n",
    "cohort_preds_final = pd.concat(cohort_preds_final, ignore_index=True)\n",
    "injured_proportion = cohort_preds_final['injured_cohort_pitcher'].mean()        # check proportion of injured pitches --> should be close-ish to 50%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65134869",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get baseline season-long ball flight averages for reference\n",
    "    # NOTE: these may be referenced for the pitch level model\n",
    "ball_flight_fts = ['rel_speed', 'rel_side', 'rel_ht', 'spin_rate']\n",
    "ball_flight_season_avgs = get_avgs(\n",
    "    cohort_preds_final,\n",
    "    group_cols=['pitcher', 'season'],\n",
    "    avg_cols=ball_flight_fts\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b863452c",
   "metadata": {},
   "source": [
    "$\\textbf{Setup Data Sequences}$\n",
    "\n",
    "- __Outing-Level Model__: Up to last outing prior to injury (last outing not included)\n",
    "- __Pitch-Level Model__: Up to final pitch prior to injury\n",
    "\n",
    "Both will be stored as days prior to last outing/pitch (7-, 15-, 30-, 45-, and 90-days prior). All sequences must be converted to arrays (then likely tensors) for model development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe381f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# days until second-to-last outing (outing-level model)\n",
    "def days_until_2nd_last_outing(group):\n",
    "    \"\"\"\n",
    "    Calculate the number of days until the second-to-last outing for each pitcher in a group.\n",
    "\n",
    "    Args:\n",
    "        group (pd.DataFrame): A DataFrame containing outings for a specific pitcher and season.\n",
    "\n",
    "    Returns:\n",
    "        pd.Series: A Series with the number of days until the second-to-last outing for each row in the group.\n",
    "    \"\"\"\n",
    "    # get unique outing dates for the pitcher\n",
    "    unique_dates = sorted(pd.to_datetime(group['game_date']).unique())\n",
    "    \n",
    "    # check if there are at least two unique outing dates\n",
    "    if len(unique_dates) < 2:\n",
    "        # if not enough outings (eg., only 1), fill with NaN\n",
    "        return pd.Series([pd.NA] * len(group), index=group.index)\n",
    "    \n",
    "    # get the second-to-last unique date\n",
    "    second_last = unique_dates[-2]\n",
    "    \n",
    "    return (second_last - pd.to_datetime(group['game_date'])).dt.days\n",
    "\n",
    "# days until last outing (pitch-level model)\n",
    "def days_until_last_outing(group):\n",
    "    \"\"\"\n",
    "    Calculate the number of days until the last outing for each pitcher in a group.\n",
    "\n",
    "    Args:\n",
    "        group (pd.DataFrame): A DataFrame containing outings for a specific pitcher and season.\n",
    "\n",
    "    Returns:\n",
    "        pd.Series: A Series with the number of days until the last outing for each row in the group.\n",
    "    \"\"\"\n",
    "    last_date = pd.to_datetime(group['game_date']).max()\n",
    "    return (last_date - pd.to_datetime(group['game_date'])).dt.days\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "b96163b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set model features -- contextual layer\n",
    "CONTEXTUAL_FTS = [\n",
    "    'p_throws',\n",
    "    'pitcher_days_since_prev_game',\n",
    "    'outing_number'\n",
    "]\n",
    "\n",
    "# set model features -- time series layer\n",
    "TIME_SERIES_FTS = [\n",
    "    # predicted load\n",
    "    'pred_peak_evt_normalized',\n",
    "    'within_outing_cumulative_evt_workload',\n",
    "    \n",
    "    # ball flight\n",
    "    'rel_speed', \n",
    "    'rel_side',\n",
    "    'rel_ht', \n",
    "    'spin_rate'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "c325759e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup pitch-level dataset\n",
    "    # pitch_uuid: unique identifier for model dataset\n",
    "pitch_dataset = cohort_preds_final.copy()\n",
    "pitch_dataset.insert(0, 'pitch_uuid', pitch_dataset.index)\n",
    "\n",
    "# compute days until last outing for each pitcher\n",
    "pitch_dataset['days_until_last_outing'] = pitch_dataset.groupby(['pitcher', 'season']).apply(days_until_last_outing).reset_index(level=[0,1], drop=True)\n",
    "\n",
    "# store sequences by days until last outing prior to injury\n",
    "pitch_level_sequences = {\n",
    "    7: [],\n",
    "    15: [],\n",
    "    30: [],\n",
    "    45: [],\n",
    "    90: []\n",
    "}\n",
    "for day in pitch_level_sequences.keys():\n",
    "    pitch_level_sequences[day] = [\n",
    "        group[TIME_SERIES_FTS + CONTEXTUAL_FTS].values\n",
    "            for _, group in pitch_dataset[pitch_dataset['days_until_last_outing'] <= day].groupby(['pitcher', 'season'])\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcafc67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup outing-level dataset\n",
    "    # originally: 27,462 distinct outings --> these will be reduced\n",
    "outing_dataset = cohort_preds_final.copy()\n",
    "outing_dataset.insert(0, 'outing_id', outing_dataset.groupby(['pitcher', 'game_date']).ngroup())\n",
    "\n",
    "# compute days until second-to-last outing for each pitcher\n",
    "    # NOTE: second-to-last because we're trying to predict \"next\" outing injury\n",
    "    # last outings will have negative days --> drop\n",
    "outing_dataset['days_until_2nd_last_outing'] = outing_dataset.groupby(\n",
    "    ['pitcher', 'season']\n",
    ").apply(days_until_2nd_last_outing).reset_index(level=[0,1], drop=True)\n",
    "outing_dataset_clean = outing_dataset[outing_dataset['days_until_2nd_last_outing'] >= 0].reset_index(drop=True)\n",
    "\n",
    "# store sequence lengths by days until second-to-last outing (ie., prior to injured outing)\n",
    "outing_level_sequences = {\n",
    "    7: [],\n",
    "    15: [],\n",
    "    30: [],\n",
    "    45: [],\n",
    "    90: []\n",
    "}\n",
    "for day in outing_level_sequences.keys():\n",
    "    outing_level_sequences[day] = [\n",
    "    group[TIME_SERIES_FTS + CONTEXTUAL_FTS].values\n",
    "        for _, group in outing_dataset_clean[outing_dataset_clean['days_until_2nd_last_outing'] <= day].groupby(['pitcher', 'season'])\n",
    "    ]  \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada5d9df",
   "metadata": {},
   "source": [
    "$\\textbf{Create Outcome Probability Grid}$\n",
    "\n",
    "Binary outcome (__1 := injured, 0 := non-injured__) is also converted to a probability for injured pitchers to encourage model learning:\n",
    "- For _outing sequences_, applies a linear increase with each outing until 1 is reached\n",
    "- For _pitch-level sequences_, a sigmoid is created over all pitches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a6f2561c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "7ccf8156",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(length: int) -> np.ndarray:\n",
    "    \"\"\" Generate a sigmoid curve for a given length.\"\"\"\n",
    "    return 1 / (1 + np.exp(-np.linspace(-6, 6, length)))\n",
    "\n",
    "def update_outcome_probabilities(\n",
    "        data: pd.DataFrame, \n",
    "        model_type: str,\n",
    "        outcome_col: str = 'injured_cohort_pitcher',\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Update the outcome probabilities for each pitcher based on their outing history.\n",
    "    \n",
    "    Args:\n",
    "        data (pd.DataFrame): The DataFrame containing the data.\n",
    "        model_type (str): The type of model being used ('pitch_level' or 'outing_level').\n",
    "        outcome_col (str): The column containing the binary outcome. Defaults to 'injured_cohort_pitcher'.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: The DataFrame with updated probabilities.\n",
    "    \"\"\"\n",
    "    # update group columns based on model type\n",
    "    match model_type:\n",
    "        case 'pitch_level':\n",
    "            pass\n",
    "        case 'outing_level':\n",
    "            group_cols = ['pitcher', 'outing_id', 'season']\n",
    "\n",
    "    # trim to outcomes\n",
    "        # applies a linear increasing probability with each outing for injured pitchers\n",
    "    outcomes_orig = data[group_cols + [outcome_col]].drop_duplicates().reset_index(drop=True).copy()\n",
    "    outcome_probs = outcomes_orig[outcome_col] / (outcomes_orig.sort_values(by=group_cols, ascending=False).groupby(['pitcher', 'season']).cumcount() + 1)\n",
    "\n",
    "    # clean outcomes to binary\n",
    "    outcomes_binary = np.where((outcome_probs < 1) & (outcome_probs > 0), 0, outcome_probs)\n",
    "\n",
    "    return outcome_probs, outcomes_binary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "7685d125",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup outcome dictionaries\n",
    "pitch_level_outcomes = {\n",
    "    7: {\n",
    "        'probs': [],\n",
    "        'binary': []\n",
    "    },\n",
    "    15: {\n",
    "        'probs': [],\n",
    "        'binary': []\n",
    "    },\n",
    "    30: {\n",
    "        'probs': [],\n",
    "        'binary': []\n",
    "    },\n",
    "    45: {\n",
    "        'probs': [],\n",
    "        'binary': []\n",
    "    },\n",
    "    90: {\n",
    "        'probs': [],\n",
    "        'binary': []\n",
    "    }\n",
    "}\n",
    "outing_level_outcomes = {\n",
    "    7: {\n",
    "        'probs': [],\n",
    "        'binary': []\n",
    "    },\n",
    "    15: {\n",
    "        'probs': [],\n",
    "        'binary': []\n",
    "    },\n",
    "    30: {\n",
    "        'probs': [],\n",
    "        'binary': []\n",
    "    },\n",
    "    45: {\n",
    "        'probs': [],\n",
    "        'binary': []\n",
    "    },\n",
    "    90: {\n",
    "        'probs': [],\n",
    "        'binary': []\n",
    "    }\n",
    "}\n",
    "\n",
    "# iterate through outing-level and update (linear probs & binary)\n",
    "for day in outing_level_outcomes.keys():\n",
    "    day_outings = outing_dataset_clean[outing_dataset_clean['days_until_2nd_last_outing'] <= day].reset_index(drop=True)\n",
    "    \n",
    "    # iterate through pitcher-season combos in the data\n",
    "    for group, rows in day_outings.groupby(['pitcher', 'season']):\n",
    "        day_probs, day_binary = update_outcome_probabilities(\n",
    "            rows,\n",
    "            model_type='outing_level'\n",
    "        )\n",
    "        outing_level_outcomes[day]['probs'].append(list(day_probs))\n",
    "        outing_level_outcomes[day]['binary'].append(list(day_binary))\n",
    "\n",
    " # iterate through pitch-level and update (sigmoid probs & binary)\n",
    "for day in pitch_level_outcomes.keys():\n",
    "    day_pitches = pitch_dataset[pitch_dataset['days_until_last_outing'] <= day].reset_index(drop=True)\n",
    "    \n",
    "    # iterate through pitcher-season combos in the df\n",
    "    for group, rows in day_pitches.groupby(['pitcher', 'season']):\n",
    "        # get sigmoid probs\n",
    "        sigmoid_probs = sigmoid(rows.shape[0])\n",
    "        pitch_level_outcomes[day]['probs'].append(list(sigmoid_probs))\n",
    "\n",
    "        # convert to binary outcomes\n",
    "        pitch_level_outcomes[day]['binary'].append(list(np.where(sigmoid_probs == sigmoid_probs.max(), 1, 0)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb76b05",
   "metadata": {},
   "source": [
    "$\\textbf{Tensor Setup}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "05a91689",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5624ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to tensors and pad to the same length\n",
    "def create_padded_tensor(sequences: list) -> torch.Tensor:\n",
    "    \"\"\" \n",
    "    Convert a list of sequences to a padded tensor.\n",
    "    \n",
    "    Args:\n",
    "        sequences (list): A list of sequences (arrays) to be converted.\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: A padded tensor of shape (batch, max_seq_len, features).\n",
    "    \"\"\"\n",
    "    seq_tensors = [torch.tensor(seq) for seq in sequences]\n",
    "    padded = pad_sequence(seq_tensors, batch_first=True, padding_value=0)        # shape: (batch, max_seq_len, features)\n",
    "\n",
    "    return padded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "33b94243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup tensor dictionaries\n",
    "pitch_level_tensors = {\n",
    "    7: {\n",
    "        'seq': None,\n",
    "        'mask': None,\n",
    "        'lengths': None\n",
    "    },\n",
    "    15: {\n",
    "        'seq': None,\n",
    "        'mask': None,\n",
    "        'lengths': None\n",
    "    },\n",
    "    30: {\n",
    "        'seq': None,\n",
    "        'mask': None,\n",
    "        'lengths': None\n",
    "    },\n",
    "    45: {\n",
    "        'seq': None,\n",
    "        'mask': None,\n",
    "        'lengths': None\n",
    "    },\n",
    "    90: {\n",
    "        'seq': None,\n",
    "        'mask': None,\n",
    "        'lengths': None\n",
    "    }\n",
    "}\n",
    "outing_level_tensors = {\n",
    "    7: {\n",
    "        'seq': None,\n",
    "        'mask': None,\n",
    "        'lengths': None\n",
    "    },\n",
    "    15: {\n",
    "        'seq': None,\n",
    "        'mask': None,\n",
    "        'lengths': None\n",
    "    },\n",
    "    30: {\n",
    "        'seq': None,\n",
    "        'mask': None,\n",
    "        'lengths': None\n",
    "    },\n",
    "    45: {\n",
    "        'seq': None,\n",
    "        'mask': None,\n",
    "        'lengths': None\n",
    "    },\n",
    "    90: {\n",
    "        'seq': None,\n",
    "        'mask': None,\n",
    "        'lengths': None\n",
    "    }\n",
    "}\n",
    "\n",
    "# iterate through outing-level and create tensors\n",
    "    # shape: (batch, max_seq_len, features)\n",
    "    # also add mask and lengths to denote actual values for training\n",
    "for day in outing_level_tensors.keys():\n",
    "    outing_level_tensors[day]['seq'] = create_padded_tensor(outing_level_sequences[day]) \n",
    "    outing_level_tensors[day]['mask'] = (outing_level_tensors[day]['seq'].abs().sum(dim=2) != 0) \n",
    "    outing_level_tensors[day]['lengths'] = outing_level_tensors[day]['mask'].sum(dim=1)\n",
    "\n",
    "    # convert outcomes to padded tensors\n",
    "    outing_level_outcomes[day]['probs'] = create_padded_tensor(outing_level_outcomes[day]['probs'])\n",
    "    outing_level_outcomes[day]['binary'] = create_padded_tensor(outing_level_outcomes[day]['binary'])\n",
    "\n",
    "# iterate through pitch-level and create tensors\n",
    "    # shape: (batch, max_seq_len, features)\n",
    "    # also add mask and lengths to denote actual values for training\n",
    "for day in pitch_level_tensors.keys():\n",
    "    pitch_level_tensors[day]['seq'] = create_padded_tensor(pitch_level_sequences[day])\n",
    "    pitch_level_tensors[day]['mask'] = (pitch_level_tensors[day]['seq'].abs().sum(dim=2) != 0) \n",
    "    pitch_level_tensors[day]['lengths'] = pitch_level_tensors[day]['mask'].sum(dim=1)\n",
    "\n",
    "    # convert outcomes to padded tensors\n",
    "    pitch_level_outcomes[day]['probs'] = create_padded_tensor(pitch_level_outcomes[day]['probs'])\n",
    "    pitch_level_outcomes[day]['binary'] = create_padded_tensor(pitch_level_outcomes[day]['binary'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "56d872a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup tensor datasets\n",
    "    # also store relevant model information: loss, num. of sequences, etc\n",
    "pitch_level_datasets = {\n",
    "    7: {\n",
    "        'probs': None,\n",
    "        'binary': None,\n",
    "        'loss_function': None,\n",
    "        'num_sequences': None,\n",
    "        'max_seq_len': None\n",
    "    },\n",
    "    15: {\n",
    "        'probs': None,\n",
    "        'binary': None,\n",
    "        'loss_function': None,\n",
    "        'num_sequences': None\n",
    "    },\n",
    "    30: {\n",
    "        'probs': None,\n",
    "        'binary': None,\n",
    "        'loss_function': None,\n",
    "        'num_sequences': None\n",
    "    },\n",
    "    45: {\n",
    "        'probs': None,\n",
    "        'binary': None,\n",
    "        'loss_function': None,\n",
    "        'num_sequences': None\n",
    "    },\n",
    "    90: {\n",
    "        'probs': None,\n",
    "        'binary': None,\n",
    "        'loss_function': None,\n",
    "        'num_sequences': None\n",
    "    }\n",
    "}\n",
    "outing_level_datasets = {\n",
    "    7: {\n",
    "        'probs': None,\n",
    "        'binary': None,\n",
    "        'loss_function': None,\n",
    "        'num_sequences': None\n",
    "    },\n",
    "    15: {\n",
    "        'probs': None,\n",
    "        'binary': None,\n",
    "        'loss_function': None,\n",
    "        'num_sequences': None\n",
    "    },\n",
    "    30: {\n",
    "        'probs': None,\n",
    "        'binary': None,\n",
    "        'loss_function': None,\n",
    "        'num_sequences': None\n",
    "    },\n",
    "    45: {\n",
    "        'probs': None,\n",
    "        'binary': None,\n",
    "        'loss_function': None,\n",
    "        'num_sequences': None\n",
    "    },\n",
    "    90: {\n",
    "        'probs': None,\n",
    "        'binary': None,\n",
    "        'loss_function': None,\n",
    "        'num_sequences': None\n",
    "    }\n",
    "}\n",
    "\n",
    "# update datasets (binary & probs)\n",
    "for day in [7, 15, 30, 45, 90]:\n",
    "    for outcome_type in ['binary', 'probs']:\n",
    "        pitch_level_datasets[day][outcome_type] = TensorDataset(\n",
    "            pitch_level_tensors[day]['seq'], \n",
    "            pitch_level_outcomes[day][outcome_type],\n",
    "            pitch_level_tensors[day]['mask'],\n",
    "            pitch_level_tensors[day]['lengths']\n",
    "        )\n",
    "        outing_level_datasets[day][outcome_type] = TensorDataset(\n",
    "            outing_level_tensors[day]['seq'], \n",
    "            outing_level_outcomes[day][outcome_type],\n",
    "            outing_level_tensors[day]['mask'],\n",
    "            outing_level_tensors[day]['lengths']\n",
    "        )\n",
    "\n",
    "        # update metadata\n",
    "        pitch_level_datasets[day]['num_sequences'] = pitch_level_tensors[day]['seq'].shape[0]\n",
    "        outing_level_datasets[day]['num_sequences'] = outing_level_tensors[day]['seq'].shape[0]\n",
    "        pitch_level_datasets[day]['max_seq_len'] = pitch_level_tensors[day]['seq'].shape[1]\n",
    "        outing_level_datasets[day]['max_seq_len'] = outing_level_tensors[day]['seq'].shape[1]\n",
    "\n",
    "        # TODO: set loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd468bf",
   "metadata": {},
   "source": [
    "$\\textbf{Model Architecture}$\n",
    "\n",
    "Both models use a CNN-(bi)LSTM architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "4767d08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba80ff31",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" MODEL ARCHITECTURES \"\"\"\n",
    "# model architecture: pitch-level estimates\n",
    "class PitchLevelModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Pitch-level model for estimating injury risk. Uses CNN layers for local feature extraction and a bi-directional LSTM for long-term patterns.\n",
    "\n",
    "    **Args (Instantiation)**:\n",
    "        num_fts (int): Number of input features.\n",
    "        cnn_channels (int): Number of convolutional channels. Default is 64.\n",
    "        lstm_hidden (int): Number of hidden units in the LSTM. Default is 96.\n",
    "        kernel (int): Kernel size for the convolutional layer. Default is 5.\n",
    "        dropout (float): Dropout rate. Default is 0.1.\n",
    "        bidir (bool): Whether to use a bidirectional LSTM. Default is True.\n",
    "\n",
    "    **Inputs**:\n",
    "        x (torch.Tensor): Input tensor of shape [B, T, K] where B is batch size, T is sequence length, and K is number of features.\n",
    "        lengths (torch.Tensor): Lengths of each sequence in the batch of shape [B].\n",
    "\n",
    "    **Outputs**:\n",
    "        logits_step (torch.Tensor): Output logits for each time step of shape [B, T].\n",
    "    \n",
    "    **Note**:\n",
    "        Use BCEWithLogitsLoss with a mask for training.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            num_fts: int, \n",
    "            cnn_channels: int = 64, \n",
    "            lstm_hidden: int = 96, \n",
    "            kernel: int = 5, \n",
    "            dropout: float = 0.1, \n",
    "            bidir: bool = True\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        # CNN layers\n",
    "        self.conv = nn.Conv1d(num_fts, cnn_channels, kernel_size=kernel, padding=kernel//2)\n",
    "        \n",
    "        # activation (ReLU) + dropout\n",
    "        self.act = nn.ReLU()\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        \n",
    "        # LSTM layers\n",
    "        self.lstm = nn.LSTM(cnn_channels, lstm_hidden, batch_first=True, bidirectional=bidir)\n",
    "        \n",
    "        # output heads\n",
    "        hdim = lstm_hidden * (2 if bidir else 1)\n",
    "        self.head_step = nn.Linear(hdim, 1)\n",
    "\n",
    "    def forward(\n",
    "            self, \n",
    "            x: torch.Tensor, \n",
    "            lengths: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        # [B,T,K] -> conv over time\n",
    "        z = self.act(self.conv(x.transpose(1,2))).transpose(1,2)        # [B,T,C]\n",
    "        z = self.drop(z)\n",
    "\n",
    "        # pack padded sequence for LSTM\n",
    "        packed = pack_padded_sequence(\n",
    "            z, \n",
    "            lengths.cpu(), \n",
    "            batch_first=True, \n",
    "            enforce_sorted=False\n",
    "        )\n",
    "        packed_out, _ = self.lstm(packed)\n",
    "        out, _ = pad_packed_sequence(packed_out, batch_first=True)      # [B,T,H]\n",
    "        \n",
    "        # get logits per timestep\n",
    "        logits_step = self.head_step(out).squeeze(-1)                   # [B,T]\n",
    "        \n",
    "        return logits_step\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "3f7cd859",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" LOSS FUNCTIONS \"\"\"\n",
    "def pitch_level_loss(\n",
    "        logits_step: torch.Tensor, \n",
    "        y_step: torch.Tensor, \n",
    "        mask: torch.Tensor, \n",
    "        pos_weight: bool = False\n",
    ") -> torch.Tensor:\n",
    "    \"\"\" \n",
    "    Compute pitch-level loss given ground truth. Valid for binary or smoothed (e.g., sigmoid) outcome labels.\n",
    "\n",
    "    Args:\n",
    "        logits_step (torch.Tensor): Logits from the model of shape [B, T].\n",
    "        y_step (torch.Tensor): Ground truth labels of shape [B, T]. Should be in [0, 1].\n",
    "        mask (torch.Tensor): Mask indicating valid time steps of shape [B, T].\n",
    "        pos_weight (bool, optional): Whether or not to use weights for positive class in BCE loss. Default is False.\n",
    "    \"\"\"\n",
    "    if pos_weight is None:\n",
    "        # setup weights\n",
    "        pos = y_step[mask].sum()\n",
    "        neg = mask.sum() - pos\n",
    "        pos_weight = neg / pos.clamp(min=1.0)\n",
    "        \n",
    "        return F.binary_cross_entropy_with_logits(logits_step[mask], y_step[mask])\n",
    "    \n",
    "    return F.binary_cross_entropy_with_logits(logits_step[mask], y_step[mask], pos_weight=pos_weight)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1bb2c2",
   "metadata": {},
   "source": [
    "$\\textbf{Sandbox: Development}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c174afe",
   "metadata": {},
   "source": [
    "$\\textit{Outcome Handling}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08009297",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gather outcomes\n",
    "    # non-injured pitcher --> outcomes = 0\n",
    "    # injured pitcher --> outcomes = ...\n",
    "example = outing_level_sequences[7]\n",
    "example_outcomes = example[['pitcher', 'season', 'outing_id', 'injured_cohort_pitcher']].drop_duplicates().reset_index(drop=True).copy()\n",
    "test_probs, test_binary = update_outcome_probabilities(example, model_type='outing_level')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5d3aa612",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_pitches = pitch_level_sequences[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c013fa6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate through all pitcher-pitches in the datespan\n",
    "for group, rows in example_pitches.groupby(['pitcher', 'season']):\n",
    "    rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "a2d9cec9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00247262, 0.00273425, 0.00302348, 0.0033432 , 0.0036966 ,\n",
       "       0.00408721, 0.0045189 , 0.00499596, 0.0055231 , 0.00610552,\n",
       "       0.00674895, 0.00745967, 0.00824462, 0.0091114 , 0.01006839,\n",
       "       0.01112476, 0.01229059, 0.01357692, 0.01499583, 0.01656054,\n",
       "       0.01828548, 0.02018641, 0.02228046, 0.0245863 , 0.02712415,\n",
       "       0.02991593, 0.03298531, 0.03635781, 0.04006084, 0.04412375,\n",
       "       0.04857786, 0.05345645, 0.05879472, 0.06462967, 0.07100002,\n",
       "       0.07794595, 0.08550885, 0.09373097, 0.10265494, 0.11232324,\n",
       "       0.12277754, 0.1340579 , 0.14620194, 0.15924378, 0.17321298,\n",
       "       0.18813338, 0.20402187, 0.22088711, 0.23872837, 0.25753434,\n",
       "       0.27728217, 0.29793663, 0.31944957, 0.3417597 , 0.36479277,\n",
       "       0.38846205, 0.41266938, 0.43730648, 0.46225681, 0.48739763,\n",
       "       0.51260237, 0.53774319, 0.56269352, 0.58733062, 0.61153795,\n",
       "       0.63520723, 0.6582403 , 0.68055043, 0.70206337, 0.72271783,\n",
       "       0.74246566, 0.76127163, 0.77911289, 0.79597813, 0.81186662,\n",
       "       0.82678702, 0.84075622, 0.85379806, 0.8659421 , 0.87722246,\n",
       "       0.88767676, 0.89734506, 0.90626903, 0.91449115, 0.92205405,\n",
       "       0.92899998, 0.93537033, 0.94120528, 0.94654355, 0.95142214,\n",
       "       0.95587625, 0.95993916, 0.96364219, 0.96701469, 0.97008407,\n",
       "       0.97287585, 0.9754137 , 0.97771954, 0.97981359, 0.98171452,\n",
       "       0.98343946, 0.98500417, 0.98642308, 0.98770941, 0.98887524,\n",
       "       0.98993161, 0.9908886 , 0.99175538, 0.99254033, 0.99325105,\n",
       "       0.99389448, 0.9944769 , 0.99500404, 0.9954811 , 0.99591279,\n",
       "       0.9963034 , 0.9966568 , 0.99697652, 0.99726575, 0.99752738])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(1 / (1 + np.exp(-np.linspace(-6, 6, rows.shape[0]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667756c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pitch_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
