{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbe7fc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from connections import AWS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8030a5",
   "metadata": {},
   "source": [
    "$\\textbf{Epidemiology: Clinical Comparison}$\n",
    "\n",
    "Compares model predictions across height-, mass-, and pitch count-match controls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7cb94e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AWS]: Port 5433 is free.\n",
      "[AWS]: Connected to RDS endpoint.\n"
     ]
    }
   ],
   "source": [
    "\"\"\" INITIALIZE AWS CONNECTION \"\"\"\n",
    "aws = AWS()\n",
    "aws.connect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a285331",
   "metadata": {},
   "source": [
    "$\\textbf{Data Loading}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1760d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load cohort of matches (see matching_create_final_cohort.ipynb for details)\n",
    "cohort_matches_final = aws.load_s3_object('epidemiology/ml/datasets/full/cohort_matches_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ec3e783",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model predictions\n",
    "model_preds = aws.load_s3_object('epidemiology/ml/datasets/preds/model_application.csv')\n",
    "model_preds.drop_duplicates(subset=['pitcher', 'game_date', 'pred_peak_evt'], inplace=True)\n",
    "\n",
    "# add season to model predictions\n",
    "model_preds['season'] = model_preds['game_date'].str[:4].astype(int)\n",
    "model_preds.sort_values(by=['pitcher', 'game_date', 'pitch_id'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ef5e0e",
   "metadata": {},
   "source": [
    "$\\textbf{Data Annotations \\& Aggregates}$\n",
    "\n",
    "__Pitch-Level Annotations__\n",
    "\n",
    "Metadata: \n",
    "- `outing_number` (_int_): Specifies the outing number in the season \n",
    "- `last_outing_before_injury` (_bool_): Whether or not this was the last outing before a pitcher injury --> can be used for injury prediction\n",
    "- `outings_until_injury` (_int_): Count of how many outings remain until an injury is observed (if applicable)\n",
    "\n",
    "Workload: \n",
    "- `within_outing_pitch_count` (_int_): Total within outing pitch count for a pitcher\n",
    "- `within_outing_cumulative_evt_workload` (_float_): Total within outing workload for a pitcher, accumulated at the pitch-level; this is the rolling sum of normalized torques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc0c40c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" PITCH LEVEL \"\"\"\n",
    "# add outing_number column\n",
    "model_preds['outing_number'] = model_preds.groupby(['pitcher', 'season'])['game_date'].rank(method='dense').astype(int)\n",
    "\n",
    "# add within outing workload metrics (pitches thrown, evt workload)\n",
    "model_preds['within_outing_pitch_count'] = model_preds.groupby(['pitcher', 'season', 'outing_number'])['pitch_id'].cumcount() + 1\n",
    "model_preds['within_outing_cumulative_evt_workload'] = model_preds.groupby(['pitcher', 'season', 'outing_number'])['pred_peak_evt_normalized'].cumsum()\n",
    "\n",
    "# add outing_before_injury column\n",
    "last_outings = model_preds[model_preds['injured_cohort_pitcher'] == 1].groupby(['pitcher', 'season'])['game_date'].max().reset_index()\n",
    "last_outings['last_outing_before_injury'] = 1\n",
    "\n",
    "# apply merges\n",
    "model_preds_annotated = model_preds.merge(last_outings, on=['pitcher', 'season', 'game_date'], how='left')\n",
    "model_preds_annotated['last_outing_before_injury'].fillna(0, inplace=True)\n",
    "model_preds_annotated.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72ac20a",
   "metadata": {},
   "source": [
    "__Outing-Level Aggregates__\n",
    "\n",
    "Total Workload Metrics:\n",
    "- `outing_total_pitch_count` (_int_): Total pitches thrown by a pitcher during an outing\n",
    "- `outing_total_evt_workload` (_float_): Total within outing EVT workload for a pitcher, summed over all pitch-level\n",
    "- `outing_avg_evt_workload` (_float_): Average per-pitch EVT workload within an outing\n",
    "- `outing_median_evt_workload` (_float_): Median per-pitch EVT workload within an outing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cda7ae90",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" OUTING LEVEL \"\"\"\n",
    "# create a copy for aggregation + store metadata\n",
    "outing_metadata = model_preds_annotated[['pitcher', 'season', 'game_date', 'injured_cohort_pitcher', 'last_outing_before_injury']].drop_duplicates()\n",
    "\n",
    "# total workload metrics\n",
    "    # NOTE: avg & median workload help distinguish from pitch counts --> how \"intense\" was each pitch that was thrown\n",
    "outing_metrics = model_preds_annotated.groupby(['pitcher', 'season', 'game_date', 'outing_number', 'pitcher_days_since_prev_game',]).agg(\n",
    "    outing_total_pitch_count=('pitch_id', 'count'),\n",
    "    outing_total_evt_workload=('within_outing_cumulative_evt_workload', 'max'),\n",
    "    outing_avg_evt_workload=('pred_peak_evt_normalized', 'mean'),\n",
    "    outing_median_evt_workload=('pred_peak_evt_normalized', 'median'),\n",
    ").reset_index()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789c4f4e",
   "metadata": {},
   "source": [
    "$\\textbf{Gather Model Predictions}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "9675adba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" MATCHED COMPARISONS \"\"\"\n",
    "#   iterate through matched rows\n",
    "#       for each match, get all model predictions for injured pitcher, non-injured pitcher (sort & store)\n",
    "matched_preds = {}\n",
    "for _, match in cohort_matches_final.sort_values('season').iterrows():\n",
    "\n",
    "    # get metadata\n",
    "    season = match['season']\n",
    "    inj_id = match['mlbamid_injured']\n",
    "    noninj_id = match['mlbamid_noninjured']\n",
    "\n",
    "    # get window of injured pitches\n",
    "    inj_first_pitch = model_preds_annotated[model_preds_annotated['pitcher'] == inj_id]['game_date'].min()\n",
    "    inj_last_pitch = model_preds_annotated[model_preds_annotated['pitcher'] == inj_id]['game_date'].max()\n",
    "\n",
    "    # filter model predictions, aggregates to date range of injured pitcher\n",
    "    model_preds_match = model_preds_annotated[\n",
    "        (model_preds_annotated['game_date'] >= inj_first_pitch) & \n",
    "        (model_preds_annotated['game_date'] <= inj_last_pitch) & \n",
    "        (model_preds_annotated['season'] == season) & \n",
    "        (model_preds_annotated['pitcher'].isin([inj_id, noninj_id]))\n",
    "    ].copy()\n",
    "    outing_metrics_match = outing_metrics[\n",
    "        (outing_metrics['game_date'] >= inj_first_pitch) & \n",
    "        (outing_metrics['game_date'] <= inj_last_pitch) & \n",
    "        (outing_metrics['season'] == season) & \n",
    "        (outing_metrics['pitcher'].isin([inj_id, noninj_id]))\n",
    "    ].copy()\n",
    "    outing_metadata_match = outing_metadata[\n",
    "        (outing_metadata['game_date'] >= inj_first_pitch) & \n",
    "        (outing_metadata['game_date'] <= inj_last_pitch) & \n",
    "        (outing_metadata['season'] == season) & \n",
    "        (outing_metadata['pitcher'].isin([inj_id, noninj_id]))\n",
    "    ].copy()\n",
    "\n",
    "    # get model predictions for injured pitcher\n",
    "    inj_preds = model_preds_match[model_preds_match['pitcher'] == inj_id]\n",
    "    noninj_preds = model_preds_match[model_preds_match['pitcher'] == noninj_id]\n",
    "\n",
    "    # store metadata & pitch/outing-level data in dictionary\n",
    "    matched_preds[inj_id] = {\n",
    "        'injured': {\n",
    "            'metadata': outing_metadata_match[outing_metadata_match['pitcher'] == inj_id].copy(),\n",
    "            'pitch_level': inj_preds,\n",
    "            'outing_level': outing_metrics_match[outing_metrics_match['pitcher'] == inj_id].copy()\n",
    "        },\n",
    "        'noninjured': {\n",
    "            'metadata': outing_metadata_match[outing_metadata_match['pitcher'] == noninj_id].copy(),\n",
    "            'pitch_level': noninj_preds,\n",
    "            'outing_level': outing_metrics_match[outing_metrics_match['pitcher'] == noninj_id].copy()\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b500afbc",
   "metadata": {},
   "source": [
    "$\\textbf{Group Comparisons}$\n",
    "\n",
    "- __Group comparison__ (i.e, inj. vs noninj. means)\n",
    "- __Pairwise comparison__ (i.e., inj. - noninj. compared to 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "318b1850",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import ttest_ind\n",
    "from statsmodels.stats.proportion import proportions_ztest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "0cae0a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup comparisons\n",
    "all_pitch_data = []                     # master list of all data (--> group comparison)\n",
    "all_diffs = {\n",
    "    'vals': [],\n",
    "    'inj_grt_than_noninj': [],\n",
    "} \n",
    "\n",
    "\n",
    "# iterate through matches\n",
    "for inj_id, match in matched_preds.items():\n",
    "    # pitch data --> add to master list\n",
    "    pitch_data = pd.concat([\n",
    "        match['injured']['pitch_level'].assign(injured=1),\n",
    "        match['noninjured']['pitch_level'].assign(injured=0)\n",
    "    ]).reset_index(drop=True)\n",
    "    all_pitch_data.append(pitch_data)\n",
    "\n",
    "    # setup pairwaise comparison\n",
    "    inj_mean = match['injured']['pitch_level']['pred_peak_evt'].mean()\n",
    "    noninj_mean = match['noninjured']['pitch_level']['pred_peak_evt'].mean()\n",
    "    \n",
    "    # compute difference\n",
    "    diff = inj_mean - noninj_mean\n",
    "    all_diffs['vals'].append(diff)\n",
    "    all_diffs['inj_grt_than_noninj'].append(int(diff > 0))\n",
    "\n",
    "# concatenate all pitch data together\n",
    "all_pitch_df = pd.concat(all_pitch_data).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "8b9aa58e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T-statistic: 17.790, p-value: 0.000\n",
      "T-statistic: -0.165, p-value: 0.565\n",
      "Z-statistic: -4563.868, p-value: 1.000\n"
     ]
    }
   ],
   "source": [
    "\"\"\" TEST 1: Group Comparison \"\"\"\n",
    "# t-test: compare means of injured vs non-injured groups\n",
    "    # NOTE: significant (--> injured sig. higher than non-injured)\n",
    "t_stat, p_value = ttest_ind(\n",
    "    all_pitch_df[all_pitch_df['injured'] == 1]['pred_peak_evt'], \n",
    "    all_pitch_df[all_pitch_df['injured'] == 0]['pred_peak_evt'], \n",
    "    alternative='greater',\n",
    "    equal_var=False\n",
    ") \n",
    "print(f\"T-statistic: {t_stat:.3f}, p-value: {p_value:.3f}\")\n",
    "\n",
    "\"\"\" TEST 2: Pairwise Comparison \"\"\"\n",
    "# t-test: compare differences to 0\n",
    "    # NOTE: not significant (--> not enough evidence to say injured match avgs are greater than non-injured)\n",
    "diffs = np.array(all_diffs['vals'])\n",
    "t_stat, p_value = ttest_ind(diffs, np.zeros_like(diffs), alternative='greater', equal_var=False)\n",
    "print(f\"T-statistic: {t_stat:.3f}, p-value: {p_value:.3f}\")\n",
    "\n",
    "\"\"\" TEST 3: Proportion of Injured > Non-Injured \"\"\"\n",
    "# proportion test: compare proportion of matches where injured avg > non-injured avg\n",
    "    # NOTE: not significant (obvious, proportion was < 0.5)\n",
    "num_injured_exceed = sum(all_diffs['inj_grt_than_noninj'])\n",
    "num_noninjured_exceed = len(all_diffs['inj_grt_than_noninj']) - num_injured_exceed\n",
    "stat, pval = proportions_ztest(num_injured_exceed, num_noninjured_exceed, value=len(all_diffs['inj_grt_than_noninj']), alternative='larger')\n",
    "print(f\"Z-statistic: {stat:.3f}, p-value: {pval:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80851e8",
   "metadata": {},
   "source": [
    "$\\textbf{Close AWS Connection}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "767fb87a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AWS]: Database connection closed.\n",
      "[AWS]: SSH tunnel stopped.\n"
     ]
    }
   ],
   "source": [
    "# close AWS connection\n",
    "aws.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c971da1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pitch_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
